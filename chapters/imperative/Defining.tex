\myChapter{Defining}\label{imperative:defining}
\section{History and Concepts}%, Concepts, and Examples}
We earlier said that \TMs inspired the imperative paradigm. What is paradigmatic about \TMs{}? It is their sequential operation and reliance on state in order to compute. A \TM computes move by move, at each step writing to its tape, updating its internal state, and shifting its read--write head a tape cell to the left or right. Its behavior is time-determined: depending on how it has modified its tape and the state it finds itself in after passing over the cells along its computational trajectory, it makes one more move, move after move, in sequence.

We then anchored our understanding of modern computers in \TMs{}. These elaborate, complex machines grow out of this simple seed, but they have not left their roots. We find again a reliance on state and sequence. The program counter advances cycle by cycle as the contents of registers and memory change.

We mentioned assembly languages in passing then and described their simple form, abbreviated mnemonics for machine instructions and operands. The imperative language family grows out of these and so inherits the \TM spirit. They have become more elaborate and complex over time, much as computers elaborated on the fundamental concept of \TMs{}.

The first imperative language was \vocab{\Fortran}{}, which is short for ``the \abbrev{IBM} Mathematical Formula Translating System.'' It was developed at \abbrev{IBM} in the 1950s as a language for scientific computing and pioneered compiler design and optimization, since no-one would trouble to make the switch from writing hand-written, hand-optimized assembly code to \Fortran unless it ran nearly as fast as such assembly code. The language theory we introduced in the chapter on compilers had not yet been developed, and \foreign{ad hoc} techniques were used instead. The problems encountered in inventing and applying these techniques spurred the development

\Fortran was a child of its time. It relied on a fixed format for code entry that was based on the punch cards used at the time. Each line of eighty characters was broken into fields of different, fixed widths, such as for numeric statement labels that were used in branching instructions, for the ``continuation character'' that indicated that the line it was on continued the previous line rather than beginning a separate statement, and for the program code itself. All of its control flow statements (other than the sequencing implied by one line of code following another) relied on the numeric labels. An example, peculiar to \Fortran{}, is the \vocab{arithmetic IF statement}. The arithmetic if transferred control to one of three statement labels depending on whether a provided arithmetic expression was less than, equal to, or greater than zero: \code{IF} \code{($\langle$expression$\rangle$)} \code{$\langle$statement label$_{1}\rangle$,} \code{$\langle$statement label$_{2}\rangle$,} \code{$\langle$statement label$_{3}\rangle$}.

\Fortran[ II] was the first \Fortran to support procedures. Procedures allow algorithms to be specified in isolation from the rest of the program and reused. Within the \vocab{procedure body}, \vocab{formal parameters} specified in the procedure's declaration are manipulated in place of the actual arguments that must be manipulated in the program. The formal parameters are ``dummy variables'' like the $x$ in the mathematical function $f(x) = x^{2}$ and exist to give a name within the function's body to all values substitutable for the formal parameter. When the procedure is called from the program, actual arguments are supplied; these are bound to the formal parameters by their position, so that the first argument is referred to by the first formal parameter, the second argument by the second formal parameter, and so forth; the computation specified by the procedure body is carried out; and, at the end of the procedure, control returns to the caller and continues where it left off. This is a powerful abstraction, and the style of programming it gives rise to is sometimes called \vocab{procedural programming}.

\Fortran procedures used what is known as a \vocab{call by reference} \vocab{evaluation strategy}. An evaluation strategy describes the way in which expressions are evaluated; the expressions that the various names for evaluation strategies focus on are, conveniently enough, functions. In call by reference evaluation, the arguments are bound to the formal parameters in such a way that modifications to the formal parameters affect the arguments' values. Suppose we have defined a function \code{MAKE-SIX} that expects a single parameter and simply sets that parameter equal to 6. If the value of $x$ is 5, and we call that function with $x$ as its argument, then, following the function call, $x$ will have the value 6.

\Fortran continues to be used and updated today. It remains a language meant for scientific computing, but now, it is trailing the innovations of other imperative languages. One of these innovations is \vocab{structured programming}, which does away with goto-based control flow in favor of \vocab{structured control flow}. The structure is one based on higher-level control flow constructs like logical if statements that execute their body depending on the truth value of the provided expression, various looping constructs that repeat their body in a predictable way, and function calls. Rather than having to deduce that one of these common patterns is being used by deciphering the various uses of labels, it is plain; rather than having to frame these relatively intuitive forms of control flow in terms of statement labels and jumps, one can express these patterns directly. This addressed the problem of ``spaghetti code'' with convoluted, goto-based control flow that made it difficult to understand and predict the operation of a program.

Another early imperative language family was \vocab{\Algol}{}, short for algorithmic language. \Algol was the product of a joint effort between European and American computer scientists to produce a common language for specifying algorithms. Each version is named by its debut year, starting with \Algol[ 58]{}. The development of \Algol saw the birth of \vocab{Backus normal form}, abbreviated \abbrev{BNF} and now read as \vocab{Backus-Naur form}, a notation for specifying grammars that has been slightly extended and used extensively since.

There were many official, numbered \Algol versions, and even more extensions and variations developed in practice, and we will ambiguously collapse them all into the single identifier \Algol{}. \Algol featured \vocab{call by value}, \vocab{call by name}, and \vocab{call by reference} evaluation strategies. With call by value, the value of the argument is provided as the value of the formal parameter, but modification of the formal parameter affects only the parameter within the function body and not the original argument. Call by name is similar to call by reference, except that each use of the parameter causes reevaluation of the associated argument. If the argument is simply a value, this is no different, but if it has side-effects, say, is a function that increments some global counter, this will be carried out each time the value of the parameter is used. This allowed for some confusing behavior, and call by reference was preferred and enforced by later versions of \Algol{}.

A notable descendent of \Algol is the C programming language, whose development began in the 70s with the \vocab{\Unix operating system} and continues today. As its development alongside an operating system might suggest, C was intended as a \vocab{systems programming language} offering relatively detailed, low-level control of the computer it is operating on. This is reflected in its use of unrestricted \vocab{memory pointers} which refer to addresses in the computer's memory. As \Unix rose in prominence, so too did C. C is still widespread and fairly widely used, and it functioned in some sense as a \foreign{lingua franca} of computing, as the \Algol family did before it. C's mark on the imperative language family is most notable in its visual appearance, where semicolons are used as statement separators and curly braces (\code{\{} and \code{\}}) are used to delimit blocks in place of other textual indicators such as \code{BEGIN} and \code{END}.

Following structured programming, the next development in the imperative family was \vocab{object-oriented programming}. Object-oriented programming introduces the concept of an \vocab{object} as a higher-level unit of abstraction than the function. An object is a bundle of state and \vocab{methods} (functions) that operate on that state. Objects are instances of a \vocab{class} or type of object, by which we mean that the objects are structured as described by the class, though each might be in a different state. Object-oriented programming entered the C family by way of C++ and, later, Java, of which the latter has perhaps replaced C as computing's \foreign{lingua franca}. These languages also introduced powerful \vocab{module}-like abstractions that allowed definitions to be grouped at an even higher level into packages or namespaces. This helps to avoid naming conflicts, which become more and more of a problem as a program grows larger and larger and requires more and more variable names, which prevents one body of code from interfering with another and eases reuse.

Java is notable for providing \vocab{garbage collection}, sometimes known as \vocab{automatic memory management}. This means that the programmer is no longer responsible for indicating when some storage referred to by a variable is no longer needed. Instead, the runtime system attempts to discover when some storage is no longer reachable or no longer needed and reclaims this by freeing up the space for use with other variables. There is a cost associated to this at run time, since the run time system must track this storage and reclaim it and lacks the knowledge of the program that the programmer has. This cost prevented its widespread adoption prior to Java. But there is also a cost to not providing garbage collection, since manual storage management has proven to be a difficult and time-consuming issue that can cause subtle problems in a program. The price is paid in development rather than at run time.

Java is also notable as being designed to run on its own associated platform, the Java \vocab{virtual machine (VM)}, rather than in a specific machine environment. This enables programs written in Java to run on any platform for which a Java virtual machine has been implemented. This too comes at a cost: the program is interpreted by the virtual machine which then uses the underlying machine to carry out the specified operation. This can be a slow process. Innovations in interpreter design and compilers have done much to ameliorate this, but it is still an issue.

Thus, imperative languages developed, in a sense, as abstractions of assembly language. They continue to rely on sequencing and state to perform computation and explicitly describe the process of computation: do this, then do that, then\dots. Mathematical notation can be used to specify formulas. Functions abstract common operations. Modules abstract over related definitions. Classes abstract over functions (now called methods) and state and allow programs to better resemble the real-life objects they are modeling. They frequently employ call by value and call by reference evaluation strategies alongside procedural and structured control abstractions. They are generally statically scoped: the extent of a variable depends primarily on where it falls in the textual description of a program. These scopes can, and do, nest, both statically (if blocks within if blocks, say) and dynamically (a procedure call places one in a new scope).%
% TODO:? I don't mention scripting languages...
% FIXME:? No examples again... :(
% FIXME:? Should I break out the history and concepts section into two sections, concepts + definitions followed by history, which makes reference to the concepts? Gar. Should probably be examples + explanation, history, concepts, in that order. ::sigh::

\section{Problems}
The imperative paradigm has problems dealing with architectures that do not reflect its heritage. Its reliance on statements that execute in sequential order, affect the associated program state, and use that state to determine their sequencing has serious problems dealing with \vocab{concurrent execution}, in which several threads of execution can be acting simultaneously and affecting each other in nontrivial, and sometimes problematic, ways. This same reliance also limits the ways in which imperative code can be composed and reused. Modules, classes, procedures, and scoping in general exists in part to address this problem by partitioning the namespace, so that one block of code's variable \code{global\_name} can differ from another's. As more and more lines of code are added to a program, the interaction between various side effects on the environment and state that are implicit in different functions and statements compounds. To sum up the imperative paradigm's problems in a single word, the problem is with scale: growing larger, in programs, in number of executing threads and processors, in problem complexity, poses a serious problem to a language family with such humble, historical origins.

\section{Bibliographic Notes}\label{imperative:defining:notes}
The proceedings of the Association for Computing Machinery's few History of Programming Languages (\abbrev{HOPL}) conferences published much valuable and fascinating material on the development of programming languages. The strongly historically rooted description of the imperative languages reflects my own views on the subject. Much of the material on functional languages contrasts them with imperative languages and discusses failings of imperative languages that appear in contrast; the problems I mention spring from this sort of comparison. If one compares the many branches of the imperative family, other problems and tradeoffs come into view, but, while interesting in themselves, they are not relevant here. The appendices of \citet{Scott:Programming:2006} feature an interesting family tree of the imperative languages as well as many capsule summaries of programming languages. The book itself is a good starting point if the diversity of programming languages catches your interest, though, as is usual, the bulk of its focus is on imperative languages.