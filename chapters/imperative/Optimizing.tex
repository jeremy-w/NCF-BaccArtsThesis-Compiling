\myChapter{Optimizing}\label{imperative:optimizing}
Our discussion of optimization in \partandnameref{Section}{background:compilers:middle} focused on the variety of properties of a program one might wish to optimize and gave examples such as speed, size, and power consumption and the problem of optimization phase ordering. Now, we will describe how one optimizes a program, along with some examples of common optimizations applied to imperative programs.

Optimization comprises two closely-related tasks: analysis, which gathers the information needed for an optimization, and application of the optimization. Each application of an optimization changes the program, so a single analysis is likely to be repeated several times. Optimizations and their analyses range from being quite generally applicable to being quite specific to the language, even to specific uses of the language. The source code might even be written in order to ease optimization, possibly through annotations with no meaning in the source language but helpful to the optimizer. Program analysis and optimization is a fruitful area of research, with new analyses and optimizations being developed constantly and older ones refined. The details of implementation are specific to each compiler and its chosen \IRs{}; as such, the \IR itself contributes in important ways to optimization. Indeed, \SSA[long] form was developed, and is extensively employed, to ease optimization.\footnote{For more on \SSA{}, see \partandnameref{Section}{background:compilers:irs}.}

\section{Analysis}
Analysis is integral to optimization. The variety of analyses are often loosely classified based on their subject. Perhaps the broadest class is \vocab{data flow analysis}. It can be distinguished from classes such as \vocab{alias analysis}, which deals with attempting to discover which names in the program refer to the same data (that is, are \vocab[alias]{aliases} for the same data), control flow analysis, and dependence analysis.

\subsection{Control Flow}
Control flow analysis is necessary to perform almost all other analyses. The aim of control flow analysis is to deduce the control flow relationships between the elements of the \IR{}. In a linear \abbrev{IR}, this makes explicit the sequential flow between adjacent statements as well as that created by jump statements, goto statements, and more structured control flow statements.

There are several approaches to control flow analysis varying in their applicability, speed, and the type of information they provide. Some methods can produce a relatively high-level structural analysis of control flow that recognizes the type of control flow created by the use of structured programming statements such as \code{while}, \code{if-then}, \code{if-then-else}, and \code{case}. Others can do little more than recognize the presence of some sort of loops as opposed to simple sequential control flow.

\subsubsection{Structural Units}
It is necessary to understand the structure of the control flow graph in order to understand the various scopes of analysis and optimization. The fundamental element of a control flow graph, typically constituting the nodes of the graph, is the \vocab{basic block (BB)}, a maximal sequence of instructions that must be executed from start to finish. This bars the possibility of either entering or exiting from the middle of a basic block, so that, for example, labeled statements can only begin a basic block. Procedure calls are a matter of some delicacy, and whether they are treated as interrupting a basic block or not depends on the purpose of the control flow analysis being performed. They might even be treated in both ways. Delayed branches also introduce problems as to how the instructions in the delay slots should be treated; fortunately, this issue can largely be ignored except for very low-level representations on architectures that make such delays visible.

We say a basic block with more than one predecessor in the control flow graph is a \vocab{join point}, since several flows of control come together in that graph. A basic block with more than one successor is similarly called a \vocab{branch point}. A single basic block can be both a join point and a branch point.

A slightly larger structural unit is the \vocab{extended basic block (EBB)}. Extended basic blocks comprise a rooted control flow subgraph. Its root is a join point. An \abbrev{EBB} is the largest connected set of basic blocks reachable from the join point that are not themselves join points. Thus, if control reaches any of the blocks in the \abbrev{EBB}, it must have gone through the root.

The procedure itself forms the next largest generally recognizable structural unit, though this is defined not in terms of the graph but rather by the program itself. The largest unit is the entire program. In between extended basic blocks and an entire procedure sit regions of various sorts, defined as suitable for different analyses and optimizations.

\subsubsection{Scopes of Analysis and Optimization}
Corresponding to these structural units are the different scopes of analysis and optimization. These names are used to describe the subgraphs of the control flow graph considered during a given analysis or optimization.
\begin{description}
\item[\vocab{local scope}] corresponds to a single basic block.
\item[\vocab{superlocal scope}] corresponds to a single extended basic block.
\item[\vocab{regional scope}] corresponds to a region not otherwise specified.
\item[\vocab{global scope}] (also called \vocab{intraprocedural scope}) corresponds to an entire procedure.
\item[\vocab{whole-program scope}] is unambiguous; you might sometimes see it called \vocab{interprocedural scope} as well, particularly in the phrase ``interprocedural analysis,'' which describes a variety of often rather intractable analyses.
\end{description}
``Global scope'' might appear to be a misnomer for anything less than the entire program, but it is generally preferred to ``intraprocedural analysis,'' since that sounds altogether too much like ''interprocedural analysis.'' Global analysis encompasses a procedure's entire control flow graph; interprocedural analysis must cope with a number control flow graphs, one for each procedure.\footnote{Attempts to introduce ``universal'' as a synonym for ``interprocedural'' as ``global'' is used for ``intraprocedural'' were unsuccessful, but the contrast between the two names might help to remember the distinction.}

\subsection{Data Flow}
Data flow analysis, together with control flow analysis, is the bread and butter of optimization. It can frequently be performed alongside control flow analysis: both require similar techniques for building and propagating information. Where control flow analysis concerns how basic blocks are related, data flow analysis concerns how various kinds of data are communicated along those relations.

Data flow analyses are posed as data flow problems. An example is the \vocab{reaching definitions problem}: What definitions of a variable could still be in force (\vocab{live}) at the point of a given use of that variable? Similar is the problem of \vocab{upward exposed variables}: Control flow graphs are generally drawn so control flows from top to bottom, and this question asks, what variables must have been defined upward of a given basic block?

These two problems typify two major classes of data flow problems, the \vocab{forward data flow problems}, like reaching definitions, and the \vocab{backward data flow problems}, like upward-exposed variables. These are so called because they require propagating information either forward along the control flow graph's edges or backwards. A third, rarer, and more troublesome class is that of the \vocab{bidirectional data flow problems}. This class is troublesome enough that it is often either not bothered with or reformulated in terms of the other two, as was the case for \vocab{partial-redundance elimination}, which seeks to discover computations of the same value that are performed multiple times along certain paths through the control flow graph.

Data flow analysis is well enough understood that it can be automated in good part by the appropriate tools. This understanding is based theoretically upon \vocab{lattices} and \vocab{flow functions}. Lattices are structured to correspond to the properties of the program under analysis. Flow functions allow us to abstractly model the effect of parts of the representation on those properties. Together, these let us abstractly simulate the effects of executing the program and discover intrinsic properties of the program, frequently independent of input values and control flow. Data flow problems can be posed in terms of these lattices and flow functions. Solutions to the problems become the \vocab{fixed points} of properly formulated data flow equations, which can be solved by iteration and, often, several other quicker and more clever methods. Solvability of such problems can be guaranteed for a variety of flow functions.

An issue that is not dealt with directly by these abstractions is the correctness of transformations based on these analyses. The analysis must be performed with an eye towards the optimizing transformation that will be based on its results. We wish to be as aggressive as possible in optimizing the program, but we cannot be so aggressive that we do not preserve its behavior. In developing and implementing optimizations, we walk a fine line between aggressiveness and conservatism: if we are too conservative in drawing conclusions from our analysis, we will fail to improve the program as much as possible; if we are overly aggressive, we will be unfaithful to the original program, and the results will be something related but different. This dilemma has its parallel in translation: a word-for-word translation is stilted and awkward, but without great care a more natural, fluid translation risks departing from the meaning of the source text.

\subsection{Dependence}
Dependence analysis aims to discover computational dependences of various sorts between the elements of the representation, often the individual statements in a low-level linear representation. It is central to instruction scheduling and is discussed in detail in \partandnameref{Section}{background:compilers:back}.

\subsection{Alias}
Alias analysis is concerned with determining when and which different names can refer to the same data. The way aliases can be created and used varies greatly from language to language. Alias analysis is often quite difficult. Making the most conservative assumptions possible\empause namely that all data whose addresses have been made available can be affected by any alias in the program\empause can guarantee correctness at the expense of preventing possible optimizations. The cost of these hyperconservative assumptions varies depending on the program. In programs that do not make extensive use of aliases, this assumption might not pose much of a problem. In programs that do use aliases extensively, the assumption could bar almost all optimization. Thus, alias analysis is necessary to creating an aggressively optimizing compiler.

There are two parts to alias analysis: \vocab{alias gathering}, which discovers which variables are aliases and basic information about what data is aliased, and \vocab{alias propagation}, which propagates this information throughout the program and completes the analysis. The propagation phase can be modeled as a data flow problem.

There are a few distinguishable types of alias information. \vocab{may information} describes what may happen but does not always happen. This information must be accounted for and its effects allowed, but it cannot be depended on to occur. \vocab{must information} is information about what must happen. This information is very useful in performing optimizations. Alias information (and the analysis that produces it) can also be \vocab{flow sensitive} and \vocab{flow insensitive}. The flow here is control flow. Flow insensitive analysis is simpler and generally can be performed as several local analyses that are then pieced together to form the whole of the information. Flow sensitive analysis is more difficult, both computationally and conceptually, but also more detailed. It requires understanding and, to some extent, simulating the program's control flow. The combinations of these factors\empause must, may, flow sensitive, and flow insensitive\empause determine how long the analysis takes, the usefulness of the information, and both the optimizations that can be based on the information and the extent of those optimizations.

\section{Optimization}
\subsection{Time}
As discussed above, optimizations can be classified by their scope. They can also be classified by when they are generally applied: early in computation, somewhere in the middle, or later. Time generally corresponds to the level of abstraction of the program representation. Early on, the representation is much closer to the source language than later, when it generally becomes much closer to assembly language.

\subsection{Examples}
We will now give several examples of optimizations. We will name the optimization, briefly describe it, and then give an example of some source language code. We then demonstrate the results of the optimization as transformed source code and provide a description of the transformations performed.
\paragraph{Common Subexpression Elimination}
There are several varieties of common subexpression elimination depending on the scope and approach. They all have the aim of avoiding redundant computation by reusing existing values. Common subexpression elimination can be usefully applied both early and late in compilation.

Listings \ref{optimizing:cse:before} and \ref{optimizing:cse:after} provide a simple example of common subexpression elimination. The initial assignments of both \code{i} and \code{j} require the computation of \code{a * b}. Common subexpression elimination will factor out this computation into a temporary value so that it need only be computed once. Notice that common subexpression elimination tends to increase register pressure since the common subexpressions require storage must be stored.

\lstinputlisting[float=p,caption={Common subexpression elimination: Source code},label=optimizing:cse:before]{chapters/imperative/examples/cseBefore}

\lstinputlisting[float=p,caption={Common subexpression elimination: Transformed code},label=optimizing:cse:after]{chapters/imperative/examples/cseAfter}

\paragraph{Dead and Useless Code Elimination}
This optimization is easy to describe, though in practice it ends up being applied in a variety of cases. It simplifies the representation, which speeds the following analyses and transformations. It is commonly run many times during compilation. It aims to eliminate code that is \vocab{useless}, that is, that computes a result no longer used, and code that is \vocab{dead} or unreachable. Dead and useless code can be the result of textual substitution as done by the C preprocessor in expanding macros or the result of other optimizations eliminating all uses of a definition or all statements in a block.

The example in listings~\ref{optimizing:deadcode:before} and~\ref{optimizing:deadcode:after} is a very artificial example in C. Code portability is often achieved in C by writing suitable preprocessor macros that are then configured based on the environment in which the code is compiled. Environment-dependent code that cannot be dealt with abstractly through macros is included conditional on other macros representing the platform. The preprocessor's conditional statements are normally used to selectively include code for compilation by the compiler as opposed to the conditional statements of the language. Here, we instead use the C language's conditional statements to include code. This results in dead and useless code that would be removed by dead and useless code elimination, as shown in the transformed code.

\lstinputlisting[float=p,caption={Dead and useless code elimination: Source code},label=optimizing:deadcode:before]{chapters/imperative/examples/deadBefore}

\lstinputlisting[float=p,caption={Dead and useless code elimination: Transformed code},label=optimizing:deadcode:after]{chapters/imperative/examples/deadAfter}

\paragraph{Code Hoisting}
Code hoisting is so called because it corresponds visually to lifting a computation up in the control flow graph, which is usually drawn so that control flows from top to bottom. Rather than specifying that a computation occur in all branches of an extended basic block, we might be able to hoist the computation up to the common ancestor of all those blocks so that it is specified only once. This reduces code size. Code hoisting is a type of \vocab{code motion}.

Listings~\ref{optimizing:hoisting:before} and \ref{optimizing:hoisting:after} provide a trivial example. As with other such examples, it is likely the programmer would perform such a painfully obvious optimization in the source code. Nevertheless, it is instructive. The computation of \code{x}, which occurs in all branches of the switch\dots{}case statement, is hoisted from the cases to before the switch. You can see this hoisting visually in terms of the corresponding control flow graphs in Fig.~\ref{optimizing:hoisting:cfgs}.

\lstinputlisting[float=p,caption={Code hoisting: Source code},label=optimizing:hoisting:before]{chapters/imperative/examples/hoistingBefore}

\lstinputlisting[float=p,caption={Code hoisting: Transformed code},label=optimizing:hoisting:after]{chapters/imperative/examples/hoistingAfter}

\input{chapters/imperative/examples/hoistingCfgs}

%\paragraph{Partial Redundancy Elimination}
%Partial redundancy elimination is closely related to code hoisting. A computation is partially redundant if it is computed along more than one path through the control flow graph but not all. One way of carrying out partial redundancy elimination is \vocab{lazy code motion}, where the laziness comes out in its attempt to postpone inserting instructions till as late as possible. Lazy code motion 

\paragraph{Loop Unswitching}
Here, switching refers to if-then-else control flow or, more generally, switch\dots{}case control flow. When this occurs within the loop, the switching occurs with each passage through the loop. If the condition determining which case of the switch is executed is loop invariant, then we can move the switch to surround the loop and then duplicate the loop within each case. Then the switch is encountered only once, when we select which variety of the loop to use. This trades code size against execution speed: there are fewer branches, so the code will run faster, but the loop body must be repeated in each case.

In listing~\ref{optimizing:unswitching:before}, we find a loop with a nested if-then-else statement. If we assume that \code{warnlevel} remains unchanged throughout the loop, then we would, each time we go through the loop, have to test \code{warnlevel} in order to select the same, still appropriate branch. Listing~\ref{optimizing:unswitching:after} shows the results of applying loop unswitching to the code in listing~\ref{optimizing:unswitching:before}. The branch is now selected prior to looping, which eliminates many tests and jumps.

Note how loop unswitching obscures the basic intent of the code, namely, "tell everyone on the team a certain message depending on the current \code{warnlevel}," and reduplicates the code governing control flow (the for-loop header). If the programmer were to apply loop unswitching manually to the source code in this case, obscuring the code's purpose would harm its long-term maintainability, and reduplicating the control flow code introduces the opportunity of updating it in one branch but failing to update it it in the other. Thus, it is inadvisable for the programmer to manually perform this optimization. Since the execution time saved by unswitching the loop could be significant, it is important that the compiler perform this optimization.

\lstinputlisting[float=p,caption={Loop unswitching: Source code},label=optimizing:unswitching:before]{chapters/imperative/examples/unswitchingBefore}

\lstinputlisting[float=p,caption={Loop unswitching: Transformed code},label=optimizing:unswitching:after]{chapters/imperative/examples/unswitchingAfter}

%I guess we're not doing induction variable elim/linear func test replacement any more.

\section{Bibliographic Notes}
While \citet[chapters~8--10]{Cooper:Engineering:2004} provides an introduction to analysis and optimization, \citet{Muchnick:Advanced:1997} concerns itself almost exclusively with analysis, optimization, and the construction of a compiler that executes these. (This chapter is heavily indebted to both books.) It leaves untouched matters of optimizations for parallel architectures and other such optimizations needed by demanding scientific computing programs. For discussion of those issues, it recommends \citet{Bannerjee:Dependence:1988,Bannerjee:Loop:1993,Bannerjee:Loop:1994,Wolfe:High-Performance:1996} and \citet{Zima:Supercompilers:1991}.

The \abbrev{LLVM} (Low-Level Virtual Machine) project \citep{Lattner:LLVM:2002} is notable among other things for its extensive use of \SSA form in its compiler architecture. It uses \SSA as its primary representation of the program through most of compilation.

Static analysis is valuable for much more than compile-time optimization. It is necessary for development of advanced \abbrev{IDEs} (interactive development environments), source code style-checking tools, and bug-finding tools, among other things. Static analysis and its many uses is an active topic of research that has led to several commercial ventures, such as Klocwork,\footnote{\url{http://www.klocwork.com/}} Coverity,\footnote{\url{http://www.coverity.com/}} and Fortify,\footnote{\url{http://www.fortifysoftware.com/}} as well as open-source research projects seeing industrial use such as Findbugs~\citep{Hovemeyer:Finding:2004}.