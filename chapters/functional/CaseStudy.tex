\myChapter{Case Study: The Glasgow Haskell Compiler}\label{casestudy}
The discussion of the previous chapter was limited to generalities. We now look at a specific implementation of functional compilation. The Glasgow Haskell compiler is an actively developed, mature compiler for the lazy functional language Haskell. It implements numerous extensions to the standard language and provides a variety of additional tools and libraries, many of which are used in developing the compiler itself.

We gave a brief history of the Haskell language towards the end of \partandnameref{Chapter}{functional:history}. The Glasgow Haskell Compiler (\GHC{}) is today the principal Haskell compiler. It is used both to produce compiled Haskell programs doing real work as well as for research into functional languages and their implementation. \GHC is written primarily in Haskell itself, though some parts (including most of the runtime system) are implemented in C.

Compilers are very complex programs made up of a number of interacting, complex parts. We make no pretense of describing the \GHC[long] \foreign{in toto.} Our study is guided by two questions:
\begin{itemize}
\item
How does \GHC use the fact it is compiling a functional language to its advantage?

\item
How does \GHC solve the problems a functional language poses for compilation?
\end{itemize}

Answering these questions entails looking at specific optimizations enabled by functional languages and optimizations required to efficiently compile functional languages. All optimizations are carried out using specific intermediate representations, so we will describe the intermediate representations used in these compilers. In the course of discussing solutions to problems introduced by functional languages, we will also briefly discuss \GHC{}'s implementation of garbage collection and pattern matching. We will also look more closely at how it transforms the source functional program into something executable in the target, von~Neumann environment.

\subsection{Intermediate Representations}
The \GHC[long] uses several progressively simpler intermediate languages. We can roughly equate each language with a certain phase in compilation:
\begin{itemize}
\item
The front end uses a representation of Haskell itself.

\item
The middle end uses a much simpler core language called, unsurprisingly, Core.

\item
The back end uses the STG and Cmm languages.
\end{itemize}

The first intermediate representation is produced and used by the front end. This representation is a representation of Haskell itself using data types and constructors. \GHC takes the unusual step of performing type inference using what is fundamentally the source language rather than a desugared, simpler, core language. This makes it easy for the compiler to report errors in terms of the code provided by the programmer.

The Haskell representation is then desugared into a very simple core language called \vocab[Core]{Core.} Core encodes values alongside their types, and so optimizations using the Core representation can take advantage of type information. This type information includes information on type equality constraints and coercions. Further details do not concern us, but can be found in an article by~\citet{Sulzmann:System:2007}.

Possibly one of the greatest advantages of carrying through types into the intermediate language, however, is not that they become available for optimization, but that they make it easy to catch errors introduced during development, since such an error is likely to introduce an erroneous term that can be caught by a simple type check. As all optimizations are performed as Core-to-Core transformations, and optimizations can interact in complex ways, error recognition by cheap type checking is very helpful.

The back end uses the STG (for Spineless Tagless G-machine) language. Core is transformed into STG through an intermediate step. The Core representation is first transformed into a Core representation of the program that is closer in spirit to STG. Only after this transformation is the Core representation transformed into an STG representation.

STG is the language of an abstract machine, the Spineless Tagless G-machine. This machine was designed for efficient translation into imperative code executed by a conventional, von~Neumann computer. However, \GHC does not translate STG code directly into native code. It instead translates it into Cmm. Cmm is \GHC{}'s implementation of C-{}- (read ``C-minus-minus''; see~\citet{Peyton-Jones:C--:1999} for a description), a language that closely resembles C but is somewhat simpler and lower-level.\footnote{\GHC neither uses nor requires all capabilities of C-{}-, and so Cmm does not implement those unneeded capabilities. Other small differences combine to make Cmm a dialect of C-{}-, which is itself a moving target. Current information on C-{}- is available from \url{http://www.cminusminus.org/}.}

Once the program is represented in Cmm, it can be compiled to native code in two different ways: directly, or through C. (The choice is the user's, though the default is direct generation of native code from the Cmm representation.)\footnote{Another transformation that would turn the generated Cmm into \CPS[long] Cmm code is currently under development.}

Direct code generation proceeds by transforming the Cmm representation into a data type representation of assembly instructions. Where the front end began by transforming the source code into a data type representation of Haskell code, the back end finishes by printing out a representation of the data type encoding of the assembly instructions. The resulting code can then be assembled into an object file.

Compilation by C is messier, less elegant, and appears to be deprecated. Since Cmm is virtually a subset of C, it is not difficult to generate something that can be compiled as a C program. This is compiled with any available C compiler. Since the program is not really a C program but a representation in C of a Spineless Tagless G-machine program, some of the assumptions made by the compiler are erroneous and result in suboptimal code. The assembly language code produced by the C compiler is thus postprocessed as a last optimization. The primary effects of this postprocessing are the removal of many unneeded register save and restore sequences and the rearrangement of the memory layout of the assembly code. This results in a corresponding rearrangement of the object file produced when the assembly code is assembled.

\subsection{Garbage Collection}
\GHC implements generational garbage collection. Generational garbage collection is based on the assumption that ``young'' objects\empause those that have been recently allocated\empause are more likely to have died than older objects. Generational garbage collectors thus focus their garbage collecting efforts on younger objects. The age of an object is described in terms of generations. The garbage collector assigns all allocated objects to one of a set of generations. A newly created object belongs to the first generation. During collection, pointers in objects of those older generations that are not being collected are used as roots to determine which objects of the generations undergoing collection are live. Objects that survive a certain number of collections are promoted to the next generation.

Garbage collection is invoked frequently to keep heap use under control; since most garbage collections only examine younger generations, such minor collections are inexpensive. When a minor collection will not suffice to reclaim sufficient memory, a major collection is performed using a mark-compact algorithm: this leads to all generations being examined, so that storage allocated to older objects that, by virtue of their age, had survived collection past the end of their lives is reclaimed.

\GHC{}'s garbage collector is of the so-called ``stop the world'' variety. During garbage collection, only the garbage collector is active. All computation ceases. This plays a surprisingly important role in ensuring that \GHC interfaces well with the outside library (\abbrev{GMP}, the \abbrev{GNU} Multiple-Precision Library) that it uses to provide arbitrary-precision arithmetic. The arithmetic library is implemented in C; if garbage collection occurred while a library function was being executed, garbage collection could relocate the data the function was was working with out from under the function's pointer to that data. However, because the garbage collector requires that the world be stopped, it is only invoked when all running threads have reached a sequence point; since none of the functions provided by this library have such a stopping point, they cannot be interrupted by the garbage collector.

\subsection{Pattern Matching}
Pattern matching is in fact a core part of the Core and STG representations. Even conditional expressions are handled through pattern matching: \lstinline{if B then X else Y} is written using a case expression as \lstinline|case B of {True -> X; False -> Y}|. \lstinline{B} is called the expression scrutinized by the case expression. In the Core and STG languages, case analysis forces evaluation of the expression scrutinized by the case analysis.

Expression evaluation is a necessary part of pattern matching in these intermediate languages. To explain why, we must describe how data type declarations are treated. As discussed earlier, a declaration such as \lstinline{data Tree a = Leaf a | Branch (Tree a) (Tree a)} creates an algebraic data type with two constructors, \code{Leaf} and \code{Branch}. The ordering of these declarations is considered significant in Core and STG, because each constructor is assigned a discriminator integer, beginning with 0 and increasing by one for each constructor declaration. Thus, \code{Leaf} would be assigned 0 and \code{Branch} would be assigned 1.

Pattern matching uses this discriminator to decide which branch of the case expression should be chosen. After evaluation of the scrutinized expression completes, its discriminator is examined; a jump is then made directly to the corresponding branch of the case expression. The simplicity of this system is due to the Spineless Tagless G-machine's inbuilt support for algebraic data types.

\subsection{Optimizations}
All optimizations are performed as Core to Core transformations~\citep{Peyton-Jones:A-transformation-based:1998}. Some optimizations, such as strictness analysis and let-floating, require significant nonlocal analysis. Some can be done in the course of several local simplification passes.

Some of these local simplifications are specified using a rewrite rule syntax~\citep{Peyton-Jones:Playing:2001} that is available to all users of the \GHC[long]{}. Others are more complex, such as function inlining. Inlining basically treats a function call as a macro; it replaces the call with an instance of the body of the function. (There are, as always, some complexities~\citep{Peyton-Jones:Secrets:2002}.) Inlining a function eliminates the need to perform a pipeline-unfriendly jump to the code for the function during evaluation. The \GHC[long] uses heuristics to determine when a function should be inlined; it will also inline a function when the programmer has specified through a source-code annotation that the function should be inlined.

Strictness analysis~\citep{Peyton-Jones:Measuring:1993} attempts to discern which expressions will perforce be evaluated. The code can then be optimized to avoid the costs of unnecessary laziness: no suspension need be created in the first place for the expression, and so the expression will not need to be forced and updated later. Thus, strictness analysis can be used to avoid a fair amount of work.

The let-floating transformations are another class of non-local transformations. Let floating describes the effect of the transformation: a let or letrec binding is shifted from one place in the source code to another. Shifting here should be understood in terms of depth within an expression: within a lambda abstraction, the body of a branch of a case expression, or the expression scrutinized by a case expression. There are advantages and disadvantages to floating let-bindings both in and out, as well as some local transformations that are generally helpful; the specific application of let-floating to a given case is decided, as is usual in optimizations, through some heuristic rules. Further information on let-floating can be found in the article by~\citet{Peyton-Jones:Let-floating:1996}.

Of all these optimizations, strictness analysis is the only one we can plainly categorize as an example of an optimization necessitated by the inherent inefficiency of a lazy language. The rest of the optimizations are little different from the code tuning transformations an imperative compiler might perform using \SSA[long] form;\footnote{This analogy is more accurate than you might at first think; \SSA can actually be looked at as transforming an imperative program into an equivalent functional program to ease analysis~\citep{Appel:SSA-is-functional:1998}.} indeed, the imperative optimization stalwart, common subexpression elimination, can also be applied to a Core program.

\subsection{Going von Neumann}
The transformation that reroots the functional program in the imperative, von~Neumann paradigm is the transformation from the program's STG representation to its Cmm representation. Of course, by the time the program has been transformed into the STG language, a significant amount of analysis and optimization has been performed with the aim of producing code that is more efficient (both in space and time) within the von~Neumann setting. Since all the ``heavy lifting'' has been done using other, more complex representations, the actual translation from STG to Cmm is fairly direct. That is not to say that it is simple; there are many details concerning the precise memory layout of a closure, the accommodation of unboxed types, and heap and control flow management.

While we can identify the transformation from STG into Cmm as the moment that the functional program becomes a viable imperative program, this single aim influences the entirety of the compiler's design. There are many other compilers for many other functional languages, all complex and all implementing their own approach to functional compilation, but with this brief survey of the \GHC[long], we bring our discussion of functional compilation to a close.

\section{Bibliographic Notes}
\Citet[\S 9]{Hudak:A-history:2007} places the \GHC[long] in the context of other implementations of the Haskell language. The version of \GHC considered here is version 6.8.2. More information on \GHC{} is available from its website, \url{http://haskell.org/ghc/}. The \GHC Commentary, written by the developers for other developers and anyone else interested in the compiler's internals, is available at~\url{http://hackage.haskell.org/trac/ghc/wiki/Commentary}; it was very helpful in preparing this chapter. The source code itself is also well-commented: if you should wish to explore functional compilation in more depth by reviewing the code for a compiler, you could scarcely hope for a compiler with better documentation.

The Spineless Tagless G-machine~\citep{Peyton-Jones:Implementing:1992,Peyton-Jones:The-Spineless:1989} refines the Spineless G-machine of~\citet{Hammond:The-Spineless:1993}, which itself is a refinement of Johnsson's G-machine~\citep{Johnsson:Efficient:2004}. Around 2007, reconsideration of the \asword{tagless} part of the Spineless Tagless G-machine led to the introduction of tags indicating whether or not a closure has been evaluated and, if so, the discriminator of its data constructor in order to reduce branch mispredictions encountered during case analysis and cache pollution caused by unnecessarily loading the info table of the closure. This work is described by~\citet{Marlow:Faster:2007}.