\subsection{Pure Untyped \LambdaCalc}
Pure untyped \lambdacalc is the original form of the \lambdacalc{}. When someone speaks of ``the \lambdacalc{}'' without qualification, this is what is meant. Its purity is due to its conceptual simplicity and elegance. We call it untyped because it makes no distinction between types: everything is of the same type.

The building blocks of the \lambdacalc are \vocab{lambda terms}. We will call the set of all lambda terms $\Lambda$. $\Lambda$ is readily defined using a context-free grammar:
\begin{align*}
\Lambda &\produces V \alt P \alt B\\
V &\produces v \alt V\prime\\
P &\produces (\Lambda \; \Lambda)\\
B &\produces (\lambda V \; \Lambda)
\end{align*}

Let us go through this, line by line, to ensure we understand it. The first production
\[
\Lambda \produces V \alt P \alt B
\]
says that we build terms in $\Lambda$ using three different rules, $V$, $P$, and $B$. If we peek ahead at the productions for $P$ and $B$, we can see that these depend on $\Lambda$. Thus, the first rule, $V$, is critical. It generates the most basic lambda terms: \vocab{variables}. The essential properties of variables are:
\begin{aenumerate}
\item They have no substructure: they are \vocab[atomic]{atomic.}
\item Each variable is distinguishable from each other.
\end{aenumerate}
The first property is plain from the definition. The second property becomes clear once we write out the terms that $V$ produces. These are none other than the infinite set $\set{v,\, v\prime,\, v\prime\prime,\, \dotsc}$, namely, an infinite set of variables, each built from the same basic symbol ($v$) by the addition of more and more primes ($\prime$).

Now that we have some terms in $\Lambda$, we are free to describe how to form new lambda terms from other lambda terms. That is what the next two rules do. Each describes one way to generate, given two lambda terms, one more lambda term.

The first way is to juxtapose two terms and enclose the result in parentheses. This is described in the context free grammar as
\[
P \produces (\Lambda \; \Lambda)
\]
and is known as \vocab{application}. If $M$ and $N$ are lambda terms, then an application of $M$ to $N$ looks like $(M \; N)$.

The second way is to follow a $\lambda$ with a variable and another term and enclose the whole in parentheses. The grammatical production corresponding to this is
\[
B \produces (\lambda V \; \Lambda)
\]
and is known as \vocab{abstraction}. If $M$ is a term, then $(\lambda v \; M)$ is an example of an abstraction. We shall call the variable $v$ in such a term the \vocab{variable of abstraction} and the term $M$ the $abstraction body$. We will sometimes adopt the point of view of constructing this term from $M$ and $v$. In that case, we say that we are ``abstracting the variable $v$ over the term $M$.''

As it stands, the readability of this notation degrades rapidly as lambda terms become more complex. The number of parentheses grows rapidly, and it becomes difficult to tell which variables are identical and which different as the number of primes in use grows. Thus, we introduce some conventions:
\begin{itemize}
\item Lowercase letters ($x$, $y$, $z$, and so forth) represent atomic variables.
\item Capital letters ($M$, $N$, and the like) represent arbitrary lambda terms.
\item Application is considered to associate to the left. This allows us to omit the parentheses introduced by abstraction except when we must override this rule for a specific term. Thus, $MNOP$ should be read as $(((M\; N)\; O)\; P)$.
\item Abstraction is considered to associate to the right. This allows us to omit many of the parentheses introduced by abstraction. Thus, $\lambda x \; \lambda y \; \lambda z \; M$ should be read as $(\lambda x\; (\lambda y\; (\lambda z\; M)))$.
\item The variable of abstraction will be separated from the abstraction body by a dot, so that we write $\lambda x.M$ instead of $\lambda x\; M$.
\end{itemize}
These conventions are summarized in Table~\ref{untyped:conventions}~on~p.~\pageref{untyped:conventions}

\begin{table}[tbp]
\caption[Conventions for \lambdacalc notation]{Notational conventions}
\label{untyped:conventions}
\myfloatalign
\begin{tabular}{ccc}
\toprule
\tableheadline{Type of Term} &\tableheadline{Becomes} &\tableheadline{Originally}
\\
\midrule
Variables &$x,\, y,\, z,\, \dotsc$ &$v,\, v\prime,\, v\prime\prime,\, \dotsc$
\\
Terms &$M,\, N,\, \dotsc$ &$M,\, N,\, \dotsc \in \Lambda$
\\
Application &$MNOP$ &$(((M\; N)\; O)\; P)$
\\
Abstraction &$\lambda x. \lambda y. \lambda z. M$ &$(\lambda v\; (\lambda v\prime\; (\lambda v\prime\prime\; M)))$ where $M \in \Lambda$
\\
\bottomrule
\end{tabular}
\end{table}

We shall write $M \equiv N$ to state that $M$ and $N$ are syntactically equivalent. Intuitively, when we say that they are syntactically equivalent, we mean that they were ``built'' the same way and, though they might use different variable names, they can be considered to be the ``same term in a different guise.'' For example, thanks to the infinity of basic variables, we can readily construct an infinity of syntactically equivalent lambda terms by picking a variable, picking another variable and applying the first to the second, then abstracting over the second:
\begin{enumerate}
\item Pick a variable.
\[
x \equiv y \equiv z \equiv \dotso
\]
\item Pick another.
\[
y \equiv z  \equiv w \equiv \dots
\]
\item Apply the first to the second.
\[
x y \equiv y z  \equiv z w \equiv \dots
\]
\item Abstract over the second.
\[
\lambda y . x y \equiv \lambda z . y z \equiv \lambda w . z w \equiv \dotso
\]
\end{enumerate}
At each step, all the terms listed are syntactically equivalent. We claim that this is justified because all these terms behave the same way, as we shall see quite soon. This makes the notion of syntactic equivalence a powerful and useful one. But this concept is more sophisticated than it might at first sound, and there are some pitfalls to watch out for in defining and using it, which we shall come to shortly.

There is one more ingredient of the pure untyped \lambdacalc{}. So far, we have established a static universe of lambda terms. We can conceive of larger and larger terms, but we cannot simplify them or do anything beyond list them. The missing spark that puts these terms in motion and enables computation is called \vocab{$\beta$-re\-duc\-tion}. $\beta$-reduction resembles the application of grammatical productions in context-free grammars, and the notation is similar, though both the behavior and notation are slightly and significantly different.

We begin by defining \vocab{single-step $\beta$-reduction}, written $\betared$. This relates lambda terms to lambda terms; specifically, it says that we can replace the application of an abstraction term to another term with the term formed from the abstraction body by substituting the other term for the variable of abstraction wherever it occurs in the body:
\[
(\lambda x . M) N \betared \replace{x}{N}{M}
\]
We can also describe how $\beta$-reduction behaves with other sorts of terms:
\begin{itemize}
\item $\beta$-reduction is allowed in either half of an application.
\[
\frac{M \betared M\prime}{M N \betared M\prime N}
\qquad
\frac{M \betared M\prime}{N M \betared N M\prime}
\]
\item $\beta$-reduction is allowed within an abstraction.
\[
\frac{M \betared M\prime}{\lambda x.M \betared \lambda x.M\prime}
\]
\end{itemize}
The notation $\frac{\langle\text{\textit{upper statements}}\rangle}{\langle\text{\textit{lower statements}}\rangle}$ means that, if we know the upper statements to be true, then the lower statements must also be true. The horizontal line between the two levels can be read ``implies.''

We can similarly define \vocab{many-step $\beta$-reduction}, \betared[*]: $M \betared[*] N$, where $N$ is not necessarily distinct from $M$, if there is some chain of zero or more $\beta$-reductions beginning with $M$ and terminating with $N$, that is, $M \betared M\prime \betared \dotsb \betared N$.\footnote{Note that we are using a two-headed arrow $\twoheadrightarrow$ here instead of the starred arrow $\derives[*]{}$ that we used for the similar concept of derivation in multiple steps with context-free grammars. As with derivation, where a subscript $lm$ or $rm$ indicated whether leftmost or rightmost derivation was used, a subscript $\beta$ here indicates that $\beta$-reduction was employed.} Unlike single-step $\beta$-reduction, which only relates different terms, many-step $\beta$-reduction also relates a term to itself: $M \betared[*] M$ always.

\subsubsection{\texorpdfstring{$\beta$-Reduction and the Perils of Names}{Beta-Reduction and the Perils of Names}}\label{untyped:nameperils}
What is going on here? We can think of it this way: abstraction binds its variable. Once we have abstracted a term over a given variable, we cannot do so again. The variable is no longer free. When we apply an abstracted term to another term, $\beta$-reduction simultaneously performs the binding of the variable to the other term and substitutes that term for the bound variable throughout the abstraction body. Now that its purpose has been fulfilled and the abstraction made concrete, the abstraction disappears. An example will make this clearer. Take $\lambda y . xy$. Applying it to some lambda term, $M$, and $\beta$-reducing it gives:
\[
(\lambda y . xy) M \betared \replace{y}{M}{xy} = xM
\]

But here there be dragons. This is where the convenient identification of syntactically equivalent terms returns with a vengeance. Suppose we take the doubly-abstracted term $\lambda x. \lambda y. x y$ and apply it to the seemingly innocent term $w y z$. Let us also apply it to the syntactically equivalent term $t u v$. What happens?
\begin{align*}
(\lambda x. \lambda y. x y) (w y z) &\betared \replace{x}{w y z}{\lambda y. xy} = \lambda y. w y z y
\\
(\lambda x. \lambda y. x y) (t u v) &\betared \replace{x}{t u v}{\lambda y. xy} = \lambda y. t u v y
\end{align*}
But, should we apply this to yet another term, say $s$, something unexpected occurs:
\begin{align*}
(\lambda y. w y z y) s &\betared \replace{y}{s}{w y z y} = w s z s
\\
(\lambda y. t u v y) s &\betared \replace{y}{s}{t u v y} = t u v s
\end{align*}
The results are obviously no longer syntactically equivalent!

This problem should be familiar to anyone acquainted with the first-order predicate calculus. The abstraction symbol $\lambda$ in the \lambdacalc behaves exactly the same as do the quantifiers $\exists$ and $\forall$ in that both bind their associated variable in the body of the term that we say is abstracted over in the \lambdacalc and quantified in the first-order predicate calculus. The problem is one of \vocab{variable capture}: we substituted $w y z$, which has as its free variables $\set{w, y, z}$, into a term in which $y$ was bound, thus incidentally binding the $y$ of $w y z$. When we substituted the syntactically equivalent $t u v$, however, all three variables remained free in the result. Thus, na\"{i}ve $\beta$-reduction does not necessarily preserve syntactic equivalence, contrary to our intent in establishing that equivalence. The only way to ensure that syntactically equivalent terms produce equivalent results is to be very careful to avoid variable capture. This is more difficult to fully specify than it sounds, and we refer you to any text on the first-order predicate calculus for the details. For the purposes of this example, it suffices that we always rename bound variables to some variable that does not occur free in the term about to be substituted. Here, that would mean renaming the bound variable $y$ to some variable other than those of $w y z$, say $v$. If we perform this renaming and then repeat our experiment, we get the appropriate results:
\begin{align*}
(\lambda x. \lambda y. x y) (w y z) &\equiv (\lambda x. \lambda v. x v) (w y z) \quad \text{(renaming $y$ to $v$)}
\\
(\lambda x. \lambda v. x v) (w y z) &\betared \replace{x}{w y z}{\lambda v. xv} = \lambda v. w y z v
\quad \text{(first application)}
\\
(\lambda v. w y z v) s &\betared \replace{v}{s}{w y z v} = w y z s \quad \text{(second application)}
\\
w y z s &\equiv t u v s \quad \text{(the results)}
\end{align*}
As you can see, the result is now syntactically equivalent to that reached when we use $tuv$ instead of $wyz$.

\subsubsection{\texorpdfstring{$\alpha$-Reduction}{Alpha-Reduction}}\label{untyped:alpha}
There are two ways around the variable capture problem. One is to simply assume that all this renaming takes place automatically and get on with the theory. This is very convenient if all you are interested in is developing the theory associated with the \lambdacalc and is a favorite choice of theoreticians. The other option is to formalize this renaming process by introducing another type of reduction, \vocab{$\alpha$-reduction}, and modify the rules surrounding $\beta$-reduction to explicitly forbid its use where variable capture would occur, thus forcing the invocation of $\alpha$-reduction before $\beta$-reduction can continue. This is somewhat messier, but it better reflects what must occur in a practical implementation of the \lambdacalc{}. While waving our hands and saying that we identify syntactically equivalent terms and all renaming occurs as necessary for things to come out as desired in the end works fine on paper and fine with humans, we must be a bit more explicit if we are to successfully implement $\beta$-reduction in a compiler.

Doing so gets even messier. We must continually come up with unique names, make repeated textual substitutions, and keep checking to ensure we're not about to capture a variable. But this mess was foisted upon us by our choice of variables. We have already made clear by our treatment of these variables that their names serve as nothing more than placeholders. They are just ways for the abstraction to point down into the term and indicate at which points we should make the substitutions called for by $\beta$-reduction. What if, instead, we reversed the relationship between the abstraction and its variable?

\subsubsection{De Bru\ij n Indices}\label{untyped:indices}
The central insight of \vocab{de Bru\ij n indices} is to eliminate the use of corresponding variable names in the abstraction in favor of numbers ``pointing'' to the appropriate lambda. Free variables are considered to point to lambdas that have yet to be introduced. Thus, instead of writing $\lambda x . xy$, we would write $\lambda.12$, since the $x$ in the former notation is bound by the first enclosing $\lambda$, and $2$ is the first number greater than the number of enclosing lambdas. The more complex term $\lambda x . (\lambda y . xy) (\lambda z. \lambda y. xyz)$ would become $\lambda . (\lambda . 2 1) (\lambda . \lambda . 321)$.

This demonstrates that we are not simply renaming variables-as-letters to variables-as-numbers. Instead, we are using two closely-related notions to assign the numbers: the level and depth of a given variable occurrence. To determine a variable occurrence's \vocab{level}, we think of starting from the outside of the expression, at level 1, and descending through it to the occurrence. Each time we enter the scope of another lambda along the way, we drop down another level in the term, from level 1 to level 2 and so on. We ultimately replace the bound variable with its \vocab{depth}. The depth is how many levels above the current level of nesting the corresponding binding $\lambda$ is located. If the occurrence and its binder are at the same level, we consider the occurrence's depth to be 1. Each level we must ascend from the variable after that to reach the binder adds one to the depth. We conceive of the free variables as sitting one above the other over the outermost lambda of the term, so that we must ascend past the top level in counting out their depth. We can thus readily identify free variables, since their depth is greater than their level.

Take the $x$s in the last example, which became respectively $2$ and $3$. Let us visually display the level of an term by dropping down a line on the page; then the conversion between variable names and de Bru\ij n indices becomes easier to see:
\begin{table}[h]
\caption{Converting to de Bru\ij n indices}
\myfloatalign
\begin{tabular}{cllllllll}
\toprule
\tableheadline{Level}
&\multicolumn{4}{c}{\spacedlowsmallcaps{Variable Names}} 
&\multicolumn{4}{c}{\spacedlowsmallcaps{De Bru\ij n Indices}}
\\
% Before adding the LEVEL column, having two midrules split in the middle looked good and visually set off the two versions of the lambda term. Once LEVEL was added, however, it did not make sense to join the midrule below it to that of the first lambda expression, and it looked too choppy to have two breaks in the midrule. Thus, a single midrule, but now, the two lambda expressions are not so clearly separated.
\midrule
%\cmidrule(r){1-1}
%\cmidrule(lr){2-5}
%\cmidrule(l){6-9}
1 &
$\lambda x.$  &                    &               &                       &
$\lambda.  $
\\
2 &
              & $(\lambda y. x y)$ & $(\lambda z.$ &                       &
              & $(\lambda  . 2 1)$ & $(\lambda  .$
\\
3 &
              &                    &               & $ \lambda y. x y z)$ &
              &                    &               & $ \lambda  . 3 2 1)$
\\
\bottomrule
\end{tabular}
\end{table}

This conversion can be performed algorithmically by keeping track of which variable names were bound at which level of nesting. We can also readily convert from de Bru\ij n indices back to variable names. This allows for the entry and display of lambda terms using variable names while reduction proceeds in terms of de Bru\ij n indices. Since de Bru\ij n indices give a unique representation for each syntactically equivalent lambda term, they sidestep the problems with variable binding and the like that we encountered earlier.\footnote{You might have noticed that, since the indices do depend on the level of nesting, they must be adjusted when substitution occurs under abstraction. But this is only a slight problem compared to the mess brought on by names, and it can be readily and efficiently dealt with.}

\subsubsection{Currying}\label{untyped:currying}
But variable names are more convenient for humans both to write and read,\footnote{De Bru\ij n suffered no confusion on this count: he intended his namefree notation to be ``easy to handle in a metalingual discussion'' and ``easy for the computer and for the computer programmer'' and expressly not for humans to read and write \citep[pp.~381--82]{Bruijn:Lambda:1972}.} and so we return to using the more conventional notation. We can even do a bit more to increase the readability of our lambda terms. We have already eliminated excess parentheses: let us now eliminate excess lambdas.

Consider the lambda term $\lambda x. (\lambda y. (xy))$. According to our conventions, abstraction associates to the right, and so this can be unambiguously written as $\lambda x. \lambda y. xy$. But, look again: the distinction between the variable bound by abstraction and the term within which it is bound is clear, since each abstracted term has the form $\lambda \text{$\langle$\textit{variable}$\rangle$} . \text{$\langle$\textit{term}$\rangle$}$. When we come across nested abstraction, as above, it is clear that those variables closer to the nested term but left of a dot are the result of abstraction at a deeper level of nesting. So let us write, instead of $\lambda x. \lambda y. xy$, rather $\lambda x y . xy$. This applies wherever we would not require that parentheses intervene between nested lambdas due to convention. Thus, we could rewrite $\lambda x. (\lambda y. xy) (\lambda z. \lambda y. xyz)$ as $\lambda x. (\lambda y. xy) (\lambda zy.xyz)$.

Notice now how a lambda term such as $\lambda xy. xy$ resembles a function of multiple arguments. If we apply it to two terms in succession, then it eventually $\beta$-reduces precisely as if we had supplied two arguments to a binary function: $(\lambda xy. xy) M N \betared[*] M N$. But, what happens if we apply it to a single term? Well, $(\lambda xy. xy) M \betared \lambda y. My$, that is, we get back a term abstracted over $y$. It is as if, on supplying only one argument to an $n$-ary function, we got back a function of \mbox{arity $n - 1$.} When we apply this term to, say, $N$, we arrive at $M N$ yet again, precisely as if we had immediately supplied both ``arguments'' to the original term.

This is not mere coincidence. We can, in fact, represent \emph{all} $n$-ary functions as compositions of $n$ unary functions. Each such function simply returns a function expecting the next argument of the original, $n$-ary function. This form of an $n$-ary function is known as its \vocab{curried} form, and the process of transforming an uncurried function into a curried function is called \vocab{currying}.\footnote{The name is a reference to the logician Haskell B. Curry, though the idea appears to be due to Moses Sch\"{o}nfinkel.}

\subsubsection{From Reduction to Conversion}\label{untyped:conversion}
Let us now step back a ways to where we had just defined $\beta$-reduction. $\beta$-reduction is a one-way process: it can never make a term more complicated than it was before, though we cannot go all the way to claiming that $\beta$-reduction always results in a less complicated term. For example, $(\lambda x.xx) (\lambda x.xx)$ always and only $\beta$-reduces to itself and thus becomes neither more nor less complicated: $(\lambda x.xx) (\lambda x.xx) \betared \replace{x}{xx}{\lambda x. xx} = (\lambda x.xx) (\lambda x.xx) \betared \dotsb$.

But $\beta$-reduction does relate terms: one term is related to another if it can eventually $\beta$-reduce to that term. In some sense, all terms that are the result of $\beta$-reduction of some other term are related in that very way. We thus name this relation by saying that such terms are \vocab{$\beta$-convertible}, so that if $M \betared[*] N$, or $N \betared[*] M$, or $L \betared[*] M$ and $L \betared[*] N$, or $M \betared[*] P$ and $N \betared[*] P$, then $M$ and $N$ (and additionally $L$ and $P$, if such is the case) are $\beta$-convertible.\footnote{To be more exact, $\beta$-conversion is the equivalence relation generated by many-step $\beta$-reduction.} This is illustrated in Fig.~\ref{untyped:betaconv}, p.~\pageref{untyped:betaconv}. We notate this relation with a subscripted equals sign: $M \betacon N$. Note that, as a consequence of the definition of $\beta$-convertibility, $M \betacon M$ for all lambda terms $M$. 

(Now that we have introduced $\beta$-conversion, it is time to emend our earlier comments on $\alpha$-reduction. What we have called $\alpha$-reduction is more commonly, and more properly, called $\alpha$-conversion, since the relation between names is inherently bidirectional: $x$ converts to $y$ as readily as $y$ converts to $x$.)

\begin{figure}[btp]
\caption[$\beta$-conversion]{$\beta$-conversion\\
Letters represent lambda terms. A directed arrow $M \to N$ means that $M$ $\beta$-reduces to $N$ in some number of steps, that is, $M \betared[*] N$.
}
\label{untyped:betaconv}
\myfloatalign
\begingroup
\SelectTips{eu}{10}
\setlength\extrarowheight{15pt}%default length is 3.0pt
\begin{tabular}{cc}
\tableheadline{From this\dots} &\tableheadline{we deduce this.}
\\
\xymatrix{M \ar[r] &N}
& 
$M \betacon N$
\\
\xymatrix{&L \ar[dl] \ar[dr]\\
M & & N} & $M \betacon L \betacon N$\\
\xymatrix{M \ar[dr] & & N \ar[dl]\\
& P}
&
$M \betacon P \betacon N$
\\
\xymatrix{
&L \ar[dl] \ar[dr]\\
M \ar[dr] & &N \ar[dl] %\ar@{=>}[rr] & & {L \betacon M \betacon N \betacon P}
\\
&P
}
&
$L \betacon M \betacon N \betacon P$
\end{tabular}
\endgroup
\end{figure}

An important property of the \lambdacalc is related to this. It is known as the \vocab{Church--Rosser property}, and it says two things.
\begin{itemize} 
\item Firstly, it says that if any lambda term can be reduced in one or more steps to two different lambda terms, then it is possible to reduce each of those lambda terms in some number of steps to the same term. More symbolically, this can be put as follows: if $M \betared[*] M_{1}$ and $M \betared[*] M_{2}$, then there exists an $M_{3}$ such that $M_{1} \betared[*] M_{3}$ and $M_{2} \betared[*] M_{3}$. What this means graphically is that, given the figure from the second row of the table in Fig.~\ref{untyped:betaconv}~(p.~\pageref{untyped:betaconv}), we can infer the existence of the term $P$ in the the last row of that table.
\item Secondly, it states that if two terms $M$ and $N$ are $\beta$-convertible into each other, then there is some other common term $P$ to which the first two can both be reduced. This is to say that, if $M \betacon N$, then we can find some $P$ such that $M \betared[*] P$ and $N \betared[*] P$ as well.
\end{itemize}

Since the Church--Rosser property is so important, we will endeavor to sketch a proof that it holds for the pure untyped lambda calculus. %CRIT-TODO: Sketch proof of Church--Rosser.

\subsubsection{Normal Forms}\label{untyped:nf}
Now that we have defined lambda terms and $\beta$-reduction, we can give a normal form for lambda terms. This is important if we are to explain the impact of the Church-Rosser property. A lambda term is said to be in \vocab{normal form} if it cannot be further $\beta$-reduced, that is to say, the lambda term $M$ is in normal form if there is no $N$ such that $M \betared N$.

Not all terms have normal forms. The lambda term $(\lambda x.xx)(\lambda x.xx)$, sometimes called $\Omega$, has no normal form, since it reduces always and only to itself. Some terms do have normal forms, but it is possible to $\beta$-reduce the term an arbitrary number of times without reaching this form. What this means practically that it is important to select the right \vocab{reducible expression (redex)} to $\beta$-reduce, otherwise one might continue $\beta$-reducing without ever terminating in the normal form. It is easy to produce examples of such terms by throwing $\Omega$ into the mix: $(\lambda xy.y) \Omega z$ has as its normal form $z$, but of course one could repeatedly select $\Omega$ for reduction and thereby never realize this. Terms that \emph{always} reduce to normal form within a finite number of $\beta$-reductions, regardless of the reduction strategy employed, are called \vocab{strongly normalizing}.

Now that the concepts of normal forms and strongly normalizing terms is clear, we can explicate the Church-Rosser property. Altogether, it means that, if a term has a normal form, then regardless of the reduction steps we use to reach that form, it will always be possible from anywhere along the way to reach the normal form: there's no way we can misstep and be kept from ever reaching the normal form short of intentionally, repeatedly making the wrong choice of expression to reduce. If the term is strongly normalizing, we can go one better and state that the reduction steps we use to reach its normal form are completely irrelevant, as all chains of reductions will eventually terminate in that normal form. The property also shows that the normal form is unique, for if $N_{1}$ and $N_{2}$ were two distinct normal forms of a given term, then they would have to share a common term $N_{3}$ to which they could both be $\beta$-reduced.

\subsubsection{Recursion and $Y$}\label{untyped:y}
The pure, untyped \lambdacalc is Turing complete. An important part of achieving this degree of expressive power is the ability to make recursive definitions using the \lambdacalc{}. The way to do so is surprisingly succinct. One common means is what is known as the \vocab{paradoxical combinator}, universally designated by $Y$. Here is its definition:
\[
Y = \lambda f. (\lambda x.f(xx))(\lambda x.f(xx))
\]
It is also known as the \vocab{fixed-point combinator}, since for every $F$, we have that $F(YF) \betacon YF$.\footnote{$Y$ is not unique in producing fixed points. Turing's fixed point operator $\Theta = (\lambda a b . b (aab)) (\lambda a b . b (aab))$ will do just as well.} Let us see how exactly this works:
\begin{align*}
Y F &\equiv (\redex{\lambda f}. (\lambda x.\redex{f}(xx))(\lambda x.\redex{f}(xx))) \redex{F} \\
&\betared (\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))}\\
&\betared F ( (\lambda x. \redex{F}(xx)) (\lambda x. \redex{F}(xx)) )\\
&\betacon F (\redex{(\lambda f. (\lambda x.f(xx))(\lambda x.f(xx)))} F)\\
&\equiv F (Y F)
\end{align*}
At each step, we have used color to indicate the terms that are involved in producing the next step. We have indicated at each step whether syntactic equivalence, $\beta$-reduction, or full-fledged $\beta$-conversion was employed. As you can see, we begin by using simple $\beta$-reduction. The breakthrough that permits us to arrive at the desired form is replacing the two instances of $F$ in the inner term $((\dotsb F \dotsb)(\dotsb F \dotsb))$ by the application of $\lambda f. (\dotsb f \dotsb)(\dotsb f \dotsb)$ to $F$.

If you look at the steps leading up to that abstraction, you can see how $Y$ leads to recursion. Let us carry this process out a bit further:
\begin{align*}
Y F &\equiv (\redex{\lambda f}. (\lambda x.\redex{f}(xx))(\lambda x.\redex{f}(xx))) \redex{F} \\
&\betared (\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))}\\
&\betared F (\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))}\\
&\betared F ( F ( ((\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))} ))\\
&\betared F ( F ( F ( ((\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))} )))\\
&\vdots
\end{align*}
As you can see quite plainly, $YF$ leads to repeated self-application of $F$. This iteration of $F$ is how it produces a fixed point.%CRIT-TODO: Does it ALWAYS reach a fixed point? Or only sometimes?

\subsubsection{A Brief Word on Reduction Strategies}\label{untyped:strategies}
Before we move on to extend the \lambdacalc with some concepts a bit more elaborate and convenient than the low-level but elegant pure, untyped \lambdacalc{}, it behooves us to put in a brief word about reduction strategies.

In the context of the \lambdacalc{}, we call them reduction strategies. In the context of programming languages in general, we use the phrase \vocab{order of evaluation}. They both come down to the same thing: where do we want to focus our efforts? And, perhaps more importantly, where should we focus our efforts? 

The choices we make in terms of \lambdacalc reduction strategy have equivalents in terms of order of evaluation of functions and their arguments. The fundamental question is this: should we evaluate the arguments before passing them on to the function, or should we call the function and just point it at its arguments so it has access to them and their values as needed?

If we first deal with the arguments and only then with the function as a whole, then we are employing a \vocab{call-by-value} evaluation strategy, so-called because the function call takes place with the values of the arguments provided to the function. The arguments are evaluated, and the resulting value is bound to the formal parameters of the function.

If we instead begin by evaluating the body of the function itself and only evaluate the arguments as necessary, we are pursuing a \vocab{call-by-name} evaluation strategy. Call-by-name gets its name from the way that the formal parameters are bound only to the names of the actual arguments. It is only when the value of one of the arguments is required to proceed with evaluating the function's body that the argument is evaluated.

The \lambdacalc equivalent of call-by-value is known as the \vocab{applicative order} reduction strategy. We can describe the applicative order strategy quite simply: reduction is first performed in the term to which the abstraction is being applied. Only when we have exhausted this possibility do we perform the application.

If we strengthen this preference into a hard and fast rule that no $\beta$-reduction is to be performed under an abstraction, then we can define an alternative normal form, known as \vocab{weak normal form}:
\begin{itemize}
\item Variables are defined to be in weak normal form.
\item An application $MN$ is in weak normal form if and only if both $M$ and $N$ are in weak normal form.
\item An abstraction $\lambda x . M$ is in weak normal form.
\end{itemize}
It is in the treatment of abstractions that weak normal form differs from normal form: weak normal form considers all abstractions $\lambda x . M$ to already be in weak normal form, while normal form requires that the term $M$ being abstracted over also be in normal form.

The \lambdacalc equivalent of call-by-name, on the other hand, is known as the \vocab{normal order} reduction strategy and corresponds to always select the leftmost--outermost reducible expression for reduction. Normal order is so called because, if the term has a normal form, we can \textit{always} reduce it to normal form by employing the normal order reduction strategy.%TODO: Sketch a proof that normal order reduction always results in normal form if it exists, as well.
The same cannot be said for applicative order, which can fail to reduce a term to normal form even when one exists. Again, the $\Omega$ term makes it easy to give an example. Something as simple as $(\lambda x y. y) \Omega z$ suffices. Under normal order evaluation, we first $\beta$-reduce $(\lambda x y. y) \Omega \betared (\lambda y. y)$ and then apply the result to $z$, giving the normal form $z$. With applicative order evaluation, however, we begin by $\beta$-reducing the first argument, $\Omega$\empause and that is as close as we shall ever get to the normal form $z$, since $\Omega$ has no normal form.

\subsubsection{Strictness}\label{untyped:strictness}
If a computation never terminates, as with the attempt to $\beta$-reduce $\Omega$ to normal form, then we say that it \vocab{diverges}. An important concept for functions is that of \vocab{strictness}. We say a function is strict in a given parameter if the evaluation of the function itself diverges whenever the evaluation of the parameter diverges. The function $\lambda x y. y$ given above is strict only in its second argument, $y$, since we can evaluate the function even when a divergent term is substituted for $x$, as recently demonstrated. A function that is strict in all arguments is called a \vocab{strict function}. The function $\lambda x. x$ can readily be seen as strict, since this function is simply the identity function.