\subsection{Pure Untyped \LambdaCalc}
Pure untyped \lambdacalc is the original form of the \lambdacalc{}. When someone speaks of ``the \lambdacalc{}'' without qualification, this is what is meant. Its purity is due to its conceptual simplicity and elegance. We call it untyped because it makes no distinction between types: everything is of the same type.

The building blocks of the \lambdacalc are \vocab{lambda terms}. We will call the set of all lambda terms $\Lambda$. $\Lambda$ is readily defined using a context-free grammar:
\begin{align*}
\Lambda &\produces V \alt P \alt B\\
V &\produces v \alt V\prime\\
P &\produces (\Lambda \; \Lambda)\\
B &\produces (\lambda V \; \Lambda)
\end{align*}

Let us go through this, line by line, to ensure we understand it. The first production
\[
\Lambda \produces V \alt P \alt B
\]
says that we build terms in $\Lambda$ using three different rules, $V$, $P$, and $B$. If we peek ahead at the productions for $P$ and $B$, we can see that these depend on $\Lambda$. Thus, the first rule, $V$, is critical. It generates the most basic lambda terms: \vocab{variables}. The essential properties of variables are:
\begin{aenumerate}
\item They have no substructure: they are \vocab[atomic]{atomic.}
\item Each variable is distinguishable from each other.
\end{aenumerate}
The first property is plain from the definition. The second property becomes clear once we write out the terms that $V$ produces. These are none other than the infinite set $\set{v,\, v\prime,\, v\prime\prime,\, \dotsc}$, namely, an infinite set of variables, each built from the same basic symbol ($v$) by the addition of more and more primes ($\prime$).

Now that we have some terms in $\Lambda$, we are free to describe how to form new lambda terms from other lambda terms. That is what the next two rules do. Each describes one way to generate, given two lambda terms, one more lambda term.

The first way is to juxtapose two terms and enclose the result in parentheses. This is described in the context free grammar as
\[
P \produces (\Lambda \; \Lambda)
\]
and is known as \vocab{application}. If $M$ and $N$ are lambda terms, then an application of $M$ to $N$ looks like $(M \; N)$.

The second way is to follow a $\lambda$ with a variable and another term and enclose the whole in parentheses. The grammatical production corresponding to this is
\[
B \produces (\lambda V \; \Lambda)
\]
and is known as \vocab{abstraction}. If $M$ is a term, then $(\lambda v \; M)$ is an example of an abstraction. We shall call the variable $v$ in such a term the \vocab{variable of abstraction} and the term $M$ the $abstraction body$. We will sometimes adopt the point of view of constructing this term from $M$ and $v$. In that case, we say that we are ``abstracting the variable $v$ over the term $M$.''

As it stands, the readability of this notation degrades rapidly as lamb\-da terms become more complex. The number of parentheses grows rapidly, and it becomes difficult to tell which variables are identical and which different as the number of primes in use grows. Thus, we introduce some conventions:
\begin{itemize}
\item Lowercase letters ($x$, $y$, $z$, and so forth) represent atomic variables.
\item Capital letters ($M$, $N$, and the like) represent arbitrary lambda terms.
\item Application is considered to associate to the left. This allows us to omit the parentheses introduced by abstraction except when we must override this rule for a specific term. Thus, $MNOP$ should be read as $(((M\; N)\; O)\; P)$.
\item Abstraction is considered to associate to the right. This allows us to omit many of the parentheses introduced by abstraction. Thus, $\lambda x \; \lambda y \; \lambda z \; M$ should be read as $(\lambda x\; (\lambda y\; (\lambda z\; M)))$.
\item The variable of abstraction will be separated from the abstraction body by a dot, so that we write $\lambda x.M$ instead of $\lambda x\; M$.
\end{itemize}
These conventions are summarized in Table~\ref{untyped:conventions}~on~p.~\pageref{untyped:conventions}

\begin{table}[tbp]
\caption[Conventions for \lambdacalc notation]{Notational conventions}
\label{untyped:conventions}
\myfloatalign
\begin{tabular}{ccc}
\toprule
\tableheadline{Type of Term} &\tableheadline{Becomes} &\tableheadline{Originally}
\\
\midrule
Variables &$x,\, y,\, z,\, \dotsc$ &$v,\, v\prime,\, v\prime\prime,\, \dotsc$
\\
Terms &$M,\, N,\, \dotsc$ &$M,\, N,\, \dotsc \in \Lambda$
\\
Application &$MNOP$ &$(((M\; N)\; O)\; P)$
\\
Abstraction &$\lambda x. \lambda y. \lambda z. M$ &$(\lambda v\; (\lambda v\prime\; (\lambda v\prime\prime\; M)))$ where $M \in \Lambda$
\\
\bottomrule
\end{tabular}
\end{table}

We shall write $M \equiv N$ to state that $M$ and $N$ are syntactically equivalent. Intuitively, when we say that they are syntactically equivalent, we mean that they were ``built'' the same way and, though they might use different variable names, they can be considered to be the ``same term in a different guise.'' For example, thanks to the infinity of basic variables, we can readily construct an infinity of syntactically equivalent lambda terms by picking a variable, picking another variable and applying the first to the second, then abstracting over the second:
\begin{enumerate}
\item Pick a variable.
\[
x \equiv y \equiv z \equiv \dotso
\]
\item Pick another.
\[
y \equiv z  \equiv w \equiv \dots
\]
\item Apply the first to the second.
\[
x y \equiv y z  \equiv z w \equiv \dots
\]
\item Abstract over the second.
\[
\lambda y . x y \equiv \lambda z . y z \equiv \lambda w . z w \equiv \dotso
\]
\end{enumerate}
At each step, all the terms listed are syntactically equivalent. We claim that this is justified because all these terms behave the same way, as we shall see quite soon. This makes the notion of syntactic equivalence a powerful and useful one. But this concept is more sophisticated than it might at first sound, and there are some pitfalls to watch out for in defining and using it, which we shall come to shortly.

There is one more ingredient of the pure untyped \lambdacalc{}. So far, we have established a static universe of lambda terms. We can conceive of larger and larger terms, but we cannot simplify them or do anything beyond list them. The missing spark that puts these terms in motion and enables computation is called \vocab{$\beta$-re\-duc\-tion}. $\beta$-re\-duc\-tion resembles the application of grammatical productions in context-free grammars, and the notation is similar, though both the behavior and notation are slightly and significantly different.

We begin by defining \vocab{single-step $\beta$-re\-duc\-tion}, written $\betared$. This relates lambda terms to lambda terms; specifically, it says that we can replace the application of an abstraction term to another term with the term formed from the abstraction body by substituting the other term for the variable of abstraction wherever it occurs in the body:
\[
(\lambda x . M) N \betared \replace{x}{N}{M}
\]
We can also describe how $\beta$-re\-duc\-tion behaves with other sorts of terms:
\begin{itemize}
\item $\beta$-re\-duc\-tion is allowed in either half of an application.
\[
\frac{M \betared M\prime}{M N \betared M\prime N}
\qquad
\frac{M \betared M\prime}{N M \betared N M\prime}
\]
\item $\beta$-re\-duc\-tion is allowed within an abstraction.
\[
\frac{M \betared M\prime}{\lambda x.M \betared \lambda x.M\prime}
\]
\end{itemize}
The notation $\frac{\langle\text{\textit{upper statements}}\rangle}{\langle\text{\textit{lower statements}}\rangle}$ means that, if we know the upper statements to be true, then the lower statements must also be true. The horizontal line between the two levels can be read ``implies.''

We can similarly define \vocab{many-step $\beta$-re\-duc\-tion}, \betared[*]: $M \betared[*] N$, where $N$ is not necessarily distinct from $M$, if there is some chain of zero or more $\beta$-re\-duc\-tions beginning with $M$ and terminating with $N$, that is, $M \betared M\prime \betared \dotsb \betared N$.\footnote{Note that we are using a two-headed arrow $\twoheadrightarrow$ here instead of the starred arrow $\derives[*]{}$ that we used for the similar concept of derivation in multiple steps with context-free grammars. As with derivation, where a subscript $lm$ or $rm$ indicated whether leftmost or rightmost derivation was used, a subscript $\beta$ here indicates that $\beta$-re\-duc\-tion was employed.} Unlike single-step $\beta$-re\-duc\-tion, which only relates different terms, many-step $\beta$-re\-duc\-tion also relates a term to itself: $M \betared[*] M$ always.

\subsubsection{\texorpdfstring{$\beta$-Reduction and the Perils of Names}{Beta-Reduction and the Perils of Names}}\label{untyped:nameperils}
What is going on here? We can think of it this way: abstraction binds its variable. Once we have abstracted a term over a given variable, we cannot do so again. The variable is no longer free. When we apply an abstracted term to another term, $\beta$-re\-duc\-tion simultaneously performs the binding of the variable to the other term and substitutes that term for the bound variable throughout the abstraction body. Now that its purpose has been fulfilled and the abstraction made concrete, the abstraction disappears. An example will make this clearer. Take $\lambda y . xy$. Applying it to some lambda term, $M$, and $\beta$-reducing it gives:
\[
(\lambda y . xy) M \betared \replace{y}{M}{xy} = xM
\]

But here there be dragons. This is where the convenient identification of syntactically equivalent terms returns with a vengeance. Suppose we take the doubly-abstracted term $\lambda x. \lambda y. x y$ and apply it to the seemingly innocent term $w y z$. Let us also apply it to the syntactically equivalent term $t u v$. What happens?
\begin{align*}
(\lambda x. \lambda y. x y) (w y z) &\betared \replace{x}{w y z}{\lambda y. xy} = \lambda y. w y z y
\\
(\lambda x. \lambda y. x y) (t u v) &\betared \replace{x}{t u v}{\lambda y. xy} = \lambda y. t u v y
\end{align*}
But, should we apply this to yet another term, say $s$, something unexpected occurs:
\begin{align*}
(\lambda y. w y z y) s &\betared \replace{y}{s}{w y z y} = w s z s
\\
(\lambda y. t u v y) s &\betared \replace{y}{s}{t u v y} = t u v s
\end{align*}
The results are obviously no longer syntactically equivalent!

This problem should be familiar to anyone acquainted with the first-order predicate calculus. The abstraction symbol $\lambda$ in the \lambdacalc behaves exactly the same as do the quantifiers $\exists$ and $\forall$ in that both bind their associated variable in the body of the term that we say is abstracted over in the \lambdacalc and quantified in the first-order predicate calculus. The problem is one of \vocab{variable capture}: we substituted $w y z$, which has as its free variables $\set{w, y, z}$, into a term in which $y$ was bound, thus incidentally binding the $y$ of $w y z$. When we substituted the syntactically equivalent $t u v$, however, all three variables remained free in the result. Thus, na\"{i}ve $\beta$-re\-duc\-tion does not necessarily preserve syntactic equivalence, contrary to our intent in establishing that equivalence. The only way to ensure that syntactically equivalent terms produce equivalent results is to be very careful to avoid variable capture. This is more difficult to fully specify than it sounds, and we refer you to any text on the first-order predicate calculus for the details. For the purposes of this example, it suffices that we always rename bound variables to some variable that does not occur free in the term about to be substituted. Here, that would mean renaming the bound variable $y$ to some variable other than those of $w y z$, say $v$. If we perform this renaming and then repeat our experiment, we get the appropriate results:
\begin{align*}
(\lambda x. \lambda y. x y) (w y z) &\equiv (\lambda x. \lambda v. x v) (w y z) \quad \text{(renaming $y$ to $v$)}
\\
(\lambda x. \lambda v. x v) (w y z) &\betared \replace{x}{w y z}{\lambda v. xv} = \lambda v. w y z v
\quad \text{(first application)}
\\
(\lambda v. w y z v) s &\betared \replace{v}{s}{w y z v} = w y z s \quad \text{(second application)}
\\
w y z s &\equiv t u v s \quad \text{(the results)}
\end{align*}
As you can see, the result is now syntactically equivalent to that reached when we use $tuv$ instead of $wyz$.

\subsubsection{\texorpdfstring{$\alpha$-Reduction}{Alpha-Reduction}}\label{untyped:alpha}
There are two ways around the variable capture problem. One is to simply assume that all this renaming takes place automatically and get on with the theory. This is very convenient if all you are interested in is developing the theory associated with the \lambdacalc and is a favorite choice of theoreticians. The other option is to formalize this renaming process by introducing another type of reduction, \vocab{$\alpha$-reduction}, and modify the rules surrounding $\beta$-re\-duc\-tion to explicitly forbid its use where variable capture would occur, thus forcing the invocation of $\alpha$-reduction before $\beta$-re\-duc\-tion can continue. This is somewhat messier, but it better reflects what must occur in a practical implementation of the \lambdacalc{}. While waving our hands and saying that we identify syntactically equivalent terms and all renaming occurs as necessary for things to come out as desired in the end works fine on paper and fine with humans, we must be a bit more explicit if we are to successfully implement $\beta$-re\-duc\-tion in a compiler.

Doing so gets even messier. We must continually come up with unique names, make repeated textual substitutions, and keep checking to ensure we're not about to capture a variable. But this mess was foisted upon us by our choice of variables. We have already made clear by our treatment of these variables that their names serve as nothing more than placeholders. They are just ways for the abstraction to point down into the term and indicate at which points we should make the substitutions called for by $\beta$-re\-duc\-tion. What if, instead, we reversed the relationship between the abstraction and its variable?

\subsubsection{De Bru\ij n Indices}\label{untyped:indices}
The central insight of \vocab{de Bru\ij n indices} is to eliminate the use of corresponding variable names in the abstraction in favor of numbers ``pointing'' to the appropriate lambda. Free variables are considered to point to lambdas that have yet to be introduced. Thus, instead of writing $\lambda x . xy$, we would write $\lambda.12$, since the $x$ in the former notation is bound by the first enclosing $\lambda$, and $2$ is the first number greater than the number of enclosing lambdas. The more complex term $\lambda x . (\lambda y . xy) (\lambda z. \lambda y. xyz)$ would become $\lambda . (\lambda . 2 1) (\lambda . \lambda . 321)$.

This demonstrates that we are not simply renaming var\-i\-a\-bles-as-let\-ters to var\-i\-a\-bles-as-num\-bers. Instead, we are using two closely-related notions to assign the numbers: the level and depth of a given variable occurrence. To determine a variable occurrence's \vocab{level}, we think of starting from the outside of the expression, at level 1, and descending through it to the occurrence. Each time we enter the scope of another lambda along the way, we drop down another level in the term, from level 1 to level 2 and so on. We ultimately replace the bound variable with its \vocab{depth}. The depth is how many levels above the current level of nesting the corresponding binding $\lambda$ is located. If the occurrence and its binder are at the same level, we consider the occurrence's depth to be 1. Each level we must ascend from the variable after that to reach the binder adds one to the depth. We conceive of the free variables as sitting one above the other over the outermost lambda of the term, so that we must ascend past the top level in counting out their depth. We can thus readily identify free variables, since their depth is greater than their level.

Take the $x$s in the last example, which became respectively $2$ and $3$. Let us visually display the level of an term by dropping down a line on the page; then the con\-ver\-sion between variable names and de Bru\ij n indices becomes easier to see:
\begin{table}[h]
\caption{Converting to de Bru\ij n indices}
\myfloatalign
\begin{tabular}{cllllllll}
\toprule
\tableheadline{Level}
&\multicolumn{4}{c}{\spacedlowsmallcaps{Variable Names}} 
&\multicolumn{4}{c}{\spacedlowsmallcaps{De Bru\ij n Indices}}
\\
% Before adding the LEVEL column, having two midrules split in the middle looked good and visually set off the two versions of the lambda term. Once LEVEL was added, however, it did not make sense to join the midrule below it to that of the first lambda expression, and it looked too choppy to have two breaks in the midrule. Thus, a single midrule, but now, the two lambda expressions are not so clearly separated.
\midrule
%\cmidrule(r){1-1}
%\cmidrule(lr){2-5}
%\cmidrule(l){6-9}
1 &
$\lambda x.$  &                    &               &                       &
$\lambda.  $
\\
2 &
              & $(\lambda y. x y)$ & $(\lambda z.$ &                       &
              & $(\lambda  . 2 1)$ & $(\lambda  .$
\\
3 &
              &                    &               & $ \lambda y. x y z)$ &
              &                    &               & $ \lambda  . 3 2 1)$
\\
\bottomrule
\end{tabular}
\end{table}

This con\-ver\-sion can be performed algorithmically by keeping track of which variable names were bound at which level of nesting. We can also readily convert from de Bru\ij n indices back to variable names. This allows for the entry and display of lambda terms using variable names while reduction proceeds in terms of de Bru\ij n indices. Since de Bru\ij n indices give a unique representation for each syntactically equivalent lambda term, they sidestep the problems with variable binding and the like that we encountered earlier.\footnote{You might have noticed that, since the indices do depend on the level of nesting, they must be adjusted when substitution occurs under abstraction. But this is only a slight problem compared to the mess brought on by names, and it can be readily and efficiently dealt with.}

\subsubsection{Currying}\label{untyped:currying}
But variable names are more convenient for humans both to write and read,\footnote{De Bru\ij n suffered no confusion on this count: he intended his namefree notation to be ``easy to handle in a metalingual discussion'' and ``easy for the computer and for the computer programmer'' and expressly not for humans to read and write \citep[pp.~381--82]{Bruijn:Lambda:1972}.} and so we return to using the more conventional notation. We can even do a bit more to increase the readability of our lambda terms. We have already eliminated excess parentheses: let us now eliminate excess lambdas.

Consider the lambda term $\lambda x. (\lambda y. (xy))$. According to our conventions, abstraction associates to the right, and so this can be unambiguously written as $\lambda x. \lambda y. xy$. But, look again: the distinction between the variable bound by abstraction and the term within which it is bound is clear, since each abstracted term has the form $\lambda \text{$\langle$\textit{variable}$\rangle$} . \text{$\langle$\textit{term}$\rangle$}$. When we come across nested abstraction, as above, it is clear that those variables closer to the nested term but left of a dot are the result of abstraction at a deeper level of nesting. So let us write, instead of $\lambda x. \lambda y. xy$, rather $\lambda x y . xy$. This applies wherever we would not require that parentheses intervene between nested lambdas due to convention. Thus, we could rewrite $\lambda x. (\lambda y. xy) (\lambda z. \lambda y. xyz)$ as $\lambda x. (\lambda y. xy) (\lambda zy.xyz)$.

Notice now how a lambda term such as $\lambda xy. xy$ resembles a function of multiple arguments. If we apply it to two terms in succession, then it eventually $\beta$-reduces precisely as if we had supplied two arguments to a binary function: $(\lambda xy. xy) M N \betared[*] M N$. But, what happens if we apply it to a single term? Well, $(\lambda xy. xy) M \betared \lambda y. My$, that is, we get back a term abstracted over $y$. It is as if, on supplying only one argument to an $n$-ary function, we got back a function of \mbox{arity $n - 1$.} When we apply this term to, say, $N$, we arrive at $M N$ yet again, precisely as if we had immediately supplied both ``arguments'' to the original term.

This is not mere coincidence. We can, in fact, represent \emph{all} $n$-ary functions as compositions of $n$ unary functions. Each such function simply returns a function expecting the next argument of the original, $n$-ary function. This form of an $n$-ary function is known as its \vocab{curried} form, and the process of transforming an uncurried function into a curried function is called \vocab{currying}.\footnote{The name is a reference to the logician Haskell B. Curry, though the idea appears to be due to Moses Sch\"{o}nfinkel.}

\subsubsection{From Reduction to Conversion}\label{untyped:conversion}
Let us now step back a ways to where we had just defined $\beta$-re\-duc\-tion. $\beta$-re\-duc\-tion is a one-way process: it can never make a term more complicated than it was before, though we cannot go all the way to claiming that $\beta$-re\-duc\-tion always results in a less complicated term. For example, $(\lambda x.xx) (\lambda x.xx)$ always and only $\beta$-reduces to itself and thus becomes neither more nor less complicated: $(\lambda x.xx) (\lambda x.xx) \betared \replace{x}{xx}{\lambda x. xx} = (\lambda x.xx) (\lambda x.xx) \betared \dotsb$.

But $\beta$-re\-duc\-tion does relate terms: one term is related to another if it can eventually $\beta$-reduce to that term. In some sense, all terms that are the result of $\beta$-re\-duc\-tion of some other term are related in that very way. We thus name this relation by saying that such terms are \vocab{$\beta$-con\-vert\-i\-ble}, so that if $M \betared[*] N$, or $N \betared[*] M$, or $L \betared[*] M$ and $L \betared[*] N$, or $M \betared[*] P$ and $N \betared[*] P$, then $M$ and $N$ (and additionally $L$ and $P$, if such is the case) are $\beta$-con\-vert\-i\-ble.\footnote{To be more exact, $\beta$-con\-ver\-sion is the equivalence relation generated by many-step $\beta$-re\-duc\-tion.} This is illustrated in Fig.~\ref{untyped:betaconv}, p.~\pageref{untyped:betaconv}. We notate this relation with a subscripted equals sign: $M \betacon N$. Note that, as a consequence of the definition of $\beta$-convertibility, $M \betacon M$ for all lambda terms $M$. 

(Now that we have introduced $\beta$-con\-ver\-sion, it is time to emend our earlier comments on $\alpha$-reduction. What we have called $\alpha$-reduction is more commonly, and more properly, called $\alpha$-con\-ver\-sion, since the relation between names is inherently bidirectional: $x$ converts to $y$ as readily as $y$ converts to $x$.)

\begin{figure}[btp]
\caption[$\beta$-con\-ver\-sion]{$\beta$-con\-ver\-sion\\
Letters represent lambda terms. A directed arrow $M \to N$ means that $M$ $\beta$-reduces to $N$ in some number of steps, that is, $M \betared[*] N$.
}
\label{untyped:betaconv}
\myfloatalign
\begingroup
\SelectTips{eu}{10}
\setlength\extrarowheight{15pt}%default length is 3.0pt
\begin{tabular}{cc}
\tableheadline{From this\dots} &\tableheadline{we deduce this.}
\\
\xymatrix{M \ar[r] &N}
& 
$M \betacon N$
\\
\xymatrix{&L \ar[dl] \ar[dr]\\
M & & N} & $M \betacon L \betacon N$\\
\xymatrix{M \ar[dr] & & N \ar[dl]\\
& P}
&
$M \betacon P \betacon N$
\\
\xymatrix{
&L \ar[dl] \ar[dr]\\
M \ar[dr] & &N \ar[dl] %\ar@{=>}[rr] & & {L \betacon M \betacon N \betacon P}
\\
&P
}
&
$L \betacon M \betacon N \betacon P$
\end{tabular}
\endgroup
\end{figure}

\paragraph{The Church--Rosser Theorem}An important property of the \lambdacalc is related to this. It is known as the \vocab{Church--Rosser theorem}, and it says that the \lambdacalc together with $\betared$ has two properties.
\begin{itemize} 
\item Firstly, it says that if any lambda term can be reduced in one or more steps to two different lambda terms, then it is possible to reduce each of those lambda terms in some number of steps to the same term. More symbolically, this can be put as follows: if $M \betared[*] M_{1}$ and $M \betared[*] M_{2}$, then there exists an $M_{3}$ such that $M_{1} \betared[*] M_{3}$ and $M_{2} \betared[*] M_{3}$. What this means graphically is that, given the figure from the second row of the table in Fig.~\ref{untyped:betaconv}~(p.~\pageref{untyped:betaconv}), we can infer the existence of the term $P$ in the the last row of that table.
\item Secondly, it states that if two terms $M$ and $N$ are $\beta$-con\-vert\-i\-ble into each other, then there is some other common term $P$ to which the first two can both be reduced. This is to say that, if $M \betacon N$, then we can find some $P$ such that $M \betared[*] P$ and $N \betared[*] P$ as well.
\end{itemize}

The first of these properties is known as the \vocab{Church--Rosser property}. The second property does not have a name of its own. Since the Church--Rosser theorem is so important, we will sketch a proof that it holds for the pure untyped \lambdacalc{}. We defer a discussion of the theorem's implications for the \lambdacalc till after we have introduced the idea of a normal form.

There are numerous proofs of the Church--Rosser theorem. The one sketched here is due to Tait and Martin--L{\"o}f by way of Barendregt.\footnote{See \citet[\S 3.2]{Barendregt:The-Lambda:1984}.} After proving a set of lemmas, the Church--Rosser theorem becomes a simple corollary.

We begin by defining the \vocab[diamond property]{diamond property.} A binary relation~$\rightarrowtail$ on lambda terms satisfies the diamond property if for all lambda terms~$M$, $M_{1}$, and $M_{2}$, such that $M \rightarrowtail M_{1}$ and~$M \rightarrowtail M_{2}$, there also exists a term~$M_{3}$ such that both $M_{1} \rightarrowtail M_{3}$ and $M_{2} \rightarrowtail M_{3}$. In a reduction diagram, the term~$M_{3}$ appears to complete the diamond begun by~$M \rightarrowtail M_{1}$ and $M \rightarrowtail M_{2}$; this is illustrated by Fig.~\ref{diamond} on page~\pageref{diamond}.

\begin{figure}[btp]
\caption[The diamond property]{The diamond property\\
The binary relation~$\rightarrowtail$ satisfies the diamond property if, for every three terms~$M$, $M_{1}$, and $M_{2}$ such that $M \rightarrowtail M_{1}$ and $M \rightarrowtail M_{2}$, there exists some fourth term~$M_{3}$ such that $M_{1} \rightarrowtail M_{3}$ and $M_{2} \rightarrowtail M_{3}$. In this diagram, solid arrows indicate assumed relations, while dashed arrows indicate inferred relations.%
}
\label{diamond}
\myfloatalign
\begingroup
\SelectTips{eu}{10}
\entrymodifiers={++}
\centerline{\xymatrix@!{
&M \ar@{>->}[dl] \ar@{>->}[dr]\\
M_{1} \ar@{>-->}[dr] & & M_{2} \ar@{>-->}[dl]\\
&M_{3}
}}%
\endgroup
\end{figure}

From this definition, it is clear that, if we can prove that $\betared[*]$ satisfies the diamond property, then we have proved that $\betared[*]$ has the Church--Rosser property. The first lemma shows that, if a binary relation $\rightarrowtail$ on a set (such as $\beta$-re\-duc\-tion $\betared$ is on the set of lambda terms) satisfies the diamond property, then so too does its transitive closure $\rightarrowtail^{*}$. This is suggested by the diagram of Fig.~\ref{diamondchase} on page~\pageref{diamondchase}.

\begin{figure}[btp]
\caption[Transitive diamonds]{Transitive diamonds\\
No matter how many $\rightarrowtail$ steps $\rightarrowtail^{*}$ might put $M_{1}$ and $M_{2}$ away from $M$, repeated application of the diamond property of $\rightarrowtail$ lets us show that its transitive closure $\rightarrowtail^{*}$ also satisfies the diamond property: there is always some $M_{3}$ such that both $M_{1} \rightarrowtail^{*} M_{3}$ and $M_{2} \rightarrowtail^{*} M_{3}$.%
}
\label{diamondchase}
\myfloatalign
\begingroup
\SelectTips{eu}{10}
%\entrymodifiers={++}
% One step one way, two the other: we can still bring them together using the diamond property. Fits in a 4 by 4 grid.
\centerline{\xymatrix@!{
&M
\ar@{>->}[dl] \ar@{>->}[dr]
\ar@/_1pc/@<-1ex>@{>->}_>{*}[dl] \ar@/^1pc/@<1ex>@{>->}^>{*}[ddrr]\\
M_{1} \ar@/_1pc/@<-1ex>@{>-->}[rrdd]_>{*} \ar@{>-->}[dr] & & \bullet \ar@{>->}[dr] \ar@{>-->}[dl]\\
& \bullet \ar@{>-->}[dr] && M_{2} \ar@{>-->}[dl] \ar@/^1pc/@<1ex>@{>-->}^>{*}[dl]\\
&&M_{3}
}}%
\endgroup
\end{figure}

This lemma is not quite what we need. Single-step $\beta$-re\-duc\-tion~$\betared$ is not reflexive, but many-step $\beta$-re\-duc\-tion is. Many-step $\beta$-re\-duc\-tion~$\betared[*]$ is in fact the \emph{reflexive} transitive closure of~$\betared$. The solution to this mismatch is to define a reflexive binary relation on lambda terms similar to~$\betared$ such that many-step~$\beta$-re\-duc\-tion is this new relation's transitive closure. Once we prove that this new relation has the diamond property, we have proven that its transitive closure~$\betared[*]$ has the Church--Rosser property.

Once we have that many-step $\beta$-re\-duc\-tion $\betared[*]$ satisfies the diamond property, it becomes simple to prove that $\beta$-con\-vert\-ible terms can be $\beta$-re\-duced to a common term (the second property specified ): The result follows readily from the definition of $\betacon$.

\subsubsection{Normal Forms}\label{untyped:nf}
Now that we have defined lambda terms and $\beta$-re\-duc\-tion, we can give a normal form for lambda terms. This is important if we are to explain the impact of the Church--Rosser theorem. A lambda term is said to be in \vocab{normal form} if it cannot be further $\beta$-reduced, that is to say, the lambda term $M$ is in normal form if there is no $N$ such that $M \betared N$.

Not all terms have normal forms. The lambda term $(\lambda x.xx)(\lambda x.xx)$, sometimes called $\Omega$, has no normal form, since it reduces always and only to itself. Some terms do have normal forms, but it is possible to $\beta$-reduce the term an arbitrary number of times without reaching this form. What this means practically that it is important to select the right \vocab{reducible expression (redex)} to $\beta$-reduce, otherwise one might continue $\beta$-reducing without ever terminating in the normal form. It is easy to produce examples of such terms by throwing $\Omega$ into the mix: $(\lambda xy.y) \Omega z$ has as its normal form $z$, but of course one could repeatedly select $\Omega$ for reduction and thereby never realize this. Terms that \emph{always} reduce to normal form within a finite number of $\beta$-re\-duc\-tions, regardless of the reduction strategy employed, are called \vocab{strongly normalizing}.

Now that the concepts of normal forms and strongly normalizing terms is clear, we can explicate the Church--Rosser theorem. Altogether, it means that, if a term has a normal form, then regardless of the reduction steps we use to reach that form, it will always be possible from anywhere along the way to reach the normal form: there's no way we can misstep and be kept from ever reaching the normal form short of intentionally, repeatedly making the wrong choice of expression to reduce. If the term is strongly normalizing, we can go one better and state that the reduction steps we use to reach its normal form are completely irrelevant, as all chains of reductions will eventually terminate in that normal form. This also shows that the normal form is unique, for if $N_{1}$ and $N_{2}$ were two distinct normal forms of a given term, then they would have to share a common term $N_{3}$ to which they could both be $\beta$-reduced.

\subsubsection{Recursion and $Y$}\label{untyped:y}
The pure, untyped \lambdacalc is Turing complete. An important part of achieving this degree of expressive power is the ability to make recursive definitions using the \lambdacalc{}. The way to do so is surprisingly succinct. One common means is what is known as the \vocab{paradoxical combinator}, universally designated by $Y$. Here is its definition:
\[
Y = \lambda f. (\lambda x.f(xx))(\lambda x.f(xx))
\]
It is also known as the \vocab{fixed-point combinator}, since for every $F$, we have that $F(YF) \betacon YF$.\footnote{$Y$ is not unique in producing fixed points. Turing's fixed point operator $\Theta = (\lambda a b . b (aab)) (\lambda a b . b (aab))$ will do just as well.} Let us see how exactly this works:
\begin{align*}
Y F &\equiv (\redex{\lambda f}. (\lambda x.\redex{f}(xx))(\lambda x.\redex{f}(xx))) \redex{F} \\
&\betared (\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))}\\
&\betared F ( (\lambda x. \redex{F}(xx)) (\lambda x. \redex{F}(xx)) )\\
&\betacon F (\redex{(\lambda f. (\lambda x.f(xx))(\lambda x.f(xx)))} F)\\
&\equiv F (Y F)
\end{align*}
At each step, we have used color to indicate the terms that are involved in producing the next step. We have indicated at each step whether syntactic equivalence, $\beta$-re\-duc\-tion, or full-fledged $\beta$-con\-ver\-sion was employed. As you can see, we begin by using simple $\beta$-re\-duc\-tion. The breakthrough that permits us to arrive at the desired form is replacing the two instances of $F$ in the inner term $((\dotsb F \dotsb)(\dotsb F \dotsb))$ by the application of $\lambda f. (\dotsb f \dotsb)(\dotsb f \dotsb)$ to $F$.

If you look at the steps leading up to that abstraction, you can see how $Y$ leads to recursion. Let us carry this process out a bit further:
\begin{align*}
Y F &\equiv (\redex{\lambda f}. (\lambda x.\redex{f}(xx))(\lambda x.\redex{f}(xx))) \redex{F} \\
&\betared (\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))}\\
&\betared F (\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))}\\
&\betared F ( F ( ((\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))} ))\\
&\betared F ( F ( F ( ((\redex{\lambda x}. F(\redex{x}\redex{x})) \redex{(\lambda x. F(xx))} )))\\
&\hphantom{\rightarrow}\vdots
\end{align*}
As you can see quite plainly, $YF$ leads to repeated self-ap\-pli\-ca\-tion of $F$. This iteration of $F$ is how it produces a fixed point.%CRIT-TODO: Does it ALWAYS reach a fixed point? Or only sometimes?

\subsubsection{A Brief Word on Reduction Strategies}\label{untyped:strategies}
Before we move on to extend the \lambdacalc with some concepts a bit more elaborate and convenient than the low-level but elegant pure, untyped \lambdacalc{}, it behooves us to put in a brief word about reduction strategies.

In the context of the \lambdacalc{}, we call them reduction strategies. In the context of programming languages in general, we use the phrase \vocab{order of evaluation}. They both come down to the same thing: where do we want to focus our efforts? And, perhaps more importantly, where should we focus our efforts? 

The choices we make in terms of \lambdacalc reduction strategy have equivalents in terms of order of evaluation of functions and their arguments. The fundamental question is this: should we evaluate the arguments before passing them on to the function, or should we call the function and just point it at its arguments so it has access to them and their values as needed?

If we first deal with the arguments and only then with the function as a whole, then we are employing a \vocab{call-by-value} evaluation strategy, so called because the function call takes place with the values of the arguments provided to the function. The arguments are evaluated, and the resulting value is bound to the formal parameters of the function.

If we instead begin by evaluating the body of the function itself and only evaluate the arguments as necessary, we are pursuing a \vocab{call-by-name} evaluation strategy. Call-by-name gets its name from the way that the formal parameters are bound only to the names of the actual arguments. It is only when the value of one of the arguments is required to proceed with evaluating the function's body that the argument is evaluated.

The \lambdacalc equivalent of call-by-value is known as the \vocab{applicative order} reduction strategy. We can describe the applicative order strategy quite simply: reduction is first performed in the term to which the abstraction is being applied. Only when we have exhausted this possibility do we perform the application.

If we strengthen this preference into a hard and fast rule that no $\beta$-re\-duc\-tion is to be performed under an abstraction, then we can define an alternative normal form, known as \vocab{weak normal form}:
\begin{itemize}
\item Variables are defined to be in weak normal form.
\item An application $MN$ is in weak normal form if and only if both $M$ and $N$ are in weak normal form.
\item An abstraction $\lambda x . M$ is in weak normal form.
\end{itemize}
It is in the treatment of abstractions that weak normal form differs from normal form: weak normal form considers all abstractions $\lambda x . M$ to already be in weak normal form, while normal form requires that the term $M$ being abstracted over also be in normal form.

The \lambdacalc equivalent of call-by-name, on the other hand, is known as the \vocab{normal order} reduction strategy and corresponds to always select the leftmost--outermost reducible expression for reduction. Normal order is so called because, if the term has a normal form, we can \textit{always} reduce it to normal form by employing the normal order reduction strategy.%TODO: Sketch a proof that normal order reduction always results in normal form if it exists, as well.
The same cannot be said for applicative order, which can fail to reduce a term to normal form even when one exists. Again, the $\Omega$ term makes it easy to give an example. Something as simple as $(\lambda x y. y) \Omega z$ suffices. Under normal order evaluation, we first $\beta$-reduce $(\lambda x y. y) \Omega \betared (\lambda y. y)$ and then apply the result to $z$, giving the normal form $z$. With applicative order evaluation, however, we begin by $\beta$-reducing the first argument, $\Omega$\empause and that is as close as we shall ever get to the normal form $z$, since $\Omega$ has no normal form.

\subsubsection{Strictness}\label{untyped:strictness}
If a computation never terminates, as with the attempt to $\beta$-reduce $\Omega$ to normal form, then we say that it \vocab{diverges}. An important concept for functions is that of \vocab{strictness}. We say a function is strict in a given parameter if the evaluation of the function itself diverges whenever the evaluation of the parameter diverges. The function $\lambda x y. y$ given above is strict only in its second argument, $y$, since we can evaluate the function even when a divergent term is substituted for $x$, as recently demonstrated. A function that is strict in all arguments is called a \vocab{strict function}. The function $\lambda x. x$ can readily be seen as strict, since this function is simply the identity function.

\subsubsection{\texorpdfstring{$\eta$-Conversion}{Eta-Conversion}}\label{untyped:eta}
Strictness is important to understanding the appropriateness of a different sort of convertibility relationship between terms. This type of con\-ver\-sion expresses the equivalence of a function expecting some number of arguments and a function that ``wraps'' that function and provides it with those arguments. We call two such expressions $M$ and $N$ \vocab{$\eta$-con\-vert\-i\-ble}, written $M \etacon N$, and we define $\eta$-con\-ver\-sion as
\[
\lambda x. F x \etacon F,\quad x \notin \FV{F}\text{ .}
\]
The last part, $x \notin \FV{F}$, should be read as ``where $x$ is not among the free variables of $F$'' and serves to exclude abstraction over variables that would capture a free variable in $F$ from being defined as $\eta$-con\-vert\-i\-ble with $F$.\footnote{If you are familiar with the concept of \asword{extensional equality,} you should have no trouble remembering the definition of $\eta$-con\-ver\-sion if you think of it as ``$\eta$ for \asword{extensional.}''}

$\eta$-con\-ver\-sion is useful because it identifies a host of iden\-ti\-cally-be\-having terms. We can produce an infinite number, even when we identify $\alpha$-con\-vert\-i\-ble expressions, starting with something as simple as the identity $\lambda x. x$. All we need do is repeatedly abstract this term with respect to $x$. This gives rise to the sequence $I$ of terms 
\begin{align*}
I_{1} &= \lambda x.x\\
I_{2} &= \etify{x}{\lambda x.x}\\
I_{3} &= \etify{x}{\etify{x}{\lambda x.x}}\\
I_{4} &= \etify{x}{\etify{x}{\etify{x}{\lambda x.x}}}\\
&\hphantom{=}\vdots
\end{align*}
Since $x$ is always bound within the body of the abstraction, this causes no problems with variable capture. But all the terms in this sequence behave the same way; in fact, the application of any one of them to a term $M$ is $\beta$-con\-vert\-i\-ble with the application of any other to the same term $M$ and, ultimately, with $M$ itself: $I_{n} M \betared I_{n-1} M \betared \dotsb \betared I_{1} M \betared M$.

But $\eta$-con\-ver\-sion might nevertheless be an inappropriate notion of equality in some cases. If we are reducing only to weak normal form, for example, then reduction of $\lambda x . \Omega$ terminates immediately, while reduction of $\Omega$ will never terminate. The introduction of types can also pose problems for the notion of $\eta$-con\-ver\-sion. But where we can employ $\eta$-con\-ver\-sion, we can at times drastically reduce the number of substitutions required due to $\beta$-re\-duc\-tion by simplifying the terms involved using $\eta$-con\-ver\-sion first. In the end, whether we decide it is appropriate or not to add $\eta$-con\-ver\-sion to our \lambdacalc{}, the operational equivalence between terms that it highlights is well worth keeping in mind.
