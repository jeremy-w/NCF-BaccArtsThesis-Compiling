\myChapter{Compiling}\label{functional:compiling}
\section{From Interpreters to Compilers}
Higher-level imperative languages in a sense grew out of assembly language. They were designed with compilation in mind. Functional languages, however, have a history of interpretation. Lisp began solely as an interpreted language, and \abbrev{ML} programs were first run using an interpreter written in Lisp. It was not immediately clear how to compile such languages. The interpreters supplied a programming environment (for example, the interpreter might support both editing and debugging of code from within the interpreter) and were used interactively. The interpreter managed the runtime system, including in particular \vocab{garbage collection}, which freed space allocated to data that had become useless. Later, such interpreters were extended to support on-the-fly compilation of function definitions.

When it comes to functional languages, the distinction between interpreter and compiler becomes blurry. Interpreters can perform compilation, and compilers for functional languages frequently provide an interactive interface in addition to simple compilation facilities. When a program written in a functional language is interpreted, the interpretation manages garbage collection, storage allocation, and other such issues. All this bookkeeping is the province of the \vocab{runtime system}, and it does not go away simply because one wants to compile a program instead of interpret it. In order to work, the compiled representation of a program written in a functional language must be coupled with a runtime system. It is a short step from providing each compiled representation with a runtime system to providing each with both a runtime system and its own, private interpreter. This code--runtime-system--interpreter package is self-sufficient: it is directly executable. Such a package is, in fact, what some compilers for functional languages produce.

Other implementations offer the option of compiling a program to a bytecode representation for interpretation afterwards by a virtual machine. The same bytecode program can then be run on a variety of platforms, so long as its minimal platform\empause the virtual machine\empause is available to host it.

\section{The Runtime System}
The runtime system is an essential part of compiling functional languages. It provides a common set of functionality needed by all compiled programs. The two most critical services it provides are primitive operations and storage management. Primitive operations are what actually perform the computation specified by $\delta$-rules. They also enable programs to interface with their environment by providing access to functionality related to the operating system. Storage management is essential, since space is allocated as needed and must be reclaimed in order to prevent excessive memory use. The run time system might also handle threading, bytecode interpretation, and runtime linking of object code. Because the runtime system oversees primitive operations and memory management, it also plays an important part in profiling the runtime behavior of the compiled code.

Every compiler for functional languages will contain a run time system of some sort. A runtime system provides several distinct, major services. Storage management can become quite involved; primitive operations are a necessary part of producing working code. Since each of these services can to some degree be dealt with independently of the others, the compiler's structure may not reflect a concept of a runtime system as such, but the basic services will nevertheless be in place and recognizable; they simply will not be grouped together.

\section{The Problem}
We have seen that functional languages are based on the \lambdacalc and provide the programmer with a variety of higher-level abstractions such as algebraic data types, pattern matching, and higher-order functions. These have few parallels in imperative languages, and they create new problems for compilation.

New abstractions are not the only source of problems encountered when compiling functional languages. We compiled imperative languages by progressively lowering the level of abstraction of the program's representation till finally we were representing the program in machine instructions. If we try the same lowering process with a functional language, we run into a snag: instead of bottoming out in machine language, we bottom out in the \lambdacalc{}. Functional languages are based on a model of computation fundamentally different from the von~Neumann model at the root of the imperative languages. To compile a functional language, we must somehow model the \lambdacalc{}'s computation-through-reduction using the von~Neumann computer that is the target platform.

We will return to these problems as we go through the phases of compilation.

\section{The Front End}
The problems confronted by the front end do not change when we move from imperative to functional languages. However, if we choose to implement the front end in the functional language itself, we can take advantage of the abstractions offered by functional languages in constructing the lexer and parser. If we instead implement generators for lexers and parsers using the functional language, it becomes a simple matter to integrate lexing and parsing into a compiler written in the language itself. Other programs written in the language then also have ready access to a lexer and parser. Perhaps this goes some way to explain why many compilers for functional languages are distributed along with both a lexer generator and parser generator written in the language of the compiler.\footnote{That this allows the compiler writer to avoid the complexities associated with defining and then using a foreign-function interface to programs produced using an imperative language could also be a motivating factor.}

As you might imagine, semantic analysis takes on a new importance in languages where type inference is taken for granted and the programmer can create new data types. Type inference can be treated as a pass in itself. Type inference replaces type checking, since once the compiler has reconstructed a valid type for a term, the type has been checked. If the term cannot be assigned a valid type, type inference has failed: either the program is not well-typed, or the programmer must supply type annotations for some term that is valid but for which a type cannot be inferred.

\section{Intermediate Representations}
Compilers for functional languages employ some \IRs not used by imperative language compilers. Functional languages are on the whole a sugaring of the \lambdacalc{}, and so it is possible to represent a program written in a functional language using smaller and smaller subsets of the full language. Thus, source-to-source transformations, where code in the source language is rewritten in the source language in an altered form, play a more important role in functional languages than is common in imperative languages. Transformations into a core language are in fact sometimes used in definitions of functional languages in order to explain the behavior of more complex constructs.

Just as programs represented in imperative languages are translated into \SSA form because this facilitates static analysis, optimization, and proof of a given optimization's correctness, it was popular around the 1980s to translate a functional language program into \vocab{continuation passing style (CPS)}. In \CPS{}, control flow and argument passing is made explicit. Every function is augmented with a further argument that serves as the \vocab{continuation}. The function is then called with another function that is to use the result of its computation as the continuation argument. Rather than returning the result \lstinline{x} of evaluating the function to a caller, the function instead invokes the continuation with \lstinline{x} as the continuation's argument. In compiling call-by-value languages, translation into \CPS has been proven to enable more transformations than are possible in the source language.

However, since the translation to \CPS is ultimately reversed during code generation, recent compilers have moved to carefully performing some of the transformations developed for use with \CPS directly in the source representation. \CPS is still used locally for some optimizations in a process known as ``contification'' or local \CPS conversion. This can be used alongside \SSA to enable further optimizations during functional language compilation.

Graph representations of the program also play a bigger part in some compilers. A large class of compilers build their back end around graph representations of the program; reduction is performed in terms of the graph. The development of such \vocab{graph reduction} machines played an important part in making lazy evaluation feasible, since they provide a ready way to conceive of substitution in reduction without copying. If all terms are represented by a collection of linked nodes, rather than copying the term to each location in order to substitute it, we instead make multiple links to the single original term. When one of the substituted terms is reduced, all terms immediately share the result: no reduction is performed more than once.

Some compilers employ \emph{typed} intermediate languages. This allows them to use the additional information provided by types throughout compilation. Instead of simply performing type checking to ensure the program is valid and subsequently ignoring types, the type of terms becomes additional fodder for analysis and optimization.

\section{The Middle End}
Just as in imperative compilers, the middle end is where the most effort is expended. Optimization is the key to producing good code and a good compiler. (Naturally, different kinds of optimization will be required depending on your idea of good.) Compilers for functional languages are typically the subjects and results of research. Different compilers are frequently based around entirely different \IRs and back ends, so work on optimization for functional languages is much more balkanized than research on optimization for imperative languages. Optimizations described in the literature are generally described in terms of improvements of an existing compiler in the context of a particular language, set of intermediate representations, and back end. It is not always clear which parts of this work applies in general to functional language compilation, and which parts are inextricable from the particular context in which they were developed.

While the particular optimizations that can be performed might differ from compiler to compiler, all compilers for functional languages confront a set of common problems due to the features that modern functional languages offer. These problems are partially addressed through enabling and performing specific kinds of optimizations and partially through design of the back end. They can also be addressed through extensions to the language itself that allow the programmer to provide further information to the compiler.

Compiler-specific language extensions are not confined to functional language compilers, of course. An imperative example would be a C compiler adding support for an equivalent of the \lstinline[morekeywords=restrict]{restrict} keyword added by the C99 standard prior to the standard's publication. The \lstinline[morekeywords=restrict]{restrict} keyword is a type qualifier meant to be used with pointers. It is used to declare that the object pointed to by the pointer will be accessed only through the pointer. This shortcuts the need for the compiler to perform difficult alias analysis to determine whether this is the case by allowing the programmer to advise the compiler that this relationship between the pointer and the pointed to holds. More importantly, this allows the programmer to declare that this restricted relationship holds even when the compiler would be unable to infer the relationship through analysis, which enables previously impossible optimizations.

Such extensions are not without peril. The \lstinline[morekeywords=restrict]{restrict} keyword also provides one more way for C programmers to shoot themselves in the foot. If an optimization relies on the fact that a pointer is declared with the \lstinline[morekeywords=restrict]{restrict} type qualifier, but the relationship indicated by the qualifier in fact does not hold, then optimization could introduce erroneous behavior into an otherwise correct program. The same difficulty is a matter of concern for other extensions that provide information relied on in optimization that cannot be verified independently through analysis; at the same time, an extension that does not also extend the potential for optimization would be redundant.

\subsection{Common Problems}
Whether the compiler chooses to extend the language or not, it still faces some common problems.
\begin{itemize}
\item
First-class functions require the construction of closures (which are defined below). A lazy evaluation strategy requires the creation of even more closures.

\item 
The immutability required to preserve referential transparency can require significant amounts of copying. For example, sorting a list recursively produces a multitude of new lists. Lists can be expensive to construct and manipulate, but they are used extensively in functional programming, as are algebraic data types and pattern matching in general.

\item
Polymorphism is desirable, but it also requires that all arguments be treated identically regardless of their type: no matter whether an argument is an integer or a list, it has to fit in the same argument box.
\end{itemize}

\begin{lstlisting}[float,caption={[Creating a closure]Creating a closure\\
A closure is a function together with an environment providing bindings for the function's free variables. The binding used for each variable is the one lexically closest to where the function is defined. In this example, the variable \code{n} is free in the definition of \code{addNto}. The closest definition of \code{n} is that made by \code{makeAdder}. Thus, evaluating \code{makeAdder 3} results in a closure containing the function \code{addNto} and an environment in which \code{n} is bound to \code{3}.},label={closure}]
makeAdder n = addNto
    where addNto m = n + m
\end{lstlisting}

\subsubsection{Closures and Suspensions}
A \vocab{closure} is formed by taking a function definition and binding any free variables to their existing definition in the closest enclosing (generally lexical) environment. The code in Listing~\ref{closure}~(p.~\pageref{closure}) returns a closure that can be used to produce a function that always adds two to its argument:
\begin{lstlisting}
add2to = makeAdder 2
\end{lstlisting}
With this definition, evaluating \lstinline{map add2to [1, 2, 3]} results in \lstinline{[3, 4, 5]}. Note that the definition of the function \lstinline{addNto} uses a variable \lstinline{n} that is not passed to it. This variable is defined in the immediately enclosing environment of \lstinline{makeAdder}. When \lstinline{makeAdder 2} is evaluated, \lstinline{n} is bound to 2 and a closure of \lstinline{addNto} is returned wherein \lstinline{n} is bound to the value \lstinline{n} had when the closure was created. Evaluating \lstinline{makeAdder 3} results in a closure where \lstinline{n} is bound to 3. If there were no definition for the free variable \lstinline{n} in the function definition, it would be impossible to produce a closure and the definition would be in error. This would be the case if we attempted to define \lstinline{g x = x + y} in an environment without any binding for \lstinline{y}.

A closure could at times be formed by partially evaluating the closure by directly substituting the definition, particularly in eagerly evaluated languages, but it is tricky to ensure this specialization of the function takes into account the already available definitions and preserves the semantics of evaluation of the unspecialized function. Instead, a closure is most often implemented as an unevaluated function together with its own environment of definitions. Only once all arguments have been provided to the function will evaluation actually occur. In this sense, a closure represents a frozen, or suspended, computation: a promise to perform some evaluation once all arguments are available to the function. Dealing with closures efficiently thus becomes an important part of enabling heavy use of higher-order functions in programs written in a functional language\empause and any functional language that encourages currying encourages frequent use of higher-order functions.

Since lazy languages only evaluate a term when necessary, they must make extensive use of suspended computations and only force their evaluation as needed. Optimizing the implementation of such suspensions thus becomes an important part of optimizing a compiler for a lazy language. Indeed, a common optimization is to introduce \vocab[strictness analysis]{strictness analysis,} which attempts to eliminate the construction of suspensions that will perforce be evaluated in the course of evaluating the program. As an example, a request to display the result of a computation requires that the entire computation be carried out to produce the result. There is no question of some part of the result not being required, since the entire result is supposed to be output. Such a display function is strict in its argument.

\subsubsection{Referential Transparency and Copies}
The immutability required to preserve referential transparency can require significant amounts of copying. For example, sorting a list recursively produces a multitude of new lists. This has nothing to do with strictness. The solution to this problem is a combination of deforestation, also called fusion, and update analysis. \vocab[deforestation (fusion)]{Deforestation} attempts to eliminate  data structures that are created only to be immediately consumed. This can be considered to some extent as a special case of \vocab[update analysis]{update analysis,} which attempts to discover when functions accepting a data structure and returning a modified copy of that data structure can be implemented so that they instead update the original data structure without producing a copy. This can be done whenever the original data structure will not be accessed in the future. With this requirement satisfied, the in-place, destructive update can be done without destroying referential transparency, since there are no remaining references through which the update of the original data structure could be discovered. Mutable references, such as are allowed by the \ML family of languages, might seem a way to allow the programmer to directly intervene to solve this problem, but mutable references begin to return us to the complications of static analysis that we encountered when discussing imperative languages.

Pattern matching plays an important part in modern functional languages. A na{\"i}ve implementation of pattern matching that goes through the patterns case by case, as we described the process of pattern matching earlier, is needlessly slow. More sophisticated implementations (using, for example, decision trees) can do much better.

\subsubsection{Polymorphism and Boxes}
In order for a function to be parametrically polymorphic, it must be able to accept any argument, regardless of the argument's type. For this to work, every argument must be superficially similar. Polymorphism forces a single, standardized representation of all arguments. Frequently, this takes the form of a pointer to heap-allocated structures with a common layout. Even arguments that could be directly represented, such as integers or floating point numbers, end up being allocated on the heap in order to look like all the other arguments.

Such a common representation is called a \vocab[boxed representation]{boxed representation} because we can think of putting every data type into a box that makes them all look the same. To actually use the data, we must unbox it; when we are done, we must box it again. This boxing can be expensive. The way to lessen this expense is to work at relaxing the constraint that required boxing in the first place by allowing functions to deal with \vocab{unboxed} arguments. Such arguments are cheaper to allocate and cheaper to work with and can lead to significant gains in efficiency. Enabling manipulation and use of unboxed arguments, and introducing an analysis that can discover when it is possible to substitute unboxed data for boxed data, is an important optimization for languages with many polymorphic functions. Unboxed types can even be added directly to the language, which allows the programmer to directly manipulate unboxed types when efficiency is of particular concern.

\section{The Back End}
The middle end used analysis to optimize the representation of the program in preparation for translation to run on a von~Neumann machine. The back end is responsible for effecting this translation. This translation consists in bridging the functional model of computation and the imperative model of computation. Conceptually, this is done by simulating functional computation-by-reduction in an imperative, von~Neumann setting.

This simulation is typically performed by an \vocab[abstract machine]{abstract machine.} There are two criteria by which an abstract machine for a functional language should be judged~\citep[p.~184]{Peyton-Jones:The-Spineless:1989}:
\begin{itemize}
\item How well it supports the functional language.
\item How effectively it can be mapped onto a real machine.
\end{itemize}

The ultimate goal of the back end and of the abstract machine is code generation. This can take the form of either code native to the given platform, or it can take the form of very low-level C code. When C code is the target language, it is in order to use the C compiler as a portable assembler. While the native assembly language can vary significantly between different platforms, the C language does not. Generating C code makes it easier to port the compiler to a new platform, since almost every platform will already have a working C compiler. Code produced in this way cannot compete with directly generated native code, but such a comparison misses the point of using C as target language. This choice is motivated by a desire to have a working compiler on as many platforms as possible as soon as possible. Native code generation can always be added later, but code generation via C allows the compiler to produce code for unanticipated platforms.

\subsection{Types of Abstract Machine}
A large variety of abstract machines has been proposed. We can roughly divide these machines into three kinds:
\begin{itemize}
\item
stack machines

\item
fixed combinator machines

\item
graph reduction machines

\end{itemize}

\subsubsection[Stack]{Stack Machines}
Stack machines, such as the Functional Abstract Machine (\abbrev{FAM})~\citep{Cardelli:Compiling:1984} and the \abbrev{SECD} (stack, environment, code, dump) machine~\citep{Landin:The-Mechanical:1964}, work by compiling the low-level intermediate language into stack instructions. The instruction set is customized to the functional language. Use of stack instructions makes it simple to apply peephole optimization to refine the stack code produced.

Stack machines are characterized by their representation of the code as stack instructions. They might also use a variety of other stacks: environment, control flow, and data. The environment stack stores environments of bindings mapping names to values. The control flow stack keeps track of the order of operations and is used to resume evaluating an expression after a detour to evaluate one of its subexpressions. The data stack is where data structures are allocated and stored and provides a way to access these structures, as well. Some of these other components might also be implemented as stacks in other kinds of abstract machines.

\subsubsection[Fixed Combinator]{Fixed Combinator Machines}
Fixed combinator machines eliminate the need to maintain an environment by transforming the entire program into a fixed set of combinators applied to known arguments. The bindings that would have been provided by the environment are instead made explicit through function application. The chosen set of combinators varies from machine to machine; a frequent subset of the chosen combinators are the $S$, $K$ and $I$ combinators, which are defined in terms of the \lambdacalc as
\begin{align*}
S &= \lambda xyz. xz (yz)\\
K &= \lambda xy. x\\
I &= \lambda x. x
\end{align*}
The elimination of the environment is elegant, as is the very small number of primitive routines (one for each of the chosen combinators), but the code size can grow tremendously, and, since each combinator only performs a minimal amount of work, it requires the evaluation of many small combinators to accomplish anything. Thus, fixed combinator machines have a high functional call overhead. Adding more combinators that do more work to the fixed set of combinators can ameliorate this somewhat, but as different sets of combinators are more appropriate for different programs, the problem cannot be eliminated.

\subsubsection[Graph Reduction]{Graph Reduction Machines}
Graph reduction machines conceive of the program as a graph of function, argument, and application nodes. Reduction takes place in terms of the graph at application nodes; the result of evaluation replaces the application node. The final value of the program is obtained by reducing the graph to the root node.

Where stack machines used an environment, and fixed combinator machines used transformation into a fixed set combinators, graph reduction machines take a third route: they transform the entire program into a set of combinators by what is known as \vocab{lambda lifting}~\citep{Johnsson:Lambda:1985,Danvy:Lambda-Lifting:2002,Fischbach:Specification:2003}. These combinators are extracted from the program itself by introducing additional abstractions over the free variables of function definitions. As in the fixed combinator machines, an environment is unnecessary, since binding via lambda replaces binding via the environment. But the use of these custom combinators, known as \vocab{supercombinators}~\citep{Hughes:Super-combinators:1982}, avoids the problems associated with the fixed combinator machines.

Graph reduction machines naturally implement a call-by-need reduction strategy. Substitution occurs by substituting a pointer to the same, shared node. Whenever that node is evaluated and updated, all pointers immediately have access to the updated value. Thus, every node is reduced at most once; this at-most-once property is known as \vocab{full laziness}.

Unfortunately, building and maintaining the graph structure is expensive. Since reduction is modeled in terms of the graph, the program-as-graph ends up being interpreted at runtime, which also limits execution speed. Because of the problems posed by conventional architectures for these abstract machines, there were attempts to implement hardware that provided direct support for graph reduction (sometimes extending support to \emph{parallel} graph reduction).

Compiled graph reduction machines work around this problem in order to achieve good performance on conventional architectures. They replace an explicit graph structure with code that acts as if the graph were present. They thus preserve the conceptual simplicity afforded by viewing the program as a graph while avoiding the expense of building and updating a graph data structure. A variety of refinements and variations on this theme are possible~\citep{Johnsson:Efficient:2004,Burn:The-spineless:1988,Peyton-Jones:The-Spineless:1989,Reid:Putting:1999,Marlow:Faster:2007,Peyton-Jones:Call-pattern:2007}.

\subsection{The Abstract Machine Design Space}
We have briefly described abstract machines in terms of several different ways they encode the functional program and evaluate it. Another way to describe an abstract machine is in terms of the decisions made in its design. This requires giving an example of the other choices that could have been made.

As you might have deduced from our earlier discussion, the two most important choices are the evaluation strategy implemented and the way the environment is handled. The major decision for evaluation strategy is between call-by-value and call-by-name. Call-by-need evaluation is merely a variation on call-by-name that implements laziness.

There are many variations on these principal strategies in terms of how precisely they are implemented. The primary variations concern how function application is performed. The two options go by the suggestive names of push/enter and eval/apply. In the push/enter approach, a function call \code{f x y} is evaluated by pushing \code{x} and \code{y} on the stack and entering the code for \code{f}. It is up to \code{f} itself to determine whether sufficient arguments are available; if insufficient arguments are provided, the function returns a closure; if sufficient are available, it completes the application and returns the result. In the eval/apply model, the caller determines whether sufficient arguments are available and controls the application process. Both approaches can be used with any evaluation strategy; eval/apply has historically been favored for call-by-value and push/enter for call-by-name, but research~\citep{Marlow:Making:2004} suggests eval/apply is preferable in both cases. Surprisingly, this conclusion is reached, not on the basis of a difference in performance (the differences are negligible), but in a reduction in the complexity of the compiler that accompanies use of eval/apply.

Environment management can be done using explicit environments or via combinators. If explicit environments are used, there is a choice of the type of environment: should definitions be shared or copied? That is to say, when entering an enclosing lexical scope, should the closure direct accesses to free variables to enclosing closures, creating something of a tree-like structure, or should each closure receive copies of its needed values? Sharing eliminates time spent copying but increases the time spent traversing data structures to reach the point of variable definition; copying requires time and space be spent making copies, but each function closure then retains only needed bindings (there is no reason to copy over unneeded ones) and access to those bindings is possible in constant time.

If copying is chosen to implement environments, there is one more question: when should the copy be performed? Different abstract machines have different answers to this question. Some possibilities are: 
\begin{itemize}
\item when a function is entered; 
\item when a closure is built and again when it is entered; and 
\item only when a closure is built.
\end{itemize}
We will not discuss these further; the interested reader is referred to an article by~\citet{Douence:A-systematic:1998} and associated technical reports~\citep{Douence:A-Taxonomy:1995,Douence:A-Taxonomy:1995b}.

We mentioned that call-by-need can be seen as a variation on call-by-name. Call-by-name necessitates sharing and updating of closures. This updating could be performed in two ways, either by the caller or by the callee, but all implementations use callee-update since this prevents every caller of the same callee from having to test the callee to see whether it has been evaluated or not. Instead, the callee, if not previously evaluated, sparks its evaluation and updates itself with the result. The callee of course also returns the result to the caller that forced its evaluation.

Graph reduction machines can be seen as implementing these same strategies but in terms of graphs. The details of transformation between the two perspectives are beyond the scope of this discourse.

\section{Bibliographic Notes}
Garbage collection is an interesting topic in itself comprising a variety of algorithms with a variety of purposes and no apparent optimum approach. The state of the art as of 1995 is described in a textbook by~\citet{Jones:Garbage:1996}. An alternative to garbage collection is region inference, which is a static analysis that enables the compiler to hardcode at compile-time the work usually performed by a dynamic garbage collector~\citep{Tofte:A-region:1998}.

\Citet{McCarthy:History:1978} describes the creation of the first Lisp interpreter. The first \ML interpreter was implemented in Lisp~\citep[see][footnote~5]{Gordon:From:2000}. By the start of the 1990s, Lisp offered a sophisticated programming environment~\citep{Layer:Lisp:1991}. \Citet{Cardelli:Compiling:1984} describes how an \abbrev{ML} compiler was developed based on the ``implementation folklore'' of various Lisps rather than using the style advocated by compiler textbooks directed towards imperative languages. The incremental, on-the-fly compilation used by Lisp systems and some interactive compilers for functional languages is also known as \vocab{just-in-time compilation} and has its own interesting history~\citep{Aycock:A-brief:2003}. Virtual machines are also an active topic of research in themselves~\citep{Shi:Virtual:2008}, as is how they relate to functional languages and abstract machines~\citep{Danvy:A-journey:2003,Ager:A-functional:2003,Ager:A-Functional:2003a,Ager:A-Functional:2004,Ager:From:2003}.

A brief history of \CPS is given by~\citet{Flanagan:The-essence:2004}. While they suggest the argument over whether compiling primarily through \CPS is worthwhile has been settled against \CPS, \citet{Kennedy:Compiling:2007} at least believes \CPS provides distinct benefits in simplicity compared to other, later intermediate representations. The best resource on \CPS{}, at least as of the early 1990s, is \textit{Compiling with Continuations}~\citep{Appel:Compiling:1992}. Speaking of intermediate representations, we should note that \SSA can actually be seen as functional programming~\citep{Appel:Compiling:1992}.

The suspensions created to represent ``frozen'' computations during the compilation of lazy languages are also known as \vocab{thunks}~\citep{Ingerman:Thunks:1961}. Thunks can be used to simulate call-by-name within a call-by-value evaluation strategy~\citep{Hatcliff:Thunks:1997}. Strictness analysis, which can be used to avoid the creation of thunks, can be seen as a case of order of evaluation (or ``path'') analysis~\citep{Bloss:Path:1994}. Strictness optimizations can cause surprising, unwelcome behavior~\citep{Beemster:Strictness:1994}.

Deforestation can be performed via so-called short cuts~\citep{Gill:A-short:1993}. Opportunities for deforestation can be recognized through higher-order type inference~\citep{Chitil:Type:1999}. The elimination of unnecessary construction of intermediate data structures is addressed more generally by stream fusion~\citep{Coutts:Stream:2007}. Update analysis is discussed by~\citet{Bloss:Update:1989}. Another surprising source of optimizations for functional aggregates are loop optimization techniques developed for use in scientific computing~\citep{Anderson:Compilation:1990}.

Call-pattern specialization~\citep{Peyton-Jones:Call-pattern:2007} can be used to reduce the cost of the pervasive use of algebraic data types and function definition through pattern matching. Efficient pattern-matching nevertheless requires some finesse~\citep{Le-Fessant:Optimizing:2001}. Unboxed representations and their benefits are discussed by~\citet{Peyton-Jones:Unboxed:1991,Thiemann:Unboxed:1995}.
%; pattern matching can become much more powerful~\citep{McBride:The-view:2004}%TODO: This is appropriate for the chapter on features, not here.

We did not discuss the problem of space leaks~\citep{Wadler:Fixing:1987} and its partial solution through black-holing~\citep{Jones:Tail:1992} for graph reducers. Black-holing makes it impossible to back up in the event of an interrupt or other exception, requiring another solution~\citep{Reid:Putting:1999} if we wish to support interrupts while avoiding the space leaks otherwise prevented by black-holing.

\Citet{Douence:A-systematic:1998} describe an overarching framework for describing and decomposing abstract machines. Our summary of the abstract machine design space drew heavily on their work. Another view~\citep{Ager:From:2003} of abstract machines makes a technical distinction between abstract machines, which operate directly on lambda terms, and virtual machines, which operate on lambda terms compiled into their own instruction set. It is also possible to go between functional evaluators and abstract machines that implement the evaluation strategy via a state transition system~\citep{Ager:A-functional:2003}.