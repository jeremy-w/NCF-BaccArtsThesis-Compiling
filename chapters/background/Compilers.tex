\myChapter{Compilers}\label{background:compilers}
A \vocab{compiler} is a translator. As in translation, there is a source language and a target language, and it is the translator's job to analyze the source text and produce ``equivalent'' text in the target language. When translating human languages, the equivalency of the original and a translation is only rough because there are so many factors on which to evaluate equivalency. When it comes to programming languages, however, we are not interested in the niceties of meter or alliteration, nor do we care about any subtleties of connotation: we want to map a computation expressed in one language to a computation expressed in another such that both produce identical outputs when given identical inputs.

Beyond that requirement, everything else about the computation is fair game for alteration. Since the source language is often unaware of the peculiarities of the target language, many of the details of how exactly the computation should be carried out are unspecified and open to interpretation: $5 \times 4$ can be calculated by straightforward multiplication of 5 and 4, but if $5 \times 2$ is already known, we need only multiply that quantity by 2, and since these multiplications happen to be powers of 2, we could instead employ bit shifts in place of multiplication, especially as a bit shift is likely to take less time than multiplication. Even if the source language is also the target language, the original source code might still be improved by careful translation without altering its behavior in the high-level sense of input-output mapping discussed above.

Alterations that will preserve observable behavior are called \vocab{safe}: correspondingly, alterations that might not are called \vocab{unsafe}. A compiler will never voluntarily perform unsafe alterations, though some allow the human user to instruct them to do so, as might at times seem desirable for runtime checks that serve only to catch errors that slipped past the programmer or when the human is able to determine that a transformation that appears to the compiler to be unsafe will, in fact, be safe in this case. The compiler must act conservatively: it can only consider safe those transformations that it can prove to be safe, and it must assume the worst where it cannot prove the behavior to be any better. The user need not make such assumptions.

You might be wondering whether, if compilers are translators, is there a similar programming language analogue for human language interpreters? There is, and they are even called \vocab{interpreters}. They perform on-the-fly interpretation: rather than translating the code for future execution, they directly execute it. They are not able to perform the extensive analysis of the whole program that compilers perform. It is, in fact, this degree of analysis that particularly distinguishes a compiler, though translation for the future rather than for the present is also frequently a characteristic. This latter characteristic, however, is less apparent in more recent developments such as \vocab{just-in-time compilation}, which might be coupled with an interpreter as in the Java Hotspot virtual machine. Our subject is neither interpreters nor just-in-time compilation, however, so this concludes our first and last words on the two subjects.

A compiler is a translator: it analyzes the source code and produces equivalent target code. This suggests a decomposition of the compiler into two parts, one performing analysis and the other generating target code. The analysis part is commonly called the \vocab{front end}, while the code generation part is called the \vocab{back end}. From our discussion of alterations and the relative speeds of multiplication versus bit shifts, you might also infer that the compiler also attempts, following its analysis, to improve the code in some fashion. This \vocab{optimization} is at times folded into the front and back ends, but as the number of optimizations employed rises and the complexity of performing them alongside the work of the front and back ends increases, it becomes wise to dedicate part of the compiler to optimization alone. Optimization can only be performed following analysis: we cannot improve what we do not understand. At the same time, we should like to generate optimized code as directly as possible. This suggests placing the optimizer between the front and back ends. Due to this common decomposition of concerns and arrangement of the flow of control between the parts, the part concerned with discovering and performing optimizations is sometimes wryly referred to as the \vocab{middle end} of the compiler.

\section{Front End: Analyzing Source Code}
The front end of the compiler is responsible for analyzing the source code. It takes a long string of characters (the program), discerns what each string is meant to be in terms of the ``types of speech'' of the programming language, figures out how the various parts fit together to form a valid program (or that they do not form a valid program, if the programmer has made an error!), and tries to infer the meaning of the parts.\footnote{For a common analogy, compare spell-checking, grammar-checking, and ``sense-checking'' or ``sanity-checking''---answering the question, ``this is a syntactically valid sentence, but does it mean anything?''---, though we have yet to achieve the latter.} As a whole, the front end of a compiler represents one of the great achievements of computer science: we have powerful formalisms that can be used to specify and automatically generate the various parts of the front end.

\subsection{Lexical Analysis}
As presented to the compiler, the source code is a very long sequence of characters. This is the domain of \vocab{lexical analysis}. A long sequence of characters does not mean much at the character-level, so the first thing the front end must do is proceed from characters to a more meaningful level of abstraction. The \vocab{lexer}, which performs lexical analysis (and is also called, quite naturally, the \vocab{lexical analyzer}), reads in characters and chunks them into \vocab{tokens}, strings of characters having some meaning at the level of the programming language's structure. These tokens are akin to parts of speech in spoken language---while the specific details of the token (``this identifier is formed by the string \code{engineIsRunning}'') might be recorded for use in later stages, they are subsumed by the token, which treats, in a sense, all nouns as nouns, regardless of whether one is ``cat'' and one is ``dog.''

This tokenization is performed systematically by simulating the operation of a \vocab{finite automaton} that recognizes tokens. A finite automaton is, like a \TM, an abstract machine, but it is far simpler and far less powerful: a \TM can do everything a \FA can, but a \FA cannot do everything a \TM can.

\subsubsection{Regular Languages}
It turns out that we can describe all decision problems as \vocab{language problems}. A language is a (potentially countably infinite) set of \vocab{words}, and words are made up of characters from a finite \vocab{alphabet} by \vocab{concatenation}, the ``chaining together'' of characters denoted by writing them without intervening space: concatenating $a$ and $b$ in that order gives $ab$. The decision problem recast as a language problem becomes, ``Given a word and a language (and, implicitly, an alphabet), determine whether the word is in the language.'' The languages for which a \TM can solve this problem are known variously as \vocab{recursive}, \vocab{decidable}, and \vocab{Turing-computable} languages. The languages whose membership problems can be solved by a \FA{}, on the other hand, are known as the \vocab{regular languages} and form a proper subset of the recursive languages.

\subsubsection{Finite Automata}
A \FA is a constructive way to describe a regular language. Each \FA is associated directly to a language, the language whose membership problem it solves. Given a word, it solves this problem by examining the word one character at a time. After it has consumed all its input, it halts operation. Based on the state in which it halts, we say either that it \vocab{accepts} the word or rejects it. We build a \FA by specifying its makeup. A \FA is made up of a finite set of states and a transition function that describes how, in each state, the \FA responds to consuming the characters of the alphabet. In specifying a \FA{}, we also specify the alphabet of its language, the \FA's initial state, and the set of \vocab[final state]{final} or \vocab{accepting states}, those states which, when the \FA halts in them, indicate acceptance of the word.

We can specify the states and transition function in two ways: either as a list and table (respectively), or graphically through a \vocab{transition diagram}. A transition diagram has circular nodes for states, typically labeled with the state name, and arrows between states, which indicate the transition function. The arrows are labeled with the character causing the state transition indicated by the arrow. Accepting states are indicated by circling the node representing their states, so that they appear as two concentric circles.

The form of the transition function distinguishes between several varieties of \FAs. A transition function that, on any character, permits a transition to only one state is known as a \vocab{deterministic \FA{}}. A transition function that permits a transition to a set of states on any character is known as a \vocab{non-deterministic \FA{}}. It accepts if any state out of the set of states it halts in is an accepting state. A final variety of \FA is distinguished by admitting not only transitions to a set of states, but ``autonomous'' transitions---transitions that occur without consuming any of the input. These are known as \vocab{\emptyword-transitions} because transitioning along them ``consumes'' only the empty word \emptyword made up of no characters. These varieties of \FAs are all equivalent in power---it is possible to convert a \FA of one type into another type such that both recognize the same language---, but some sorts describe a language more naturally or concisely than others. \FAs are unique in that, for a given regular language, there is a \vocab{minimal deterministic \FA{}}, a deterministic \FA with the fewest number of states possible that is unique up to renaming of states.

\subsubsection{Regular Expressions}
We can also describe regular languages declaratively, using \vocab{regular expressions}. These do not describe how to recognize a given language, but rather describe the language directly. This is done by augmenting the alphabet with a direct linguistic interpretation and by adding special symbols representing operations on this linguistic interpretation. 

The linguistic interpretation associated to a character is direct and intuitive: the character $a$ represents the language consisting of that single character, $\set{a}$. It is natural to generalize this direct representation to words: the word $w$ represents the language consisting of that single word, $\set{w}$. Words are built up by concatenation. To aid in describing many concatenations of a simple structure, we can introduce some notation. Iterated concatenation of a \regex $w$ with itself is represented by superscripts: $w^{0}$ is the language of only \emptyword, the empty word; $w^{1}$ is just $\set{w}$ itself; and, as a rule, $w^{n} = w^{n-1}w$. We can represent unlimited concatenation using the \vocab{Kleene star} $\kstar$: $w\kstar$ represents the set of all concatenations of $w$ with itself, including $w^{0}$: $w\star = \set{w^{0}, w^{1}, w^{2}, \dotsc}$. If we wish to exclude the possibility of $w^{0},$ we can use the otherwise equivalent \vocab{positive closure} operator $\posclos$: $w\posclos = \set{w^{1}, w^{2}, \dotsc}.$ To represent choice or \vocab{alternation} in the language---either \regex $w$ or \regex $v$ is acceptable---we can introduce a corresponding operator; $+$ and $\vert$ are both popular choices for representing it: we shall use \alt\ here. Thus, the \regex $a \alt b$ represents the language $\set{a, b},$ while, more generally, the \regex $w \alt v$ constructed by the alternation of the \regexes $w$ and $v$ represents the language $L(w) \union L(v)$, where we use $L(w)$ to represent the language associated to the \regex $w$.\footnote{In general, where $X$ is any description of a language, whether by \TM or \FA or \regex or by any other description aside from the sets representing the languages themselves directly, we write $L(X)$ for the language described by $X$.} Finally, to allow unambiguous composition of \regexes, we can introduce clarifying parentheses. These let us describe, for example, the language $(a + b)\kstar b$, the language comprising all strings of zero or more $a$s or $b$s ending with a $b$.

While \regexes are very useful for describing regular languages, they do not provide a way to recognize the languages they describe. Fortunately, regular expressions happen to be readily interconvertible with \FAs.

\subsubsection{Lexing}
With \regexes to describe the lexical structure of tokens and \FAs to perform the actual work of recognizing tokens, we have a ready way to perform tokenization. Simply scan through the character stream till every recognizing \FA will begin to fail; of those that make it this far and will accept, select the token of the highest priority as that summing up the scanned text. The introduction of prioritization and a ``maximal munch'' rule provide ways to resolve ambiguity deriving from our wishing to chunk an input, the program, that in truth belongs to a language unrecognizable by a \FA{}, into words belonging to various token-languages recognized by \FAs.

In truth, however, the lexer is responsible for more than simply recognizing tokens. It works in cooperation with the parser (which we shall describe next) by feeding it a stream of tokens. Further, it records information associated to the tokens, often in a global, shared \vocab{symbol table} associating to each token, or symbol, some information, such as the text or value of the token. It might even use information in the symbol table or information provided by the parser to make a distinction between tokens that it is impossible or exceedingly difficult to make with \regexes alone.

\subsection{Syntax Analysis}
\vocab{Syntax analysis} follows lexical analysis. If lexical analysis is concerned with categorizing words by part of speech, then syntax analysis is concerned with understanding how these parts of speech are grammatically related and whether the sentences so formed are grammatical or not.

\subsubsection{Context-Free Languages}
In fact, ``grammatical'' is precisely the word, for the formalism affording ready syntax analysis is that of context-free grammars. As with the regular languages, we are able to describe a given context-free language either constructively or declaratively. The context-free languages are a proper superset of the regular languages and a proper subset of the recursive languages. Roughly, the context-free languages are distinguished from the regular languages by their ability to describe ``matching bracket'' constructs, such as the proper nesting of parentheses in an arithmetic expression, while the recursive languages are distinguished from the context-free languages in part by their ability to cope with context-sensitive languages.\footnote{The context-sensitive languages are, however, only a proper subset of the recursive languages.}

\subsubsection{Context-Free Grammars}
We use \vocab{context-free grammars} to specify context-free languages declaratively. As with \regexes and \FAs, context-free grammars operate in the context of a specific alphabet. The letters of the alphabet are called \vocab{terminals} or \vocab{terminal symbols}. They augment this alphabet with a finite set of \vocab{non-terminals (non-terminal symbols)} to be used in specifying grammatical \vocab{productions}, which function as \vocab{rewrite rules}. Together, the set of terminal and non-terminal symbols are called \vocab{grammar symbols}, as they specify all the symbols used by the grammar. Analogous to the start state of the \FA is the context-free grammar's distinguished \vocab{start symbol}. Putting these rules together, one arrives at a grammar specification like the following:
\[
G = \left(N, T, \Sigma, P, S\right)
\]%
\[
N = \set{A, B} \qquad T = \set{a, b} \qquad \Sigma = {a, b}
\]
\[\begin{split}
P = \{&S \produces A, &&S \produces B, &&S \produces aABb,\\
&A \produces a \alt \emptyword, &&B \produces b \alt \emptyword\}
\end{split}\]
where $N$ is the set of non-terminals, $T$ the set of terminals, $\Sigma$ the alphabet, and $P$ the set of productions, where \produces\ is read as ``produces''. The symbol to the left of the arrow is called the \vocab{head} of the production, while those to the right are called the \vocab{body}. %(The symbol \produces is also sometimes written \altproduces.)
``Computation'' proceeds by substitution: for example,
\[
S \derives[S \produces aABb] aABb \derives[A \produces a] aaBb \derives[B \produces b] aabb
\]
where \derives\ is read as ``derives in one step'' and the rule justifying the derivation is written above the arrow. Taking a cue from regular expressions, we can also write \derives[\star] for ``derives in zero or more steps'' (all grammar symbols derive themselves in zero steps) and \derives[+] for ``derives in one or more steps,'' where the productions justifying the derivation are implicit in the superscript star; the keen reader should perhaps like to construct their own explicit, step-by-step derivation. The language defined by the grammar is defined to be those strings made up only of terminal symbols that can be derived from the start symbol.

\paragraph{Parse Trees} We can use a \vocab{parse tree} to represent the derivation of a word in the language without concern for unnecessary sequencing of derivations imposed by our sequential presentation. For example, our choice to derive $a$ from $A$ prior to deriving $b$ from $B$ above is irrelevant, but that we first derived $aABb$ from $S$ before performing either of the remaining derivations is not, since the heads of these derivations are introduced by the derivation from $S$. We define parse trees constructively: 
\begin{aenumerate}
\item Begin by making the start symbol the root.
\item\label{parsetree:construction:choose} Select a non-terminal on the leaves of the tree with which to continue the derivation and a production for which it is the head.
\item Create new child nodes of the chosen head symbol, one for each symbol in the body.
\item Repeat from \ref{parsetree:construction:choose}.
\end{aenumerate}
At any point in time, the string of symbols derived thus far---those on the leaves, read in the same order applied to the child nodes in the body of a production---is called a \vocab{sentential form}. The process terminates when a word in the language is derived, as no non-terminal leaf nodes remain.

\paragraph{Ambiguity} Parse trees represent the derivation of a word without regard to unnecessary sequencing. A given tree represents a given parse. If more than one parse tree can derive the same word in the language, the grammar is said to be \vocab{ambiguous}. This corresponds to the use of a significantly different ordering of productions and potentially even of a different set of productions. The grammar is called ambiguous because, given such a word, it is uncertain which productions were used to derive it. The grammar we gave above is ambiguous when it comes to the empty word \emptyword, because $S \derives A \derives \emptyword$ and $S \derives B \derives \emptyword$ are both valid derivations with corresponding significantly different valid parse trees of \emptyword. However, if we were to eliminate the productions $S \derives A$ and $S \derives B$ from the grammar, we would then have an unambiguous grammar for the context-free language comprising $\set{ab, aab, abb, aabb} \equiv \set{a^{i}b^{j} \where 1 \leq i, j \leq 2}$. The only sequential choices are insignificant: in deriving $aabb$, we must have derived $aABb$ from $S$, but following that, did we first derive the second $a$ or the second $b$?

\paragraph{Derivation Order} While the parse trees for a word in a language factor out differences between possible derivations of the word other than those reflecting ambiguity in the grammar, when performing a derivation or constructing such a parse tree, we must employ such ``insignificant'' sequencing. There are two primary systematic ways to do so: always select the leftmost nonterminal symbol in step \ref{parsetree:construction:choose} of the parse tree construction process, or always select the rightmost. These ways of deterministically choosing the next symbol to replace in the derivation give rise to what are unsurprisingly known as a \vocab{leftmost derivation} and a \vocab{rightmost derivation}; to indicate the use of one or the other, the derivation arrow in all its forms is augmented with a subscript of $lm$ for leftmost derivation and $rm$ for rightmost, giving \derives[][lm] and \derives[][rm]. Since this is only a matter of choice in constructing the parse tree, it should be clear that, for any given parse tree, there exist both leftmost and rightmost derivations of its sentential form. %Leftmost and rightmost derivations will be important when we discuss parsing, which, given a word, must solve the problem of finding a valid derivation. (The problem of what to do if the sentence should prove ungrammatical, that is, if no valid derivation exists, is another problem in itself, that of error handling.)

\subsubsection{Pushdown Automata}
We can also specify context-free languages constructively using an abstract machine called a \vocab{pushdown automaton}. A \PDA is a \FA augmented with a stack and associated stack alphabet. It has an initial stack symbol as well as an initial state. Its transition function is parameterized by the input symbols, the current state, and the symbol currently on top of the stack and specifies the next state and the stack symbols with which to replace the current top of stack. This can be thought of as, every move, the \PDA consults the current input, its state, and the top of the stack, changes to the corresponding state, pops the current top of the stack off the stack, and pushes the stack symbols specified by the transition function onto the stack. We again have a choice of representing this either with a table or graphically. While \FA transition diagrams had arrows labelled 
\[
\token{input symbol}
\]
the arrows of \PDA transition diagrams are labelled 
\[
\token{input symbol}, \token{stack symbol to pop} / \token{stack symbols to push}
\]
where the convention used for the stack operations is that the symbol that is to be on top of the stack after pushing is leftmost (that is, the stack conceptually grows to the left).

There are some casualties of the transition from \FAs to the increased descriptive power of \PDAs. \PDAs are inherently non-deterministic: they always admit \emptyword-transitions and can be in a set of states at any given time. This non-determinism is essential for them to define the context-free languages. The languages described by deterministic \PDAs, while still a proper superset of the regular languages, are only a proper subset of the context-free languages. Further, there is no algorithmic way to produce a minimal \PDA for a given language. This poses a particular problem for parsing: as with lexing, we would like to use grammars to describe the syntactic structure and \PDAs to perform parsing by recognizing that structure, but we must now find some way for our inherently deterministic computers to cope with this inherent non-determinism in a reasonable amount of time.

\subsubsection{Parsers}
As exaggeratedly hinted at above, while grammars define a language, parsers are faced with an input that they must characterize as either of that language or not. They must, in fact, do more than simply check that their input is grammatical: they must construct an \IR of their input to pass on to the next part of the compiler.

We also mentioned the problem of the non-determinism inherent to \CFGs and \PDAs. Fortunately, for most realistic inputs, non-determinism will only to a limited degree be essential to proper parsing: so long as we only face insignificant questions of sequencing, we will have no problem determining what to do next.\footnote{Do not forget that, as we have been the ones to create programming languages, we also control the structure of the admissible inputs, so we can guarantee that only predominantly harmless non-determinism is encountered.} It is only when the parser must entertain several possibilities simultaneously (corresponding to a \PDA being in multiple, simultaneous states) and wait till later in its input for this multiplicity to be resolved in favor of one or another of the parses in process that we run into a real problem. Even when this occurs, it is likely to be affect only part of the input, and methods have been developed that handle even this non-determinism gracefully.

The remainder of our discussion of parsers will focus on several of the more common of their many types. The level of our discussion will be one of summary, not of definition; for details, the interested reader is referred to the literature discussed in \partandnameref{Section}{background:conclusion:bibliographicnotes}.

\paragraph{Recursive Descent Parsers}
Recursive descent parsers discover a \emph{leftmost derivation} of the input string during a \emph{left-to-right scan} of the input, whose alphabet, thanks to the lexer, will be tokens rather than individual letters and symbols. One function is responsible for handling each token; parsing begins by calling the function associated to the start symbol. They discover the derivation by recursively calling themselves as necessary. The parser is aware of the current input symbol via what is known as \vocab{lookahead}. Since we are dealing with an actual machine, however, we are not restricted to lookahead of a single symbol, though we might prefer to do with only a single symbol's lookahead for effiency' sake. Those grammars parsable by a recursive descent parser with $k$ tokens of lookahead are known as \vocab{LL(k)}: leftmost derivation by left-to-right scan employing $k$ tokens of lookahead.

When recursive descent parsers use one token of lookahead, they act much like a \PDA. The implicitly managed function call stack acts as the \PDA's stack. However, since they trace out a leftmost derivation with only a limited number of tokens of lookahead, they must anticipate the proper derivation with minimal information about the rest of the input stream. This makes recursive descent parsers one of the most limited forms of parsers, though they might be the parser of choice in some cases because of the naturalness of expression they can admit and the simplicity and compactness of their parsers. Many of the disadvantages of recursive descent parsers can be overcome by admitting variable tokens of lookahead, with more tokens being used as needed to disambiguate the choice of production. 
%TODO: Mention ANTLR in the conclusion!

\paragraph{Precedence Parsing}
Recursive descent parsers are sometimes coupled with precedence parsers in order to facilitate parsing of arithmetic expressions. The order in which operations should be carried out is determined by a frequently implicit grouping determined by operator associativity and precedence. For example, multiplication is normally taken to have higher precedence than addition, so that $3 \times 5 + 4$ is understood to mean $(3 \times 5) + 4 = 19$ and not $3 \times (5 + 4) = 27.$ The left associativity of multiplication determines that $2 \times 2 \times 2$ should be understood as $(2 \times 2) \times 2.$ This becomes important, for example, in cases where the operands have side effects: suppose \code{id} is a unary function printing its input and then returning its input unchanged. Then \code{id}(1) + \code{id}(2) + \code{id}(3) will print \code{123} if addition is understood to be left-associative, but it will print \code{231} if addition is understood to be right-associative, even though the result of the additions will be identical due to the associativity property of addition.\footnote{Both of these cases further assume that the left argument of the binary operator + is evaluated before the right. Were the right argument to be evaluated first, \code{213} and \code{321} would be printed instead.}

Operator precedence parsing is preferred over the use of LL$(k)$ grammar rules not only because it is somewhat unobvious how to enforce the desired associativity and precedence in an LL$(k)$ grammar, but also because doing so introduces a chain of productions that exist solely to enforce the desired associativity and precendence relations between the expression operators. Beyond its use in concert with recursive descent parsers, precedence parsing has mostly been subsumed by the class of grammars we shall describe next. The central idea of using precedence to disambiguate an otherwise ambiguous choice of productions has lived on in them as a useful tool to avoid excessive modifications to a grammar that would only obscure its behavior.

\paragraph{LR$(k)$ Parsers}
The \vocab[LR$(k)$ family of parsers]{LR$(k)$}---left-to-right scan, rightmost derivation with $k$ tokens of lookahead---family of parsers is perhaps the most commonly used in practice. I say ``family'' because a number of subtypes (to be discussed shortly) were developed to workaround the exponential space and time requirements of the original \abbrev{LR}$(k)$ algorithm. The class of grammars recognizable by an \abbrev{LR}$(k)$ parser is known as the \abbrev{LR}$(k)$ grammars, and it is possible to give a reasonably straightforward \abbrev{LR}$(k)$ grammar for most programming languages. However, it was some time before clever algorithms that avoided unnecessary requirements of exponential space and time were developed, and so other, more restrictive classes of grammars with less demanding parsers were developed and deployed. These classes are more restrictive only in terms of the grammars they will accept, not in terms of the languages they accept: all parsers of the LR family accept the same class of languages, they simply place different more or less restrictive demands on the form of the grammars describing those languages.

Where LL$(k)$ parsers create a derivation from the top down by starting with the goal symbol and eventually building a derivation for the input, LR$(k)$ parsers build a rightmost derivation in reverse by reading in the input till they determine that they have seen the body of a production and then reducing the body to the head. They eventually reduce the entire input to the start symbol (often in this context called the \vocab{goal symbol}), at which point parsing is complete. They use a stack to store the symbols seen and recognized so far, so in the course of parsing they carry out a very limited set of actions: shifting input onto their stack, reducing part of the stack to a single symbol, accepting the input as a valid word in the grammar, and indicating an error when none of the above applies. Because of this behavior, such a bottom-up parser is often called a \vocab{shift-reduce parser}.

\paragraph{SLR$(k)$ Parsers}
The earliest and most restricted such class is known as the \vocab{simple LR$(k)$}, or \abbrev{SLR}$(k)$. These parsers use a simplistic method of determining what action to take while in a given state and reading a given input that introduces conflicts that more sophisticated methods would be capable of resolving. In a shift-reduce parser, there are two possible types of conflicts: 
\begin{description}
\item[\vocab{shift/reduce conflicts}] where the parser has seen what it considers the body of a valid symbol at this point in the parse but has also seen a viable prefix of yet another symbol, so it cannot determine whether to reduce to the former or shift further symbols onto the stack in an attempt to recognize the latter.
\item[\vocab{reduce/reduce conflicts}] where the parser has seen the entirety of the body of two productions that appear to be valid at this point in the parse and is unable to determine which to reduce to.
\end{description}

\paragraph{LALR$(k)$ Parsers}
More sophisticated parsing methods are more discriminating about what productions are still valid at a given point in the parse by taking into account more or less of the parsing process and input seen thus far, so called \vocab{left context} as it is to the left of where the parser presently is in consuming the input. (In this analogy, the lookahead symbols could be considered right context, though that term is never used.) One such method is known as \vocab{look-ahead LR (LALR)}. These parsers can be seen as ``compressed LR parsers,'' though this compression that can introduce spurious reduce/reduce conflicts that would not occur in a full LR parser. This has historically been seen as an acceptable tradeoff for the reduction in table size and construction time, since any LR grammar can be reformulated as an LALR grammar, but with more sophisticated LR algorithms developed later that retained the full power of full LR parsers while producing comparable levels of compression wherever possible (meaning that parsing an LALR grammar with such an LR parser would require the same space as parsing it with an LALR grammar), such a tradeoff became unnecessary, though it remains widespread. 

%TODO: Mention that similar methods allow creating a table-driven implementation of a DFA lexer.

\paragraph{Table-Driven Parsers}
Whereas recursive descent parsers and operator descent parsers can be hand-coded, many of the other parsing algorithms were developed to operate by way of precomputed tables.\footnote{That is not to say that the others cannot also be implemented through tables, simply that the table method is not felt to be the necessity that it is for these others.} They explicitly model a \FA, called the \vocab{characteristic finite automaton}; the tables allow the transition function to be implemented purely by table lookup. As hand-creation of tables is time-consuming and error-prone, tables for parsing are generally created algorithmically and the resulting tables used with a \vocab{driver} that simply does little more than gather the information necessary to perform the operations specified by the table.

\paragraph{Direct-Coded Parsers}
Parsers implemented entirely in code (rather than as a set of tables with a driver) were long seen as something to be generated only by humans, while parsers generated from a higher-level grammar description were to be implemented by way of tables. However, another possibility, often faster and smaller because of its lower overhead and its lack of a need to encode a rather sparse table, is to have the parser generator create a direct-coded parser, a parser that is not table-driven but yet is generated from a higher-level description rather than being written by hand.

\paragraph{GLR Parsers} LR parsers are restricted to parsing only LR languages. However, a very similar technique can be used to parse all context-free languages. \vocab{Generalized LR (GLR) parsers} are more general than LR parsers in two senses: 
\begin{itemize}
\item They are able to parse all context-free grammars, not just LR grammars.
\item Their method of parsing is a generalization of that used in LR parsers.
\end{itemize}
They generalize the parsing method of shift-reduce LR parsers by coping with ambiguity in the grammar by duplicating the parse stack and pursuing competing parses in parallel. When they determine a particular parse is in fact invalid, it and its stack are destroyed. If the grammar is in fact ambiguous and multiple parses are possible, this might lead to a \vocab{parse forest} instead of a parse tree. Making such parsers feasible requires some effort, and part of that effort was to replace several duplicate parse stacks by what amounts to a ``parse lattice'' that share as many grammar symbols as possible as parses converge and diverge, much reducing the space requirements of the parser as well as time spent repeating the identical shifts and reduces on different parse stacks. It is also important to employ similar compression methods as with the newer LR parser generation algorithms, so that extra space and time is only employed as strictly necessary to deal with non-LR constructs.

\paragraph{Semantic Actions}
We generally desire to know more than that a given input is grammatical: we want to create a representation of the information discovered during parsing for later use. This is done by attaching \vocab{semantic actions}, to productions in parsers and to recognized tokens in lexers. Such actions are invoked when the production is reduced or the token recognized, and they are used to build the representation and, in the lexer, to emit the recognized token for the parser's use. They also can be used to compute attributes of the nodes in the parse tree, as discussed next.
%TODO: talk about scannerless parsers -- from incestuous commingling to rad idea -- in the conclusion/bibliography. From modularity to power and effectiveness....

\subsection{Semantic Analysis}
\vocab[semantic analysis]{Semantic analysis}, also known as \vocab[context-sensitive analysis], follows scanning and parsing. Its job is to ``understand'' the program as parsed. Not all elements of the language can be checked by either regular expressions or context-free grammars; checking these falls to the semantic analyzer. Approaches to semantic analysis vary widely; while a formalism that permits generating semantic analyzers from a higher-level description, as is done for lexers and parsers, exists, its use has yet to become widespread. Frequently, semantic analysis is done purely through \foreign{ad hoc} methods.

A program in truth has two aspects to its semantics, the static and the dynamic. \vocab[static semantics]{Static semantics} are those aspects of the program's meaning that are fixed and unchanging. A common example is the type of variables (though there are languages that employ dynamic typing). These aspects are particularly amenable to analysis by the compiler, and information derived from understanding them can be used to optimize the program. A program's \vocab{dynamic semantics} are those aspects of the program that are only determined at runtime. A compiler can attempt to prove through analysis certain properties of the running program, for example, that an attempt to access an array element that does not exist (the eleventh element of a ten-element array, for example) can never occur. Some languages require that the compiler guarantee certain runtime behavior: if it is unable to provide that guarantee at compile time through analysis, the compiler must insert code to check the property at runtime. Java requires that no out-of-bounds array access occur: any such attempt must be refused and raise an error. Since these runtime checks can slow down a program, a frequent point of optimization in languages requiring such checks is proving at compile-time properties that enable the omission of as many such checks as possible from runtime. Many languages, particularly older languages, do not require runtime checks even where they might be worthwhile, while some compilers might permit disabling the insertion of runtime checks, an option favored by some for the generation of final, production code after all debugging has occurred.

\subsection{Attribute Grammars}
The formalism mentioned above for performing semantic analysis is that of \vocab{attribute grammars}. Attribute grammars piggy-back on the concepts of context-free grammars and parse trees. They associate to each grammar symbol a finite set of \vocab{attributes} that store information and to each production \vocab{semantic rules} that specify how the attributes of the symbols involved in that production are to be computed. Attributes are partitioned into two sets, those of the heads of productions, called \vocab{synthesized attributes}, and those of symbols of the body of a production, called \vocab{inherited attributes}. These can be viewed as flows of information respectively up and down the parse tree.

Attribute grammars are used like so: Once a parse tree is constructed, its symbols are decorated with \vocab{attribute instances}. Each symbol has its attributes, and each occurrence of that symbol in the tree has its own instances of those attributes; the attributes are common between two occurrences of the same symbols, but the values of their instances of those attributes will likely differ. A parse tree together with its attribute instances is called an \vocab{attributed tree}. It is within this attributed tree that attribute evaluation occurs. \vocab{Attribute evaluation is the computation of the values of the attribute instances of an attributed tree}. Such an evaluation will not necessarily terminate, and determining an appropriate order for evaluation such that evaluation can be performed efficiently is nontrivial.

Part of making this formalism usable involves, as with context-free grammars, finding sufficiently powerful, restricted classes of attribute grammars that can be used to capture the semantic information desired while enabling efficient evaluation. Two such classes are the \vocab{$S$-attributed grammars}, which admit only synthesized attributes, and so can be evaluated through a simple bottom-up walk of the parse tree, and the \vocab{$L$-attributed grammars}, which permit the attribute values of a given symbol in a production to depend only on the inherited attributes of the head of the production and the synthesized attributes of any symbols to the symbol's left; like the $S$-attributed grammars, they admit information flow from bottom-to-top within the parse tree, but they also allow for left-to-right information flow, as well, and can be readily evaluated in a left-to-right, depth-first walk of the parse tree, as occurs during recursive descent parsing.

Problems faced by practical implementations of the attribute grammar formalism include the management of storage for the multitude of attribute instances used during evaluation and the amount of attributes that exist solely to share non-local information. Non-local information is in general a problem with attribute grammars, and while a symbol table can be used alongside the grammar to avoid this issue, it is also an end-run around the formalism.

%TODO: Move to conclusion of chapter: Attribute grammars are useful to far more than compiler writers; they can be put to good use in the generation of debuggers, syntax-aware editors, and, more broadly, interactive development environments.

\section{``Middle End'': Optimizing the \abbrev{IR}}
% mostly just setting up the chapters in the next two parts; I've said a lot about this in other places, and the details come in those parts and don't even belong here

\section{Back End: Generating Target Code}
\subsection{Instruction Selection}

\subsection{Instruction Scheduling}

\subsection{Register Allocation}

\section{Intermediate Representations}
Translation begins with a source language and ends with a target, but those are rarely the only representations of the program used during compilation. Between the initial representation of the source code input to the compiler and the final representation of the target code output from the compiler, the compiler will use various \vocab{intermediate representations (\abbrev{IR}s)}. These need not resemble either the initial or final representation in the least, and the compiler is not restricted to use only one intermediate representation. Intermediate representations are, in a sense, common, private languages used between different parts of a compiler that support the operation of those parts.

The \abbrev{IR}s chosen affect all parts of the compiler, both on the superficial level of simple representation of the code and on the deeper level of how the compiler carries out its translation and even how much the compiler can prove about the runtime behavior of the code to exploit in optimizing it. For all its importance, the \abbrev{IR} remains more a matter of craft than science. Many \abbrev{IR}s have been used---estimates of two for every compiler ever created are likely conservative---, but this myriad of \abbrev{IR}s nevertheless is susceptible to categorization along various axes. Two such axes are the form of the \abbrev{IR} and the level of abstraction of the \abbrev{IR}.

\subsection{Form}
Intermediate representations divide broadly into classes based on their structure: those whose structure is linear, and those whose structure is graphical.

\subsubsection{Linear}
Linear \abbrev{IR}s resemble the structure of most programming languages, in that they have an implicit sequencing: begin at the beginning and process each instruction in turn till the last instruction is processed. Jump instructions of some form or another---either as higher-level, structured control flow constructs such as \code{while} and \code{for}, or as lower-level jumps and branches to labeled statements (or, at an even lower level, to statements a certain offset away)---can be used to explicitly alter this implicit order.

Linear \abbrev{IR}s have the advantage of being easy to represent for debugging or otherwise observing the actions of the compiler. They can also be easily written out to a text file. They simply become lines of text. They also share the same disadvantages as text: they have no easy way to share identical lines between sections beyond threading through them again via jumping, which only works on the instruction-level (\code{x := foo(bar)}) and not on the value-level (\code{foo(bar, baz, \dots)} itself), which can inflate the size of the code in the \abbrev{IR} and obscure shared computations. At the same time, because of their similarity to most target languages, a linear \abbrev{IR} can be a very good choice for when compiler must finally perform target code generation.

\subsubsection{Graphical}
Graphical \abbrev{IR}s can obscure control flow in favor of representing higher-level structure. Tree-based \abbrev{IR}s suffer from the same issues of size and repetition of common substructure as textual \abbrev{IR}s. Graphical \abbrev{IR}s based on \vocab{directed acyclic graphs}, which can be thought of as trees that admit merging of branches,\footnote{Or, if you are more mathematically inclined, can be thought of as directed graphs restricted not to have cycles, that is, a sequence of arcs leaving one node that can be traversed obeying their direction in order to return to the initial node. It is clear which viewpoint prevailed in the name of the structure.} can avoid both of these faults, though since, in imperative programming languages, $x$ at one point in the program need not be the same as $x$ at another, this may not serve to elicit truly redundant computation. Graphical \abbrev{IR}s always introduce a question of representation: many data structures can be used to represent graphs and many algorithms can be used to carry out the same operation, and each choice of data structure and algorithm has its own tradeoffs.

It is also not convenient to represent a graphical \abbrev{IR} as some form of output for human consumption; the \abbrev{IR} must either be sequenced and encoded into a linear form, or more complex and time-consuming techniques must be employed to create a graphical representation. This latter is not an option for storing information for the compiler's consumption: the \abbrev{IR} must then be encoded into a linear representation, though the compiler does not require a textual representation---a novel binary representation developed to suit the compiler's needs might in this case be the better choice. Regardless of problems of representation, many operations performed by the compiler are best expressed as operations on a graph, and a graph is often the most natural form to view the code from, as in the \vocab{control flow graph} that graphically depicts blocks of sequentially executed code (so-called \vocab{basic blocks}) connected by directed arcs to blocks that control might transfer to.

\subsection{Level of Abstraction}
\IRs can also be classified by their level of abstraction. Some levels of abstraction are more appropriate for the application of some optimizations than others, while for some, it is essential to be working at a given level of application: for example, optimizations dealing with register usage require that register usage be exposed by, expressed in, and directly manipulable through the \IR{}. In this case, only a low-level \IR will do.

High-level \IRs are frequently very close to the source language. They often include direct representations of structured control flow and indexed array accesses. However, much like the source language itself, they are not very suitable for the application of many optimizations, so they see only limited use within a compiler. An example of a high-level linear \IR would basically be a simple high-level programming language. A common high-level graphical \IR is the \vocab{abstract syntax tree (AST)}. An \AST is something of an abbreviated parse tree; it omits ``uninteresting nodes'' and eliminates the lower-level information of the parse tree in favor of a more semantically relevant and concise form.

Mid-level \IRs are much like high-level \IRs{}, except that they will generally require explicit computation of array accesses and eliminate structured control flow in favor of labels, jumps, and branches. It is very possible to blend high- and mid-level \IRs{}.

Low-level \IRs expose many more details about the target language and target machine. While this strongly suggests use of what is virtually the assembly language of the target machine, it is still possible to employ a graphical \IR{}. Such an \IR will have to provide a way to indicate indirection through memory addresses (in the jargon of C and its relatives, this would be called ``pointer dereferences'').

\subsection{Symbol Tables}
We include along with the \IR{} the tables of information maintained by the compiler. The most prominent of these is the \vocab{symbol table}, which records information on all symbols---variables, function names, and the like---in use in the program. The type of information in the symbol table reflects the where in the compilation process the compiler is and partially determines the level of the current \IR. Basic information is usually gathered through cooperation between the scanner and parser and is often necessary for and augmented during static semantic analysis. Other parts of the compiler will introduce further annotations to the symbols. The information stored for a symbol might include details such as the name, storage class (statically allocated, dynamically allocated, or created and destroyed along with a procedure), type, size, and much more. Use of a symbol table is in some senses analogous to allowing all semantic rules access to the attributes of the goal symbol: the table provides a way to readily aggregate information collected from a variety of places, in a variety of ways, at a variety of times.

\section{Bootstrapping, Self-Hosting, and Cross-Compiling}
Compilers have their own chicken-and-egg problem: Compilers are programs written in some language that compile programs written in a language, potentially programs written in the same language in which they themselves are written. Compilers written in their own source language are known as \vocab{self-hosting compilers} and are a particularly puzzling instance of this problem. Further, compilers run on a variety of machines: where did the first compiler for a new machine come from? These problems have several solutions. One can go about growing a compiler incrementally, by way of another compiler, by way of an interpreter, or by cross-compiling.\footnote{\vocab{T-diagrams} are frequently employed to explain these methods, but I have always found them more confusing than helpful and omit them here. The \spacedlowsmallcaps{T} encodes the three relevant issues of a compiler: the source language to the left, the target language to the right, and the machine the compiler runs on at the base.}

To grow a compiler incrementally, one implements a compiler for a subset of the source language in a language understood by an existing compiler (or even in machine language) and then uses this core language to write a compiler that can translate a greater subset of the source language; this can be repeated as many times as necessary to encompass the entire language.

One can implement a compiler for the desired source language in a language already understood by a running compiler. Once that compiler has been used to generate a compiler for the new source language, a compiler for that language can be written using the language and compiled with this compiler to obtain a self-hosting compiler.

If an interpreter for the language exists, a self-hosting compiler can be written immediately and run in the interpreter on its own source code to create a compiled version of itself. Due to the comparative slowness of interpreted code next to compiled code, it may be necessary to interpret only a skeleton of the compiler and use that to compile only the same skeleton. This skeleton, for example, might omit all optimization and use only the simplest of algorithms for code generation. Once a skeleton compiler exists, it can be run on code for the full compiler, producing a compiler capable of optimization and clever code generation. This compiler, however, will not be as efficient as possible, since it was compiled with the skeleton compiler: this can be resolved by having the slow-running, full compiler compile its own source code, at which point the desired faster, full compiler will be obtained.

\vocab{Cross-compilation} is a quick way to produce compilers for new machines.\footnote{Cross-compilers are also essential in embedded situations, where the target does not have the resources to run a compiler and it is impossible to develop an application for the machine using the machine itself.} A cross-compiler is a compiler that compiles across target machines, for example a C compiler running on a \abbrev{SPARC} machine but generating code runnable by a PowerPC. By cross-compiling the compiler itself, a compiler for the new machine is readily obtained. This allows one to leverage all the work put into creating the compiler for the original machine.

Clearly, a variety of solutions to this problem exist, but I hope the central idea of \vocab{bootstrapping} has come through, that of using what you have now to pull yourself up to where you would like to be. The particular approach employed depends very much on the circumstances and on the preferences of the compiler writers.
%TODO: If you're not going to use T-diagrams, at least use some diagrams! Even quasi-flowcharts would help here.