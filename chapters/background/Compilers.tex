\myChapter{Compilers}\label{background:compilers}
A \vocab{compiler} is a translator. As in translation, there is a source language and a target language, and it is the translator's job to analyze the source text and produce ``equivalent'' text in the target language. When translating human languages, the equivalency of the original and a translation is only rough because there are so many factors on which to evaluate equivalency. When it comes to programming languages, however, we are not interested in the niceties of meter or alliteration, nor do we care about any subtleties of connotation: we want to map a computation expressed in one language to a computation expressed in another such that both produce identical outputs when given identical inputs.

Beyond that requirement, everything else about the computation is fair game for alteration. Since the source language is often unaware of the peculiarities of the target language, many of the details of how exactly the computation should be carried out are unspecified and open to interpretation: $5 \times 4$ can be calculated by straightforward multiplication of 5 and 4, but if $5 \times 2$ is already known, we need only multiply that quantity by 2, and since these multiplications happen to be powers of 2, we could instead employ bit shifts in place of multiplication, especially as a bit shift is likely to take less time than multiplication. Even if the source language is also the target language, the original source code might still be improved by careful translation without altering its behavior in the high-level sense of input-output mapping discussed above.

Alterations that will preserve observable behavior are called \vocab{safe}: correspondingly, alterations that might not are called \vocab{unsafe}. A compiler will never voluntarily perform unsafe alterations, though some allow the human user to instruct them to do so, as might at times seem desirable for runtime checks that serve only to catch errors that slipped past the programmer or when the human is able to determine that a transformation that appears to the compiler to be unsafe will, in fact, be safe in this case. The compiler must act conservatively: it can only consider safe those transformations that it can prove to be safe, and it must assume the worst where it cannot prove the behavior to be any better. The user need not make such assumptions.

You might be wondering whether, if compilers are translators, is there a similar programming language analogue for human language interpreters? There is, and they are even called \vocab{interpreters}. They perform on-the-fly interpretation: rather than translating the code for future execution, they directly execute it. They are not able to perform the extensive analysis of the whole program that compilers perform. It is, in fact, this degree of analysis that particularly distinguishes a compiler, though translation for the future rather than for the present is also frequently a characteristic. This latter characteristic, however, is less apparent in more recent developments such as \vocab{just-in-time compilation}, which might be coupled with an interpreter as in the Java Hotspot virtual machine. Our subject is neither interpreters nor just-in-time compilation, however, so this concludes our first and last words on the two subjects.

A compiler is a translator: it analyzes the source code and produces equivalent target code. This suggests a decomposition of the compiler into two parts, one performing analysis and the other generating target code. The analysis part is commonly called the \vocab{front end}, while the code generation part is called the \vocab{back end}. From our discussion of alterations and the relative speeds of multiplication versus bit shifts, you might also infer that the compiler also attempts, following its analysis, to improve the code in some fashion. This \vocab{optimization} is at times folded into the front and back ends, but as the number of optimizations employed rises and the complexity of performing them alongside the work of the front and back ends increases, it becomes wise to dedicate part of the compiler to optimization alone. Optimization can only be performed following analysis: we cannot improve what we do not understand. At the same time, we should like to generate optimized code as directly as possible. This suggests placing the optimizer between the front and back ends. Due to this common decomposition of concerns and arrangement of the flow of control between the parts, the part concerned with discovering and performing optimizations is sometimes wryly referred to as the \vocab{middle end} of the compiler.

\section{Intermediate Representations}
Translation begins with a source language and ends with a target, but those are rarely the only representations of the program used during compilation. Between the initial representation of the source code input to the compiler and the final representation of the target code output from the compiler, the compiler will use various \vocab{intermediate representations (\abbrev{IR}s)}. These need not resemble either the initial or final representation in the least, and the compiler is not restricted to use only one intermediate representation. Intermediate representations are, in a sense, common, private languages used between different parts of a compiler that support the operation of those parts.

The \abbrev{IR}s chosen affect all parts of the compiler, both on the superficial level of simple representation of the code and on the deeper level of how the compiler carries out its translation and even how much the compiler can prove about the runtime behavior of the code to exploit in optimizing it. For all its importance, the \abbrev{IR} remains more a matter of craft than science. Many \abbrev{IR}s have been used---estimates of two for every compiler ever created are likely conservative---, but this myriad of \abbrev{IR}s nevertheless is susceptible to categorization along various axes. Two such axes are the form of the \abbrev{IR} and the level of abstraction of the \abbrev{IR}.

\subsection{Form}
Intermediate representations divide broadly into classes based on their structure: those whose structure is linear, and those whose structure is graphical.

\subsubsection{Linear}
Linear \abbrev{IR}s resemble the structure of most programming languages, in that they have an implicit sequencing: begin at the beginning and process each instruction in turn till the last instruction is processed. Jump instructions of some form or another---either as higher-level, structured control flow constructs such as \code{while} and \code{for}, or as lower-level jumps and branches to labeled statements (or, at an even lower level, to statements a certain offset away)---can be used to explicitly alter this implicit order.

Linear \abbrev{IR}s have the advantage of being easy to represent for debugging or otherwise observing the actions of the compiler. They can also be easily written out to a text file. They simply become lines of text. They also share the same disadvantages as text: they have no easy way to share identical lines between sections beyond threading through them again via jumping, which only works on the instruction-level (\code{x := foo(bar)}) and not on the value-level (\code{foo(bar, baz, \dots)} itself), which can inflate the size of the code in the \abbrev{IR} and obscure shared computations. At the same time, because of their similarity to most target languages, a linear \abbrev{IR} can be a very good choice for when compiler must finally perform target code generation.

\subsubsection{Graphical}
Graphical \abbrev{IR}s can obscure control flow in favor of representing higher-level structure. Tree-based \abbrev{IR}s suffer from the same issues of size and repetition of common substructure as textual \abbrev{IR}s. Graphical \abbrev{IR}s based on \vocab{directed acyclic graphs}, which can be thought of as trees that admit merging of branches,\footnote{Or, if you are more mathematically inclined, can be thought of as directed graphs restricted not to have cycles, that is, a sequence of arcs leaving one node that can be traversed obeying their direction in order to return to the initial node. It is clear which viewpoint prevailed in the name of the structure.} can avoid both of these faults, though since, in imperative programming languages, $x$ at one point in the program need not be the same as $x$ at another, this may not serve to elicit truly redundant computation. Graphical \abbrev{IR}s always introduce a question of representation: many data structures can be used to represent graphs and many algorithms can be used to carry out the same operation, and each choice of data structure and algorithm has its own tradeoffs.

It is also not convenient to represent a graphical \abbrev{IR} as some form of output for human consumption; the \abbrev{IR} must be sequenced and encoded into a linear form. However, many operations performed by the compiler are best expressed as operations on a graph, and a graph is often the most natural form to view the code from, as in the \vocab{control flow graph} that graphically depicts blocks of sequentially executed code (so-called \vocab{basic blocks}) connected by directed arcs to blocks that control might transfer to.

\subsection{Level of Abstraction}
\abbrev{IR}s can also be classified by their level of abstraction. 
% TODO: write this subsection! read some Muchnick first for this; he goes into great depth about IRs
\subsubsection{High-Level}
\subsubsection{Mid-Level}
\subsubsection{Low-Level}

\section{Front End: Analyzing Source Code}
The front end of the compiler is responsible for analyzing the source code. It takes a long string of characters (the program), discerns what each string is meant to be in terms of the ``types of speech'' of the programming language, figures out how the various parts fit together to form a valid program (or that they do not form a valid program, if the programmer has made an error!), and tries to infer the meaning of the parts.\footnote{For a common analogy, compare spell-checking, grammar-checking, and ``sense-checking'' or ``sanity-checking''---answering the question, ``this is a syntactically valid sentence, but does it mean anything?''---, though we have yet to achieve the latter.} As a whole, the front end of a compiler represents one of the great achievements of computer science: we have powerful formalisms that can be used to specify and automatically generate the various parts of the front end.

\subsection{Lexical Analysis}
As presented to the compiler, the source code is a very long sequence of characters. This is the domain of \vocab{lexical analysis}. A long sequence of characters does not mean much at the character-level, so the first thing the front end must do is proceed from characters to a more meaningful level of abstraction. The \vocab{lexer}, which performs lexical analysis (and is also called, quite naturally, the \vocab{lexical analyzer}), reads in characters and chunks them into \vocab{tokens}, strings of characters having some meaning at the level of the programming language's structure. These tokens are akin to parts of speech in spoken language---while the specific details of the token (``this identifier is formed by the string \code{engineIsRunning}'') might be recorded for use in later stages, they are subsumed by the token, which treats, in a sense, all nouns as nouns, regardless of whether one is ``cat'' and one is ``dog.''

This tokenization is performed systematically by simulating the operation of a \vocab{finite automaton} that recognizes tokens. A finite automaton is, like a \TM, an abstract machine, but it is far simpler and far less powerful: a \TM can do everything a \FA can, but a \FA cannot do everything a \TM can.

\subsubsection{Regular Languages}
It turns out that we can describe all decision problems as \vocab{language problems}. A language is a (potentially countably infinite) set of \vocab{words}, and words are made up of characters from a finite \vocab{alphabet} by \vocab{concatenation}, the ``chaining together'' of characters denoted by writing them without intervening space: concatenating $a$ and $b$ in that order gives $ab$. The decision problem recast as a language problem becomes, ``Given a word and a language (and, implicitly, an alphabet), determine whether the word is in the language.'' The languages for which a \TM can solve this problem are known variously as \vocab{recursive}, \vocab{decidable}, and \vocab{Turing-computable} languages. The languages whose membership problems can be solved by a \FA{}, on the other hand, are known as the \vocab{regular languages} and form a proper subset of the recursive languages.

\subsubsection{Finite Automata}
A \FA is a constructive way to describe a regular language. Each \FA is associated directly to a language, the language whose membership problem it solves. Given a word, it solves this problem by examining the word one character at a time. After it has consumed all its input, it halts operation. Based on the state in which it halts, we say either that it \vocab{accepts} the word or rejects it. We build a \FA by specifying its makeup. A \FA is made up of a finite set of states and a transition function that describes how, in each state, the \FA responds to consuming the characters of the alphabet. In specifying a \FA{}, we also specify the alphabet of its language, the \FA's initial state, and the set of \vocab[final state]{final} or \vocab{accepting states}, those states which, when the \FA halts in them, indicate acceptance of the word.

We can specify the states and transition function in two ways: either as a list and table (respectively), or graphically through a \vocab{transition diagram}. A transition diagram has circular nodes for states, typically labeled with the state name, and arrows between states, which indicate the transition function. The arrows are labeled with the character causing the state transition indicated by the arrow. Accepting states are indicated by circling the node representing their states, so that they appear as two concentric circles.

The form of the transition function distinguishes between several varieties of \FAs. A transition function that, on any character, permits a transition to only one state is known as a \vocab{deterministic \FA{}}. A transition function that permits a transition to a set of states on any character is known as a \vocab{non-deterministic \FA{}}. It accepts if any state out of the set of states it halts in is an accepting state. A final variety of \FA is distinguished by admitting not only transitions to a set of states, but ``autonomous'' transitions---transitions that occur without consuming any of the input. These are known as \vocab{\emptyword-transitions} because transitioning along them ``consumes'' only the empty word \emptyword made up of no characters. These varieties of \FAs are all equivalent in power---it is possible to convert a \FA of one type into another type such that both recognize the same language---, but some sorts describe a language more naturally or concisely than others. \FAs are unique in that, for a given regular language, there is a \vocab{minimal deterministic \FA{}}, a deterministic \FA with the fewest number of states possible that is unique up to renaming of states.

\subsubsection{Regular Expressions}
We can also describe regular languages declaratively, using \vocab{regular expressions}. These do not describe how to recognize a given language, but rather describe the language directly. This is done by augmenting the alphabet with a direct linguistic interpretation and by adding special symbols representing operations on this linguistic interpretation. 

The linguistic interpretation associated to a character is direct and intuitive: the character $a$ represents the language consisting of that single character, $\set{a}$. It is natural to generalize this direct representation to words: the word $w$ represents the language consisting of that single word, $\set{w}$. Words are built up by concatenation. To aid in describing many concatenations of a simple structure, we can introduce some notation. Iterated concatenation of a \regex $w$ with itself is represented by superscripts: $w^{0}$ is the language of only \emptyword, the empty word; $w^{1}$ is just $\set{w}$ itself; and, as a rule, $w^{n} = w^{n-1}w$. We can represent unlimited concatenation using the \vocab{Kleene star} $\kstar$: $w\kstar$ represents the set of all concatenations of $w$ with itself, including $w^{0}$: $w\star = \set{w^{0}, w^{1}, w^{2}, \dotsc}$. If we wish to exclude the possibility of $w^{0},$ we can use the otherwise equivalent \vocab{positive closure} operator $\posclos$: $w\posclos = \set{w^{1}, w^{2}, \dotsc}.$ To represent choice or \vocab{alternation} in the language---either \regex $w$ or \regex $v$ is acceptable---we can introduce a corresponding operator; $+$ and $\vert$ are both popular choices for representing it: we shall use \alt\ here. Thus, the \regex $a \alt b$ represents the language $\set{a, b},$ while, more generally, the \regex $w \alt v$ constructed by the alternation of the \regexes $w$ and $v$ represents the language $L(w) \union L(v)$, where we use $L(w)$ to represent the language associated to the \regex $w$.\footnote{In general, where $X$ is any description of a language, whether by \TM or \FA or \regex or by any other description aside from the sets representing the languages themselves directly, we write $L(X)$ for the language described by $X$.} Finally, to allow unambiguous composition of \regexes, we can introduce clarifying parentheses. These let us describe, for example, the language $(a + b)\kstar b$, the language comprising all strings of zero or more $a$s or $b$s ending with a $b$.

While \regexes are very useful for describing regular languages, they do not provide a way to recognize the languages they describe. Fortunately, regular expressions happen to be readily interconvertible with \FAs.

\subsubsection{Lexing}
With \regexes to describe the lexical structure of tokens and \FAs to perform the actual work of recognizing tokens, we have a ready way to perform tokenization. Simply scan through the character stream till every recognizing \FA will begin to fail; of those that make it this far and will accept, select the token of the highest priority as that summing up the scanned text. The introduction of prioritization and a ``maximal munch'' rule provide ways to resolve ambiguity deriving from our wishing to chunk an input, the program, that in truth belongs to a language unrecognizable by a \FA{}, into words belonging to various token-languages recognized by \FAs.

In truth, however, the lexer is responsible for more than simply recognizing tokens. It works in cooperation with the parser (which we shall describe next) by feeding it a stream of tokens. Further, it records information associated to the tokens, often in a global, shared \vocab{symbol table} associating to each token, or symbol, some information, such as the text or value of the token. It might even use information in the symbol table or information provided by the parser to make a distinction between tokens that it is impossible or exceedingly difficult to make with \regexes alone.

\subsection{Syntax Analysis}
\vocab{Syntax analysis} follows lexical analysis. If lexical analysis is concerned with categorizing words by part of speech, then syntax analysis is concerned with understanding how these parts of speech are grammatically related and whether the sentences so formed are grammatical or not.

\subsubsection{Context-Free Languages}
In fact, ``grammatical'' is precisely the word, for the formalism affording ready syntax analysis is that of \vocab{context-free grammars}. As with the regular languages, we are able to describe a given context-free language either constructively or declaratively. The context-free languages are a proper superset of the regular languages and a proper subset of the recursive languages. Roughly, the context-free languages are distinguished from the regular languages by their ability to describe ``matching bracket'' constructs, such as the proper nesting of parentheses in an arithmetic expression%FIXME: and recursive languages?
%; the recursive languages are distinguished from the context-free languages by the introduction...not of context, sense context-sensitive langs are a proper subset of the recursive langs...hmm....

\subsubsection{Context-Free Grammars}
\paragraph{LL($k$)}
\paragraph{LR($k$)}
\paragraph{SLR($k$)}
\paragraph{LALR($k$)}
\paragraph{GLR}

\subsubsection{Pushdown Automata}

\subsubsection{Parsers}
\paragraph{Table-Driven}
\paragraph{Direct-Coded}
\paragraph{Recursive Descent}
%TODO: Should we back up and talk about ways of encoding a DFA for lexing? tables, directly, etc.?
%FIXME: semantic actions! mention semantic actions! for lexers too!
%scannerless parsers -- from incestuous commingling to rad idea

\subsection{Static Semantic Analysis}

\section{``Middle End'': Optimizing the \abbrev{IR}}

\section{Back End: Generating Target Code}
\subsection{Instruction Selection}

\subsection{Instruction Scheduling}

\subsection{Register Allocation}

\section{Bootstrapping and Self-Hosting}