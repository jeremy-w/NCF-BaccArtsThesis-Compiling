\myChapter{Computers}\label{background:computers}
\section{From Abstract Turing Machines to Concrete Computing Machines}
A Turing machine takes some input, acts on it, and, if its computation terminates, produces some output. For example, we can specify a Turing machine that takes as input a natural number and checks whether that number is even or odd, and we can guarantee that it will always halt with an answer. To compute the solution to another problem, we must specify another Turing machine. This is fine when we are working with paper and pencil, but Turing machine computations executed via paper and pencil offer no advantage over any other work with paper and pencil and have the disadvantage of being exceedingly tedious. What if we wanted to move beyond paper-and-pencil machines and manufacture machines that perform these computations in the real world, machines that will not become bored and make a mistake, and, further, can carry out the computations much faster than we? In that case, producing a separate machine for every computation would not be of much use. Indeed, what we need is a universal machine, a single machine capable of computing anything any Turing machine can compute.

This \vocab{universal Turing machine} would accept as input the description of another Turing machine and data for that machine to operate upon and then simulate the operation of the input machine on the input data. By devising an encoding for the description of a Turing machine that can be processed by a Turing machine, we can build this abstract machine. What remains is to build the concrete machine.

What parts would such a machine need? From a user's perspective, any Turing machine performs three primary activities:
\begin{itemize}
\item accept input
\item perform the computation for this input
\item produce output.
\end{itemize}
Two of these steps involve communicating with the user; one is entirely internal to the machine. When we move to a universal \TM, what was once internal becomes external: the need to simulate the action of another \TM demands some way to store the description of the \TM while simulating it.

Considering Turing machines has in fact brought us to the essential parts of a modern computer:
\begin{itemize}
\item means of accepting input and communicating output
\item storage for input, both programs and data
\item facilities to process instructions and data.
\end{itemize}
This chapter will describe these three fundamental divisions of a computer with a particular emphasis on aspects of their implementation that affect compilation.

Before we move on, let us take one last look at Turing machines in light of this list. The processing facilities of the universal \TM are its transition function and states operating per the definition of \TMs. Input-output facilities are not actually part of the \TM: input appears on the tape, computation occurs, and we somehow observe the final state and tape contents of the \TM. The universal \TM's storage is its tape. It is interesting that both data and program (the description of the machine to be simulated) lie on the same tape. A single memory for both instructions and data is the hallmark of the \vocab{von Neumann architecture} and distinguishes it from the \vocab{Harvard architecture}, which uses separate memories for program and data.\footnote{To be fair, it is possible to define universal \TMs naturally homologous to both of these architectures; it is simply our exposition that makes the von~Neumann appear the more natural.} While \TMs are a tremendously useful model for computation in the abstract, and while they also serve surprisingly well to bridge the conceptual distance from the abstract model to the concrete machine, that is as far as they will bring us. In the rest of this chapter, we will be talking about time-bound, space-bound, hardware computers, the clever methods used to reduce the limitations introduced by reality, and how those methods affect compilation.

\section{Processor}\label{background:computers:processor}
The processor is the brain of the computer. It is responsible for reading instructions, decoding and dispatching them for execution, and directing the other functional units in their operation. It does this repeatedly in a rapid instruction-fetch--decode--dispatch cycle: processor performance is often measured in terms of millions of instructions per second (\vocab{MIPS}), as well as in terms of the average cycles per instruction (\vocab{CPI}). The time to execute an instruction varies from a few cycles (simple arithmetic operations) to millions of cycles (loading values from memory). The processor keeps track of which instruction to fetch next via its \vocab{program counter (PC)}. Every time it fetches an instruction, it increments the \abbrev{PC} to the address of the location of the next instruction.

\subsection{The Fetch-Decode-Dispatch Cycle}
The processor only understands binary digits, or \vocab{bits.} The instructions themselves are simply distinguished sequences, or \vocab{strings}, of bits with an agreed upon meaning. The bit strings are given meaning as part of an \vocab{\ISL (ISL)}. Each \ISL is used to talk with a specific \vocab{\ISA (ISA)}. Each processor is an implementation of some \ISA and understands the \ISL designed for that architecture. In a sense, the \ISA is a description of an interface between an \ISL and a particular processor. It leaves the details unspecified, and this is where the various processors implementing a particular \ISA differentiate and distinguish themselves. As a loose analogy, consider a caller ID unit. It has to be able to connect to the phone system, and it has to speak the same language as the phone system to be able to receive the information it displays, but beyond that it is free to vary its shape, size, and the number of callers it can remember, among other things.

The processor computes according to the instructions it is given. It executes the instructions one after another. Before it can follow an instruction, it first has to get it, that is, the processor must \vocab[instruction fetch]{fetch} the instruction. Next, it must read and understand it. This process of looking over and understanding an instruction is called \vocab[instruction decode]{instruction decoding}. The first step of decoding an instruction is to recognize what sort of instruction has been fetched. Various sorts of instructions have various parts (called \vocab{operands}) relevant to what the processor is supposed to do; for example, an instruction to read in a location in memory will have to specify the location. After the processor understands these parts of the instruction, the processor has completed decoding the instruction. The instruction can then be \vocab[instruction dispatch]{dispatched} for execution and the next instruction can be fetched by the processor.%Instruction decoding is simpler if instructions are all of a fixed length and have a very predictable form. There is then no question of how many bits to read in for each instruction. On the other hand, not all instructions require the same amount of information. Stretching all instructions to fit the same bed of Procrustes can lead to the instructions taking more space than necessary. If storage is at a premium, then, variable-length instructions might be the better choice. This was particularly true in the past, and this historical decision persists in the now ubiquitous x86 architecture's variable length instructions.

What sorts of instructions are there? Instructions generally cover arithmetic operations (add, subtract, multiply, divide) on integers and floating point numbers,\footnote{Floating-point numbers are the computer equivalent of scientific notation. The ``point'' of ``floating point'' is the decimal point, whose position relative to the significant digits (those that we actually bothered to write down) can be changed by varying the exponent.} shifts (left and right\empause the bitstring \code{001} shifted left by 2 becomes \code{100}), logical operations (and, or, not, exclusive or),  jumps (instructing the processor to change its \abbrev{PC} and begin fetching instructions at another location) and conditional branches (jumps that are predicated on a particular condition, like a register's being nonzero), and memory operations (load from memory into a register, store from a register into memory, and possibly copy from register to register or memory location to memory location). The way conditional branches are supported and the types of conditional branch instructions provided vary from processor to processor, as do the ways that memory locations can be specified. Many other operations may be provided, such as those meant to deal particularly with strings of alphabetic characters coded into bits in one way or another or instructions meant to deal with numbers encoded as binary-coded decimal rather than as binary numbers. Each instruction set is different.

\subsection{Functional Units}
The instructions themselves are carried out by other functional units. Many of the arithmetic operations will eventually be routed through an arithmetic logic unit (\vocab{ALU}). Those dealing with floating point numbers, however, are likely to be sent to either a floating point unit or even a floating point co-processor.

Storage for operands is frequently provided by \vocab{registers.} Registers may be either special-purpose (available for use only in floating point operations, for example, or devoted to storing the \abbrev{PC} or the number zero), in which case they are likely to be divided into \vocab{register classes}, or general-purpose (available for any use in any instruction). Others may be functionally general-purpose but reserved for use by the operating system or assembler. The trend has been to greater numbers of general-purpose registers. Certain registers are exposed to the programmer via the\ISL and guaranteed by the \ISA{}, but the implementation is likely to make use internally of far more registers. If all the operands of an instruction must be in registers, the instruction is said to be register-register. Some instruction sets have register-memory or even memory-memory instructions, where one or even all operands of the instruction are in memory. This was particularly common in the past.

\subsection{Assembly Language}
Bit-string instructions are fine for machines, but they are difficult for humans to work with. For this reason, \vocab{assembly languages} were developed. Assembly languages represent the instructions with alphabetic abbreviations such as \code{j} for \emph{jump}, \code{beq} for \emph{branch if equal}, or \code{add} for \emph{add}. They will often allow the use of textual labels for the specification of branch and jump targets and the use of names for registers as opposed to purely numbers. They will accept directives as to the alignment of data in memory and convert character strings to bit strings in a particular encoding rather than requiring the programmer to perform the conversion. They might also provide greater levels of abstraction, such as providing pseudoinstructions like a richer set of branch conditionals that can be readily translated into the processor-provided instructions or allowing the programmer to define macros\footnote{So-called by abbreviation of ``macro-instruction.'' These are ``big'' instructions that stand in for and eventually expand out into a longer or more complicated sequence of instructions. The instructions might also be capable of accepting arguments to be substituted for placeholders throughout the expanded sequence.} for common code sequences.

The \vocab{assembler} is responsible for translating this symbolic instruction language into the binary instruction set language. It assembles assembly language code into \vocab{object code} executable by the processor. Its action is that of a compiler, though its job is generally much simpler than that of compilers for higher-level languages whose level of abstraction is much farther from the underlying processor.%TODO: Either explain ``higher-level languages'' in the introduction, or gloss it here. Umm, should explain level of abstraction somewhere, too...

\subsection{Types of Processors}
There have been many types of processors, but the two dominant types are the \vocab{complex instruction set computers (CISC)} and the \vocab{reduced instruction set computers (RISC).}

\CISCs were developed when computing resources were very limited and most programming was done in assembly languages. Since the instruction set language itself was programmers' primary interface to the machine, it seemed worthwhile to provide higher-level instructions that accomplished more complex goals, such as copying an entire string of characters from one place in memory to another or repeating an instruction a number of times determined by a counter register. They also frequently used variable-length instructions, to minimize the amount of space required by instructions\empause more complex instructions would frequently require more information, and so more bits, than simpler instructions.

The complex instruction sets of \CISCs were meant make it easier for people to program in assembly language. With the development of higher-level languages and compilers, these features were no longer necessary. In fact, compilers of the time were unable to take full advantage of these complex instructions and often generated instead equivalent sequences of simpler instructions. More complex instructions also require more complex logic and so more space within the processor. Supporting the complex instruction set slowed the processor and made it difficult to take advantage of more advanced processor design ideas. By reducing the complexity of the instruction set, the amount of space needed in the processor to implement the instruction set could be reduced, enabling the inclusion of greater numbers of fast registers. \RISCs capitalized on these observations. One of the most noticeable simplifications of their instruction sets is the elimination of memory-memory, register-memory, and memory-register operations beyond those necessary to load data from memory to registers and store data to memory from registers, leading to the characterization of \RISC architectures as \vocab{load-store architectures}.

\RISC ideas have been highly influential, and \RISC processors are often used in embedded situations, such as in cell phones, \abbrev{PDA}s, washing machines, automobiles, and microwaves. However, when \abbrev{IBM} elected to go with Intel's 8086 series \CISC processors rather than Motorola's 68000 series \RISC processors for its personal computers, it set the stage for the x86 and its successors to become the most common non-embedded processors. Modern \CISC processors, such as those made by Intel and \abbrev{AMD,} integrate elements of the \RISC philosophy into their designs while preserving compatibility with older x86 software. The \CISC instructions are often translated by the processor into \RISC microcode, which is then executed by the processor. Many of the more ``\CISC{}y'' instructions have been preserved for compatibility but allowed to ``atrophy'' to where they will execute much more slowly than a longer sequence of simpler instructions accomplishing the same result. This blending of \RISC and \CISC ideas, which eliminates any clear distinction between the two approaches to processor design, has brought us to what might be called a post-\RISC era.

\subsection{Issues Particularly Affecting Compilation}
When you compile software, you want it to run as well as possible on your computer. Often, this means as fast as possible, and as much work has gone into making the processors themselves run as fast as possible, the processor provides a lot for a compiler writer to worry about.

\subsubsection{Registers}
The number of registers available for compilation affects the usefulness of various optimizations as well as the speed of the compiled code. Storing to memory is slow, while registers are fast: the more data that can be kept in register, the better. Thus, the more general purpose registers available to the compiler when it comes time to allocate the generated code among the available registers, the better. Some processors provide a means to utilize the greater number of implementation-supplied registers; the \vocab{register windows} of Sun Microsystem's \abbrev{SPARC} (Scalable Processor ARChitecture) machines are one example. As mentioned above, some architectures will specify that various registers are reserved for various purposes and unavailable to the compiler. The architecture might also specify how registers are to be treated during a procedure call by defining a \vocab{calling convention}.\footnote{Even if an architecture has nothing to say about register usage in procedure call, a programming language specification might specify a calling convention in an attempt to guarantee interoperability between programs written in the language.} All of this directly affects the code generated by a compiler targeting the architecture.

\subsubsection{Historical Accidents and Implementation-Specific Extensions}
As mentioned above, while the instruction set language might provide a special-purpose instruction for handling, say, string copying, this might actually execute slower than a sequence of simpler instructions accomplishing the same thing. Thus, it is not sufficient to be familiar with the specification of the instruction set language or even of the instruction set architecture: decisions made in the implementation of the specific processor can affect choices made during compilation (or at least in some cases should). Other operations retained for compatibility might also be best avoided.

At the same time, various implementations will extend the instruction set in ways their designers hoped would be of use to compiler writers. Examples are the streaming \abbrev{SIMD} extensions\footnote{Generally further abbreviated to \abbrev{SSE}, \abbrev{SSE2}, etc.\ for the various generations of extensions. The \abbrev{SIMD} part stands for ``single instruction, multiple data.'' An example of such an instruction would be an add operation that specifies the addition of two vectors (likely representing a point in three-dimensional space), each made up of several data components.} added to various successors of the x86 architecture by Intel and \abbrev{AMD} meant to speed up code compiled for multimedia applications.

\subsubsection{Pipelining and Speculation}
In an attempt to speed up instruction execution in general, modern processors are deeply \vocab[pipelining]{pipelined}. Pipelining exploits \vocab{instruction-level parallelism}. Rather than decoding an instruction, dispatching it, and waiting for its execution to complete before beginning to execute the next instruction, instructions are decoded immediately, one after another, and dispatched, so that their execution overlaps. Pipelining does not decrease how long it takes an instruction to execute, but it does increase the number of instructions that can be executed per unit of time, making it appear that instructions are executing faster.

The depth of the pipeline places an upper limit on the number of instructions whose execution can overlap. However, various hazards of an instruction sequence can prevent an instruction from completing every cycle, and so prevent a new instruction from being dispatched each cycle. This causes a decrease in the number of instructions that can be executed in a given time period, a quantity referred to as \vocab{instruction throughput}.
\begin{description}
\item[\vocab{structural hazards}] occur when multiple instructions require simultaneous use of the same functional units.
\item[\vocab{data hazards}] occur when an instruction requires data that is not yet available.
\item[\vocab{control hazards}] occur when the program counter is to be altered, but how it will be altered is not known sufficiently in advance to keep the pipeline full.
\end{description}
All of these hazards cause \vocab{stalls}, also known as \vocab{bubbles}\empause space in the pipeline where an instruction would be executing, except that it cannot.

Various methods are employed to address these hazards. Structural hazards can be addressed by carefully designing the instruction set and the processor. Within the processor, data hazards are addressed by \vocab{forwarding}. Forwarding diverts data to where it is needed as soon as it is available, rather than waiting for it to become available through the usual means. For example, if an instruction is waiting on a value to be loaded from memory to a register, rather than waiting for the value to enter the datapath and finally be committed to register before loading it from the register, forwarding will make the value available as soon as it enters the datapath. It will eventually be stored to the targeted register, but in the mean time, the instruction that was waiting on the value can continue execution.\footnote{Some instruction sets expose the delay following branch (and load-store) instructions to the user in what is called a \vocab{delayed branch}: the next (or next several) instructions following a branch instruction are always executed. As pipelines deepened, exposing the delay to the user became less and less feasible, and successors of such instruction sets have often phased out such instructions in favor of non-delayed versions.} Outside the processor, data hazards are partially addressed by the introduction of a memory hierarchy, which we will discuss in \partandnameref{Section}{background:computers:memory}.

Control hazards are addressed by \vocab{branch prediction}. This can be as simple as always assuming a branch will not be taken or more complex, such as dynamically tracking whether the branch is taken more often than not during execution and acting accordingly. The processor begins executing instructions as if the branch had gone the way it predicted; when the branch is finally decided, if it goes the way that was predicted, there is no stall. If it goes the other way, there must still be a stall. Further, the processor must be able to undo all the instructions executed along the path not taken.

This is a specific example of a more general technique called \vocab{speculative execution}, in which a guess is made about some property of a part of the instruction stream, execution continues as if the guess were correct, and then is either confirmed or undone depending on whether the guess was correct or not. This is useful not only for branches, but for reordering the instruction stream in general to increase instruction-level parallelism.

\subsubsection{Multiple Issue Processors}
Another way to address these hazards and improve performance in general is to move to \vocab{multiple issue} processors. Rather than issuing at most a single instruction each cycle, multiple issue processors issue multiple instructions whenever possible. They are able to do this because much of the datapath has been duplicated, perhaps multiple times. Instructions can then truly execute simultaneously rather than simply overlapping. Clever methods are employed to ensure the appearance of sequential execution is not violated and to resolve dependencies between instructions. However, instruction sequences that have been optimized to maximize instruction-level parallelism will run faster; an optimizing compiler will take advantage of this.

In fact, in \vocab{static multiple issue} processors, compilers have no choice but to take advantage of this, as the processor itself merely performs multiple issues by following instructions. The burden of scheduling instructions to maximize instruction-level parallelism and taking advantage of the architecture falls entirely on the compiler. This has the advantage of enabling the compiler to leverage its knowledge of the properties of the entire program to demonstrate that reordering and simultaneously scheduling instructions is safe, but it has the disadvantage of directly exposing much of the internal design of such a processor, so that a program is more likely to have to be recompiled in order to run on even a slightly different processor.%
%NOTE: we do not discuss VLIW, vectorization

\vocab{Dynamic multiple issue}, or \vocab{superscalar}, processors attempt to exploit instruction-level parallelism at runtime. As they read in instructions, they reorder them, issue multiple instructions whenever possible, and speculatively execute instructions when they can. Since all of this goes on ``behind the scenes,'' a compiler can completely ignore it and still produce runnable code. At the same time, a sequence of instructions tailored for a specific processor can maximize the amount of instruction-level parallelism exploitable by that processor. Thus, unlike with static multiple issue processors, knowledge of the specific implementation of an \ISA using dynamic multiple issue is advantageous to the compiler but is not necessary to produce code that will run, and code that runs on one implementation of the \ISA should run sufficiently well on all other implementations of the same, regardless of whether or not dynamic multiple issue is employed.%
%NOTE: we do not discuss register renaming, predicated execution

Not only do pipelining, speculation, and multiple issue greatly complicate the development of a processor, they also make it more difficult to predict how generated code will be executed, as well as placing great emphasis on optimizing for instruction level parallelism. Examples of the effect these have on compilers are the efforts taken to minimize the number of branches and keep values in register as long as possible, though this latter is even more severely motivated by the presence of a memory hierarchy.

\section{Memory}\label{background:computers:memory}
A processor is nothing without memory. In the course of computing, the processor needs some way to store the data it is computing with. This necessitates some amount of temporary memory. Moreover, we should like for this memory to be very fast, so that computations not take an inordinate amount of time. However, we should also like to introduce a more persistent form of memory. This could take the form of something rather non-technical, such as punched cards, or something more technically sophisticated, such as a hard drive. We should also like this persistent storage to be as fast as possible, but we would be willing to sacrifice speed for capacity.

It is not surprising, then, that there should arise a definite memory hierarchy. This hierarchy is organized as a pyramid with the processor at its apex: memory closer to the processor can be accessed more quickly but can store much less (for reasons of both cost and space), while memory further from the processor is slower but much more capacious; by the time we reach the lowest level, \vocab{secondary storage}, the capacity has become virtually unlimited. This severe difference\empause fast and small on the one hand, slow and large on the other\empause is fine so long as we can restrict our computations to use the faster memory and leave the slower purely for storage. But this is rarely possible.

In order to reduce the need to use slow, secondary storage, we exploit the frequent fact of spatial and temporal locality in memory accesses to promote contiguous blocks of secondary storage into \vocab{caches}\empause faster but less capacious memory that mirrors the contents of secondary storage. Caches are organized into levels based on how close they are to the processor, and so how fast and small, generally from level one (L1) to at most level three (L3). Memory accesses first attempt to hit on the desired memory in cache; only if that fails do they have to resort to disk. The time to realize that the attempt to hit the memory in the cache has failed is called the \vocab{miss penalty}. This penalty is only introduced because of caching, but without caching, one would pay the far higher price of always having to wait for the disk to respond.

One common way to reduce both the miss penalty and the time to hit is to restrict the number of places a given block of memory can be placed in the cache. If the data at an address can be stored in any cache line, one must check every cache line to be sure that the data is not there. Such a cache is called \vocab{fully associative}. If the data at an address can only go in exactly one line, one can readily determine whether or not it is in the cache. Such a cache is called \vocab{one-way set associative}. However, because each level of the memory hierarchy is smaller than the one below it, there are always fewer cache lines than there are blocks of memory that could need to be stored in the cache. Limiting each block to being stored in only one, or only two, or any number of places fewer than the number of lines in the cache introduces a competition for those limited number of places among the blocks of memory that can only be stored in those places. This is in addition to the necessary competition for being in any line at all of the space-limited cache that occurs even in a fully associative cache.

While caches seek to exploit spatial and temporal locality, precisely how is a matter of some delicacy, with no clear best solution. One can attempt to reduce the miss penalty or time to hit, increase the capacity of the cache, improve its strategy for loading in a new \vocab{cache line} (the unit of storage within the cache, amounting to a fixed number of bits) from disk and its selection of a line to evict, but one cannot do all of these at once. Multilevel caches only further complicate things.

To help in thinking about such issues, one can characterize the types of cache misses through the ``three Cs'':
\begin{description}
\item[\vocab{compulsory misses}] occur on first access to a block of memory because a memory access has to miss before its line can be brought into the cache. They cannot be avoided, though they can be prevented from causing a delay by \vocab{prefetching}, which provides a way to indicate that a given block of memory might be needed in the future and should be loaded now.
\item[\vocab{capacity misses}] occur when a previously evicted block is requested again because a cache can only store so many lines of memory. They can be decreased by increasing the size of the cache.
\item[\vocab{conflict misses}] occur only in non-fully associative caches, when a block that was evicted by another block competing for the same set of cache lines is requested again. They can be decreased by increasing the associativity of the cache.\footnote{While competition occurs between all blocks for all cache lines in a fully associative cache, the misses that occur due to that competition are classed as capacity misses.}
\end{description}
Of all of these, the one that has the most direct effect on compilation is compulsory misses, provided prefetching is available. Otherwise, it is simply the existence of a memory hierarchy and its workings that affect a compiler. These details can make some data structures more efficient than others, affecting how the compiler codes the runtime behavior of the program. It also makes some uses of memory to store data during the program more efficient than others: use of one memory layout or another also falls to the compiler.

The existence of a memory hierarchy has a major effect on both compilation and compiler design. It affects compilation by increasing the need for and desirability of optimizations to increase spatial and temporal locality of memory accesses, reduce the need for storage, and confine space needs to registers internal to the processor as much as possible. It affects compiler design not only because of its effects on the code a compiler must generate, but also because the memory hierarchy has an effect on the behavior of the data structures and algorithms used to implement the compiler. Most algorithms are developed, either intentionally or na\"ively, in a \vocab{flat memory model} that assumes unlimited fast memory. As soon as one begins considering the effect of the memory hierarchy on the data structures and algorithms used, formerly optimal implementations may no longer be so.

Early attempts to develop algorithms and data structures within the context of a memory hierarchy used the \vocab{disk-access model}, which parameterizes its algorithms based on properties of the memory hierarchy such as the block size of and number of blocks in the cache (also called the width and height of the cache). These parameters are often not available and difficult, if not impossible, to determine at runtime. Introducing this explicit parameterization also makes code less portable and maintainable. Further, the model presumes fine-grained control over the behavior of the cache and storage that frequently is not available.

The later \vocab{cache-oblivious model} addresses these problems: while proofs of the behavior of its algorithms and data structures are by necessity parameterized, its data structures and algorithms are not, and behave well so long as at least a two-level memory hierarchy exists that can be modeled in the fast-slow small-large fashion appropriate to the two levels of cache and storage. Such a relationship exists generally between all levels of a memory hierarchy, so this suffices to guarantee the desired performance. This guarantee can and has been made precise in a formal fashion; for details, consult the \nameref{background:computers:bibliographicnotes} for this chapter.

\section{Input-Output}\label{background:computers:inputoutput}
Input-output is the computer's interface with the outside world. It encompasses everything from keyboards and monitors to network connections, disk access, and, in some sense, communication and synchronization between multiple \vocab[process]{processes} (that is, currently running programs with their own memory context and position in their instructions) and between multiple \vocab{threads} of execution. (Threads make up a process: each process has at least one thread of execution, but it might also divide its work between multiple threads.) Input-output requires implementation support. Input-output is primarily supported in one of two ways, through busy-wait (``spin blocking'') protocols and through the use of interrupts.

In a \vocab{busy-wait} input-output protocol, the computer provides a means for a program to indicate it wishes to perform output or receive input from a device. A signal is used to indicate the status of the input-output ``channel'': once it is ready to accept output or provide input, the signal is set to indicate this. On performing the desired action, the signal is reset. This requires constant polling of the signal to see whether its status has changed. The process performing this polling is unable to proceed while it cycles between checking whether the status has changed and waiting between checks. A process behaving in such a way is said to be \vocab{spin blocking}. Signals are also often provided by the processor for use in synchronizing the actions of processes using shared-memory concurrency. Atomic operations such as test-and-set might also be provided to help support concurrent execution.

Input-output can also be synchronized via \vocab{interrupts}. Interrupts are a sort of ``unexceptional exception'' in that they frequently make use of the processor's facilities for handling \vocab{exceptions} (such as division by zero and numeric over- and underflow) but are only exceptional in the sense that they require an immediate, real-time response and a switch in context from the currently running process to a special exception-handler that will perform the input-output. (Interrupts are also often used by operating systems to provide access to the system routines they provide.) How interrupts and exceptions are supported and the types of interrupts and exceptions supported vary from processor to processor, but such facilities are common in most, if not all, modern processors because of the advantage of such an implementation over busy-wait. While use of interrupts is preferable to busy-wait, it is not always possible. For example, communication over a network as occurs in using the Internet is frequently handled via busy-wait since most of the protocol stack is implemented in software.

Input-output is highly implementation-dependent and is a frequent source of complexity and difficulty both in the design of programming languages (particularly functional languages, for reasons to be discussed later) and processors. %TODO: replace this with a forward-reference to the discussion
It is also slow and time-consuming. Due to the complexity of input-output, however, many of the issues are often exposed to and expected to be managed at the program-level rather than the language-level, so input-output is not a frequent target of optimizations in the back-end, nor would it be likely to be a very fruitful target. Handling concurrency and parallelism, on the other hand, may be the responsibility of the compiler, particularly in parallel programming languages. This is frequently true of the quasi-concurrency of \vocab{exceptions} included in many newer languages. Exceptions are programmatic realizations of exceptional conditions that can arise at runtime, such as division by zero or the inability to access a given file. Whereas programs would generally give up and abort execution on encountering such a problem in the past, exceptions make it possible to recover from exceptional conditions and continue execution. They also mean that execution might suddenly need to jump to an exception handler at unpredictable times. Where permitted by the language, choices made by the compiler in how to support concurrency and parallelism can affect program runtime and safety.

\section{Bibliographic Notes}\label{background:computers:bibliographicnotes}
The universal \TM was introduced by Alan Turing in his seminal paper \citet{Turing:On-Computable:1937}. If the idea particularly intrigues you, you might enjoy \citet{Herken:The-Universal:1995}, a collection of short papers related to and inspired by Turing's own.

\citet{Patterson:Computer:2007} is a good introduction to the design and architecture of modern computers, including an influential \abbrev{RISC} assembly language and familial variations thereof. Another work by the same authors, \citet{Hennessy:Computer:2006}, goes into far more detail. Information on specific instruction-set architectures and languages is generally freely available online from the manufacturer.

For an introduction to the cache-oblivious memory hierarchy model, you could do no better than to start with \citet{Demaine:Cache-Oblivious:2002}. This paper briefly surveys the history of memory hierarchy models, formally introduces the cache-oblivious model, and explores some of its essential data structures with proofs of their space behavior. It provides pointers to the relevant literature throughout.