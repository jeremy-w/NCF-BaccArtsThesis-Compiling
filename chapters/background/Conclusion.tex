\myChapter{Conclusion}\label{background:conclusion}
This part provided background information essential to understanding the remainder of this work.
\begin{itemize}
\item In \partandnameref{Chapter}{background:beginnings}, we introduced the basic ideas of \lambdacalc and \TMs. These provide the fundamental models of computation for the functional and imperative paradigms, respectively. This connection will be made clearer in the following parts.

\item In \partandnameref{Chapter}{background:computers}, we used Turing machines as a bridge to modern computers. Succeeding sections described the three major parts of a computer: processor, memory, and input-output. Roughly, the processor is what lets a computer compute, memory provides storage, and input-output is what makes computers useful by allowing them to affect and interact with the world. We stressed the variety of processor architectures while giving some taste of that variety. We explained the existence of a memory hierarchy as well as the obstacle it presents to execution speed. We gave a rough sketch of how input-output is implemented in computers. We did not have much to say beyond this, since many of the details of input-output are more pertinent to programming languages themselves rather than their compilers.

\item In \partandnameref{Chapter}{background:compilers}, we surveyed compiler architecture and design. We introduced the three-part structure of a compiler and discussed each part. Along the way, we sketched the theory that lies at the basis of each part and how it is used in practice. We also briefly surveyed \IRs and their importance to the compiler. Lastly, we broached the chicken-and-egg issue of developing a compiler for a new programming language, implementing a compiler in its own source language, and similar logistical problems of compiler construction. The important point is that compilers neither develop in a vacuum nor spring fully-formed from the pregnant void, but evolve gradually, though this evolution may involve the seemingly contradictory device of the compiler effectively ``pulling itself up by its own bootstraps.''
\end{itemize}

\section{Bibliographic Notes}\label{background:conclusion:bibliographicnotes}
Later sections of this thesis treat \lambdacalc in more detail. %TODO: Convert to forward reference.
\Citet{Hopcroft:Introduction:2007} is a standard textbook covering languages and \TMs and touching on computational complexity. Its emphasis is on the abstract machines and the languages themselves as opposed to scanning and parsing. The classic reference for compiler design is \citet{Aho:Compilers:2006}, known affectionately as ``the dragon book'' for its cover art. (The color of the dragon is sometimes used to give the edition.) Many more recent texts still refer the reader to it for its detailed information on scanning and parsing, which is dealt with more cursorily in more modern texts to allow more discussion of optimization. A new edition was released in 2006 with some new content, but it is unclear to me at this time to what extent some of the older content has been updated. For example, the presentation of LALR parser generators as the culmination of LR parser generators was no longer true even when the older 1986 edition was released; the exponential space-time problem of the original LR algorithm had already been addressed in \citet{Pager:The-lane:1973}. (\Citeauthor{Pager:The-lane:1973}'s method was later illustrated and explained more clearly and briefly, though less formally, in \citet{Spector:Efficient:1988}.)
%FIXME: This is a pathetic excuse. You need to get your hands on a copy of the new dragon.

A good, modern, introductory textbook on compiler design is \citet{Cooper:Engineering:2004}. \Citet{Muchnick:Advanced:1997} picks up where a course using that book would leave off by giving more advanced information on the basic content and covering optimization and analysis in great detail. As one implements more optimizations in a compiler, the problem of optimization phase ordering, mentioned in~\ref{background:compilers:middle-end:phase-ordering} on page~\pageref{background:compilers:middle-end:phase-ordering}, grows in importance. One cannot escape the problem even by abandoning compilers, as it affects even even hand-optimized code \citep{Hines:Using:2005}. \Citet{Kulkarni:Fast:2005} describes an interesting attack on the problem by way of genetic algorithms.

Textbooks on compilers often seem to give the impression that scanning and parsing are a solved problems and the world has moved on. While that might be the case for scanning, parsing is still an active area of research. The Purdue Compiler Collection Tool Set bucked the trend of providing LR-style parser generators in favor of developing an LL parser generator. This parser generator is now a project in itself, ANTLR (ANother Tool for Language Recognition) \citep{Parr:ANTLR:1995}. Other areas of research are implementing practical full LR parsers (see Menhir \citep{Pottier:Menhir:2007} for an example) and GLR parsers (for example, Elkhound \cite{McPeak:Elkhound:2002}), as well as addressing problems of development of domain-specific languages and composable grammars; see, for example, \citet{Wyk:Context-aware:2007} and \citet{Bravenboer:Concrete:2004} and other work by those authors.\nocite{Bravenboer:Declarative:2006}\nocite{Bravenboer:Preventing:2007} A better reference than \citet{Aho:Compilers:2006} for parsing is \citet{Grune:Parsing:2007}.
%CRIT-TODO: Reorganize so that each chapter has its own bib notes. Writing a part-long one has shown that it will too long and too disconnected from the material that inspired it. Even I've forgotten what should be mentioned here!