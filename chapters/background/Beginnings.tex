\myChapter{Beginnings}\label{background:beginnings}
% Need to practically write this twice this because otherwise there's a bad interaction between the smallcaps and the italic in the section heading in the body versus the desired italics in the table of contents; it seems small caps italics is not an option...
% Also, can't use \Deutsch in the title---LaTeX coughs something about capacity exceeded. This is okay, because I wanted to use Babel to handle German hyphenation, and this is not going to be hyphenated in the TOC, anyway.
\section[A Sticky \foreign{Entscheidungsproblem}]{A Sticky Entscheidungsproblem}
The decision problem %, known under its German name \Deutsch{Entscheidungsproblem,} 
was an important problem in twentieth-century mathematical logic.\footnote{For example, it is intimately bound up with Hilbert's tenth problem: 
\begin{quote}
Given a Diophantine equation with any number of unknown quantities and with rational integral numerical coefficients: \emph{To devise a process according to which it can be determined in a finite number of operations whether the equation is solvable in rational integers.}
\end{quote}
}
It addresses the fundamental question of what we can and cannot know. There are many ways to pose the decision problem. One formulation was given by Hilbert and Ackermann in their 1928 book \textit{Principles of Theoretical Logic.} They call the dual problems of determining the universal validity and determining the satisfiability of a logical expression the \vocab{decision problem}. The problem is solved when one knows a ``process'' that determines either property of any given logical expression in first-order logic. The particular first-order logic they had in mind was that propounded in their book.%, \Deutsch{der engere Funktionenkalk\"ul} (renamed to \Deutsch{der engere Pr\"adikatenkalk\"ul} in the 1959 edition).
They were not able to be so clear about what they meant by ``process.''

By the 1930s, not only was the nebulous idea of a process formalized, but the decision problem had been solved in a way unanticipated by Hilbert: it was impossible to provide such a process.

The idea of a process was formalized three ways:
\begin{itemize}
\item the theory of \vocab{recursive functions}
\item the \vocab{\lambdacalc}
\item the \vocab{Turing machine.}
\end{itemize}
From \lambdacalc springs the functional paradigm, while the Turing machine inspires the imperative paradigm. %NOTE: Henckell disagrees: T-machine does not embody stored-program, fetch-execute von Neumann arch similarities.

\section{Church and His Calculus}
Church developed the \lambdacalc in hope of providing a logical basis for all of mathematics. While he was ultimately frustrated in this, he succeeded in creating a rich framework for both logic and, eventually, computer programming.

The \lambdacalc formulates computation as term rewriting and distills the concept of the function to textual substitution. The text comes in the form of \vocab{lambda terms;} the rewriting comes as \vocab{reduction rules.} To define the set of lambda terms $\Lambda$, we seed it with an infinite set of variables $V = \set{v, v\prime, v\prime\prime, \dotsc}$ and then further admit all expressions built using two operations, \vocab{application} and \vocab{abstraction:}
\begin{align*}
x \in V &\implies x \in \Lambda\\
M,N \in \Lambda &\implies (MN) \in \Lambda \quad\text{(application)}\\
M \in \Lambda,\, x \in V &\implies (\lambda x M) \in \Lambda \quad\text{(abstraction)}
\end{align*}
The fundamental reduction rule of the \lambdacalc is that of \vocab{$\beta$-reduc\-tion}: the application of an abstracted term $\lambda x M$ to another term $N$ can be replaced by $M$ with $N$ substituted for every occurrence of $x$ throughout $M$, or, written more symbolically, $\forall M,N \in \Lambda,$
\[
(\lambda x M)N = \replace{x}{N}{M}.
\]

We will have more to say about the \lambdacalc later in \partandnameref{Chapter}{functional:theory}. For now, we will content ourselves with pointing out that $N$ in $(\lambda x M)N$ may be \emph{any} other lambda term, including another abstracted term; the only distinction between ``functions'' $(\lambda x M)$ and ``literals'' $v, v\prime,$ etc.\ is that ``functions'' provide opportunities for $\beta$-reduction.

\section{Turing and His Machines}
Turing was working expressly to address the \Deutsch{Entscheidungsproblem}. He formalized computation by way of an abstract machine. A ``process'' is embodied in a machine. In the case of the decision problem, it would accept logical expressions\empause instances of the decision problem\empause as input.\footnote{These expressions would, of course, have to be suitably encoded for its consumption.} If there were an algorithm for the decision problem, the machine would then be able determine the answer for all instances. Instead, Turing found that any such machine would never be able to decide whether all possible input instances are or are not satisfiable; the decision problem is fundamentally \vocab{undecidable}, which is another way of saying it is not computable.\footnote{This is not to say that some individual instances of the problem are not decidable, but that there is no solution to the problem as a whole.}

Turing's machines look very much like a high-level sketch of our modern von~Neumann machines. They consist in a finite control (the program), a read-write head, and an infinitely long tape (the memory). The tape is divided into cells: each cell is either marked with a symbol or blank. The problem instance is written on the tape and the machine started; if it comes to a halt, the state it is in indicates the yea-nay--result of the computation. The final contents of the tape can be used to communicate actual details of the answer: for the decision problem as given above, the final state could be used to indicate that, yes, the input instance is satisfiable, while the tape could contain Boolean values satisfying the equation.

Formally, we can treat a Turing machine as a six-tuple $(Q, \Sigma, B, \delta, q_{0}, F)$:
\begin{description} % For some reason, uppercase args to \item were being lowercased. Protecting didn't work, but \MakeUppercase did. ::shrug::
\item[\MakeUppercase{$Q$}] is the finite set of states the control can be in.
\item[$\Sigma$] is the finite alphabet available for writing the input on the tape.
\item[\MakeUppercase{$B$}] is a distinguished blank symbol that cannot be part of the input; prior to placing the input on the tape, the tape is nothing but an endless sequence of cells filled with $B$.
\item[$\delta$] is a state transition function, $\delta\from (\Sigma \union B) \times Q \to (\Sigma \union B) \times Q \times D$, describing how the Turing machine reacts to reading a symbol $\sigma$ in state $q$:
\begin{itemize}
\item it writes some symbol, either the blank symbol or an input symbol; 
\item it moves from its current state to some state in $Q$, possibly the same state; and
\item its head moves some direction, either left $L$ or right $R$ (that is, $D = \set{L, R}$).
\end{itemize}
\item[$q_{0}$] is the initial state of the machine.
\item[\MakeUppercase{$F$}] is the set of \vocab{accepting states}; $F \subset Q$, and if the machine concludes its computation, that is, \vocab{halts} in some state in $F$, this indicates an affirmative answer to the question posed it. The computation is concluded when the machine can make no further move, which occurs when $\delta(\sigma, q)$ is undefined.\footnote{This makes $\delta$ a partial function. We can restore its totality by introducing the possibility of transitioning to a distinguished \spacedallcaps{HALT} action, but this is not really necessary.}
\end{description}

In the Turing machines' sequential operation and reliance on changes in their state and data store to perform computation, we find the roots of the imperative paradigm. Even more plain is the resemblance to our modern-day computers. %The finite control of a \TM corresponds to the stored program, while its tape corresponds to memory. But note that the instructions are encoded separately from the data; this is the arrangement that would become known as the \vocab{Harvard architecture,} which would itself become a mere footnote in the history of computing machines. The architecture that eventually came to dominate was the \vocab{von Neumann,} characterized by a single memory where both data and program reside. This, again, has its Turing machine analogue in the \vocab{universal Turing machine,} which accepts as input the description of another Turing machine and data for that machine to operate upon and then simulates the operation of the input machine on the input data.