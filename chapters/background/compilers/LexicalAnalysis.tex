\subsection{Lexical Analysis}
As presented to the compiler, the source code is a very long sequence of characters. This is the domain of \vocab{lexical analysis}. A long sequence of characters does not mean much at the character-level, so the first thing the front end must do is proceed from characters to a more meaningful level of abstraction. The \vocab{lexer}, which performs lexical analysis (and is also called, quite naturally, the \vocab{lexical analyzer}), reads in characters and chunks them into \vocab{tokens}, strings of characters having some meaning at the level of the programming language's structure. These tokens are akin to parts of speech in spoken language\empause while the specific details of the token (``this identifier is formed by the string \code{engineIsRunning}'') might be recorded for use in later stages, they are subsumed by the token, which treats, in a sense, all nouns as nouns, regardless of whether one is ``cat'' and one is ``dog.''

This tokenization is performed systematically by simulating the operation of a \vocab{finite automaton} that recognizes tokens. A finite automaton is, like a \TM, an abstract machine, but it is far simpler and far less powerful: a \TM can do everything a \FA can, but a \FA cannot do everything a \TM can.

\subsubsection{Regular Languages}
It turns out that we can describe all decision problems as \vocab{language problems}. A language is a (potentially countably infinite) set of \vocab{words}, and words are made up of characters from a finite \vocab{alphabet} by \vocab{concatenation}, the ``chaining together'' of characters denoted by writing them without intervening space: concatenating $a$ and $b$ in that order gives $ab$. The decision problem recast as a language problem becomes, ``Given a word and a language (and, implicitly, an alphabet), determine whether the word is or is not in the language.'' The languages for which a \TM can solve this problem are known variously as \vocab{recursive}, \vocab{decidable}, and \vocab{Turing-computable} languages. The languages whose membership problems can be solved by a \FA{}, on the other hand, are known as the \vocab{regular languages} and form a proper subset of the recursive languages.

\subsubsection{Finite Automata}
A \FA is a constructive way to describe a regular language. Each \FA is associated directly to a language, the language whose membership problem it solves. Given a word, it solves this problem by examining the word one character at a time. After it has consumed all its input, it halts operation. Based on the state in which it halts, we say either that it \vocab{accepts} the word or rejects it. We build a \FA by specifying its makeup. A \FA is made up of a finite set of states and a transition function that describes how, in each state, the \FA responds to consuming the characters of the alphabet. In specifying a \FA{}, we also specify the alphabet of its language, the \FA's initial state, and the set of \vocab[final state]{final} or \vocab{accepting states}, those states which, when the \FA halts in them, indicate acceptance of the word.

We can specify the states and transition function in two ways: either in a table, as in Fig.~\ref{lexing:fatables}, or graphically through a \vocab{transition diagram}. A transition diagram has circular nodes for states, typically labeled with the state name, and arrows between states, which indicate the transition function. The arrows are labeled with the character causing the state transition indicated by the arrow. Accepting states are indicated by circling the node representing their states, so that they appear as two concentric circles. Fig.~\ref{lexing:fafigs} provides three examples of transition diagrams.

\input{chapters/background/compilers/fatables}

\input{chapters/background/compilers/fafigs}

The form of the transition function distinguishes between several varieties of \FAs. A transition function that, on any character, permits a transition to only one state is known as a \vocab{deterministic \FA (DFA)}. A transition function that permits a transition to a set of states on any character is known as a \vocab{non-deterministic \FA (NFA)}. It accepts if any state out of the set of states it halts in is an accepting state. A final variety of \FA is distinguished by admitting not only transitions to a set of states, but ``autonomous'' transitions\empause transitions that occur without consuming any of the input. These are known as \vocab{\emptyword transitions} because transitioning along them ``consumes'' only the empty word \emptyword\ made up of no characters. This variety of \FA is known accordingly as an \vocab{\emptyword--non-deterministic \FA (\emptyword NFA)}. These varieties of \FAs are all equivalent in power\empause it is possible to convert a \FA of one type into another type such that both recognize the same language\empause but some sorts describe a language more naturally or concisely than others. \FAs[F] are unique in that, for a given regular language, there is a \vocab{minimal deterministic \FA{}}, a deterministic \FA with the fewest number of states possible that is unique up to renaming of states.

Figs.~\ref{lexing:fatables} and~\ref{lexing:fafigs} describe the same three \FAs in two ways, both in a table and through a transition diagram. All three \FAs recognize the same language. We can describe this language through \FAs as done here or through the regular expression $(a \alt b)\kstar b$, which will be discussed in the next section. From the transition diagrams, what do you think this regular expression means?

The \emptyword--non-deterministic \FA is the most visually complex. It was constructed algorithmically from the regular expression given above by patching together simpler \FAs by way of \emptyword\ transitions. The many \emptyword\ transitions make it highly non-deterministic. The simple non-deterministic \FA was created by identifying states joined solely by \emptyword\ transitions. It is the most elegant of the three. Its sole non-determinism consists in state $0$ having transitions to two different states on the character $b$, both to itself and to the final state $1$. The deterministic \FA was constructed from the non-deterministic. Its state $1$ behaves like the non-deterministic \FA when it is in the set of states $\set{0,1}$, which it enters after encountering the character $b$.

\subsubsection{Regular Expressions}
We can also describe regular languages declaratively, using \vocab{regular expressions}. These do not describe how to recognize a given language, but rather describe the language directly. This is done by augmenting the alphabet with a direct linguistic interpretation and by adding special symbols representing operations on this linguistic interpretation. 

The linguistic interpretation associated to a character is direct and intuitive: the character $a$ represents the language consisting of that single character, $\set{a}$. It is natural to generalize this direct representation to words: the word $w$ represents the language consisting of that single word, $\set{w}$. Words are built up by concatenation. To aid in describing many concatenations of a simple structure, we can introduce some notation. Iterated concatenation of a \regex $w$ with itself is represented by superscripts: $w^{0}$ is the language of only \emptyword, the empty word; $w^{1}$ is just $\set{w}$ itself; and, as a rule, $w^{n} = w^{n-1}w$. We can represent unlimited concatenation using the \vocab{Kleene star} $\kstar$: $w\kstar$ represents the set of all concatenations of the language represented by $w$ with itself, including $w^{0}$: $w\kstar = \set{w^{0}, w^{1}, w^{2}, \dotsc}$. If we wish to exclude the possibility of $w^{0},$ we can use the otherwise equivalent \vocab{positive closure} operator $\posclos$: $w\posclos = \set{w^{1}, w^{2}, \dotsc}.$ To represent choice or \vocab{alternation} in the language\empause either \regex $w$ or \regex $v$ is acceptable\empause we can introduce a corresponding operator; $+$ and $\vert$ are both popular choices for representing it: we shall use \alt\ here. Thus, the \regex $a \alt b$ represents the language $\set{a, b},$ while, more generally, the \regex $w \alt v$ constructed by the alternation of the \regexes $w$ and $v$ represents the language $L(w) \union L(v)$, where we use $L(w)$ to represent the language associated to the \regex $w$.\footnote{In general, where $X$ is any description of a language, whether by \TM or \FA or \regex or by any other description aside from the sets representing the languages themselves directly, we write $L(X)$ for the language described by $X$.} Finally, to allow unambiguous composition of \regexes, we can introduce clarifying parentheses. These let us describe, for example, the language $(a \alt b)\kstar b$, the language comprising all strings of zero or more $a$s or $b$s followed by a $b$.

While \regexes are very useful for describing regular languages, they do not provide a way to recognize the languages they describe. Fortunately, regular expressions happen to be readily interconvertible with \FAs.

\subsubsection{Lexers}
With \regexes to describe the lexical structure of tokens and \FAs to perform the actual work of recognizing tokens, we have a ready way to perform tokenization. Simply scan through the character stream till every recognizing \FA will begin to fail; of those that make it this far and will accept, select the token of the highest priority as that summing up the scanned text. This introduction of prioritization provides an intuitive way to resolve ambiguity deriving from our wishing to chunk an input, the program, that in truth belongs to a language unrecognizable by a \FA{}, into words belonging to various token-languages recognized by \FAs.

For example, consider developing a lexer for the input
\begin{lstlisting}[language=bash]
if ifPredicate; 
then
    echo "True.";
else
    echo "False.";
fi
\end{lstlisting}
intended to report whether the provided predicate is true or false.\footnote{The syntax of the example is basically that of the Bash shell, except we have eliminated the prefix sigils that make it easy to recognize variables.} The desired tokenization is illustrated in Fig.~\ref{lexing:tokens} on page~\pageref{lexing:tokens} along with a sequence of lexing rules that leads to this tokenization.

Before you can understand the rules, you must first understand some common extensions to the regular expression notation introduced so far:
\begin{aenumerate}
\item A set of characters enclosed in square brackets is equivalent to the alternation of those characters, so $[abc] = (a \alt b \alt c)$.

\item Within square brackets, an inclusive range of characters is indicated by interposing a dash between the two endpoints. A regular expression matching any capital letter $A$ through $Z$, then, is $[A-Z]$, which is equivalent to $(A \alt B \alt \dotsb \alt Z)$.

\item When a caret immediately follows the opening square bracket, this inverts the sense of the bracket-alternation: the listed characters are excluded from matching, but any other character of the alphabet will match the expression. Thus, $[\hat{\hphantom{\text{\mt{"}}}}\text{\small"}]$ matches any single character of the alphabet other than {\small"}.
\end{aenumerate}

\input{chapters/background/compilers/tokens}

The rules' order is important: earlier rules are assigned a higher priority than later. For example, since the rule recognizing a keyword, \lstinline[language=flex]{KW}, precedes the rule recognizing an identifier token, \lstinline[language=flex]{IDENTIFIER}, \lstinline[language=bash]{if} is tokenized as a keyword rather than an identifier, even though both rules match the two alphabetic characters \lstinline[language=bash]{i} followed by \lstinline[language=bash]{f}.

In truth, the lexer is responsible for more than simply recognizing tokens. It works in cooperation with the parser (which we shall describe next) by feeding it a stream of tokens. Further, it records information associated with the tokens, often in a global, shared \vocab{symbol table} associating to each token, or symbol, some information, such as the text or value of the token and the file and line number where it was first encountered. It might even use information in the symbol table or information provided by the parser to make a distinction between tokens that it is impossible or exceedingly difficult to make with \regexes alone.

For example, if the language of the text being scanned in Fig.~\ref{lexing:tokens}~(page~\pageref{lexing:tokens}) required that all functions and variables be declared before use, the lexer would be able to eschew the \lstinline[language=flex]{ID} token in favor of distinct \lstinline[language=flex]{FUNCID} and \lstinline[language=flex]{VARID} tokens by using information about the class of the identifier already stored in the symbol table to distinguish between the two.

Fig.~\ref{lexing:symtab} on page~\pageref{lexing:symtab} is an example of the earlier scanning rules of Fig.~\ref{lexing:tokens}~(page~\pageref{lexing:tokens}) adapted to use this approach. We have introduced two new keywords, \lstinline[morekeywords=var]{var} and \lstinline[morekeywords=func]{func}, and completely changed the identifier rule. Indeed, our lexing method has become more sophisticated: there is no longer is simple one-to-one correspondence between regular expressions and tokens. Instead, matching the input to a regular expression binds the matched text to a variable, \lstinline[language=CAML]{match}, and executes an associated action. This action can affect the environment in which lexing occurs and use that environment to decide how to classify the matched text. This occurs here through the variable \lstinline{context}, which is used to determine whether we are declaring a new variable or function identifier, or whether we should be able to lookup which of the two the current match is in the symbol table.\footnote{We have assumed that the parser, discussed next, has taken care of updating the symbol table so that the lookup will succeed if the variable or function identifier was previously declared.}

\input{chapters/background/compilers/symtab}