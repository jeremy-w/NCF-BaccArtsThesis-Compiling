\subsection{Lexical Analysis}
As presented to the compiler, the source code is a very long sequence of characters. This is the domain of \vocab{lexical analysis}. A long sequence of characters does not mean much at the character-level, so the first thing the front end must do is proceed from characters to a more meaningful level of abstraction. The \vocab{lexer}, which performs lexical analysis (and is also called, quite naturally, the \vocab{lexical analyzer}), reads in characters and chunks them into \vocab{tokens}, strings of characters having some meaning at the level of the programming language's structure. These tokens are akin to parts of speech in spoken language\empause while the specific details of the token (``this identifier is formed by the string \code{engineIsRunning}'') might be recorded for use in later stages, they are subsumed by the token, which treats, in a sense, all nouns as nouns, regardless of whether one is ``cat'' and one is ``dog.''

This tokenization is performed systematically by simulating the operation of a \vocab{finite automaton} that recognizes tokens. A finite automaton is, like a \TM, an abstract machine, but it is far simpler and far less powerful: a \TM can do everything a \FA can, but a \FA cannot do everything a \TM can.

\subsubsection{Regular Languages}
It turns out that we can describe all decision problems as \vocab{language problems}. A language is a (potentially countably infinite) set of \vocab{words}, and words are made up of characters from a finite \vocab{alphabet} by \vocab{concatenation}, the ``chaining together'' of characters denoted by writing them without intervening space: concatenating $a$ and $b$ in that order gives $ab$. The decision problem recast as a language problem becomes, ``Given a word and a language (and, implicitly, an alphabet), determine whether the word is or is not in the language.'' The languages for which a \TM can solve this problem are known variously as \vocab{recursive}, \vocab{decidable}, and \vocab{Turing-computable} languages. The languages whose membership problems can be solved by a \FA{}, on the other hand, are known as the \vocab{regular languages} and form a proper subset of the recursive languages.

\subsubsection{Finite Automata}
A \FA is a constructive way to describe a regular language. Each \FA is associated directly to a language, the language whose membership problem it solves. Given a word, it solves this problem by examining the word one character at a time. After it has consumed all its input, it halts operation. Based on the state in which it halts, we say either that it \vocab{accepts} the word or rejects it. We build a \FA by specifying its makeup. A \FA is made up of a finite set of states and a transition function that describes how, in each state, the \FA responds to consuming the characters of the alphabet. In specifying a \FA{}, we also specify the alphabet of its language, the \FA's initial state, and the set of \vocab[final state]{final} or \vocab{accepting states}, those states which, when the \FA halts in them, indicate acceptance of the word.

We can specify the states and transition function in two ways: either in a table, as in Fig.~\ref{lexing:fatables}, or graphically through a \vocab{transition diagram}. A transition diagram has circular nodes for states, typically labeled with the state name, and arrows between states, which indicate the transition function. The arrows are labeled with the character causing the state transition indicated by the arrow. Accepting states are indicated by circling the node representing their states, so that they appear as two concentric circles. Fig.~\ref{lexing:fafigs} provides three examples of transition diagrams.

\input{chapters/background/compilers/fatables}

\input{chapters/background/compilers/fafigs}

The form of the transition function distinguishes between several varieties of \FAs. A transition function that, on any character, permits a transition to only one state is known as a \vocab{deterministic \FA (DFA)}. A transition function that permits a transition to a set of states on any character is known as a \vocab{non-deterministic \FA (NFA)}. It accepts if any state out of the set of states it halts in is an accepting state. A final variety of \FA is distinguished by admitting not only transitions to a set of states, but ``autonomous'' transitions\empause transitions that occur without consuming any of the input. These are known as \vocab{\emptyword-transitions} because transitioning along them ``consumes'' only the empty word \emptyword\ made up of no characters. This variety of \FA is known accordingly as an \vocab{\emptyword--non-determinisitc \FA (\emptyword NFA)}These varieties of \FAs are all equivalent in power\empause it is possible to convert a \FA of one type into another type such that both recognize the same language\empause but some sorts describe a language more naturally or concisely than others. \FAs[F] are unique in that, for a given regular language, there is a \vocab{minimal deterministic \FA{}}, a deterministic \FA with the fewest number of states possible that is unique up to renaming of states.

Figs.~\ref{lexing:fatables} and~\ref{lexing:fafigs} describe the same three \FAs in two ways, both in a table and through a transition diagram. All three \FAs recognize the same language. We can describe this language through \FAs as done here or through the regular expression $(a \alt b)\kstar b$, which will be discussed in the next section. From the transition diagrams, what do you think this regular expression means?

The \emptyword non-deterministic \FA is the most visually complex. It was constructed algorithmically from the regular expression given above by patching together simpler \FAs by way of \emptyword\ transitions. The many \emptyword\ transitions make it highly non-deterministic. The simple non-deterministic \FA was created by identifying states joined solely by \emptyword\ transitions. It is the most elegant of the three. Its sole non-determinism consists in state $0$ having transitions to two different states on the character $b$, both to itself and to the final state $1$. The deterministic \FA was constructed from the non-deterministic. Its state $1$ behaves like the non-deterministic \FA when it is in the set of states $\set{0,1}$, which it enters after encountering a $b$.

\subsubsection{Regular Expressions}
We can also describe regular languages declaratively, using \vocab{regular expressions}. These do not describe how to recognize a given language, but rather describe the language directly. This is done by augmenting the alphabet with a direct linguistic interpretation and by adding special symbols representing operations on this linguistic interpretation. 

The linguistic interpretation associated to a character is direct and intuitive: the character $a$ represents the language consisting of that single character, $\set{a}$. It is natural to generalize this direct representation to words: the word $w$ represents the language consisting of that single word, $\set{w}$. Words are built up by concatenation. To aid in describing many concatenations of a simple structure, we can introduce some notation. Iterated concatenation of a \regex $w$ with itself is represented by superscripts: $w^{0}$ is the language of only \emptyword, the empty word; $w^{1}$ is just $\set{w}$ itself; and, as a rule, $w^{n} = w^{n-1}w$. We can represent unlimited concatenation using the \vocab{Kleene star} $\kstar$: $w\kstar$ represents the set of all concatenations of the language represented by $w$ with itself, including $w^{0}$: $w\kstar = \set{w^{0}, w^{1}, w^{2}, \dotsc}$. If we wish to exclude the possibility of $w^{0},$ we can use the otherwise equivalent \vocab{positive closure} operator $\posclos$: $w\posclos = \set{w^{1}, w^{2}, \dotsc}.$ To represent choice or \vocab{alternation} in the language\empause either \regex $w$ or \regex $v$ is acceptable\empause we can introduce a corresponding operator; $+$ and $\vert$ are both popular choices for representing it: we shall use \alt\ here. Thus, the \regex $a \alt b$ represents the language $\set{a, b},$ while, more generally, the \regex $w \alt v$ constructed by the alternation of the \regexes $w$ and $v$ represents the language $L(w) \union L(v)$, where we use $L(w)$ to represent the language associated to the \regex $w$.\footnote{In general, where $X$ is any description of a language, whether by \TM or \FA or \regex or by any other description aside from the sets representing the languages themselves directly, we write $L(X)$ for the language described by $X$.} Finally, to allow unambiguous composition of \regexes, we can introduce clarifying parentheses. These let us describe, for example, the language $(a \alt b)\kstar b$, the language comprising all strings of zero or more $a$s or $b$s followed by a $b$.

While \regexes are very useful for describing regular languages, they do not provide a way to recognize the languages they describe. Fortunately, regular expressions happen to be readily interconvertible with \FAs.

\subsubsection{Lexers}
With \regexes to describe the lexical structure of tokens and \FAs to perform the actual work of recognizing tokens, we have a ready way to perform tokenization. Simply scan through the character stream till every recognizing \FA will begin to fail; of those that make it this far and will accept, select the token of the highest priority as that summing up the scanned text. The introduction of prioritization and a ``maximal munch'' rule provide ways to resolve ambiguity deriving from our wishing to chunk an input, the program, that in truth belongs to a language unrecognizable by a \FA{}, into words belonging to various token-languages recognized by \FAs.

In truth, however, the lexer is responsible for more than simply recognizing tokens. It works in cooperation with the parser (which we shall describe next) by feeding it a stream of tokens. Further, it records information associated to the tokens, often in a global, shared \vocab{symbol table} associating to each token, or symbol, some information, such as the text or value of the token. It might even use information in the symbol table or information provided by the parser to make a distinction between tokens that it is impossible or exceedingly difficult to make with \regexes alone.

%CRIT-TODO: Add an example, say, simple keyword v. identifier recognition in a small grammar. if versus ifIsMonday?