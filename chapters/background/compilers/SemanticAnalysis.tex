\subsection{Semantic Analysis}
\vocab[semantic analysis]{Semantic analysis}, also known as \vocab{context-sensitive analysis}, follows scanning and parsing. Its job is to ``understand'' the program as parsed. Not all elements of the language can be checked by either regular expressions or context-free grammars; checking these falls to the semantic analyzer. Approaches to semantic analysis vary widely; while a formalism that permits generating semantic analyzers from a higher-level description, as is done for lexers and parsers, exists, its use has yet to become widespread. Frequently, semantic analysis is done purely through \foreign{ad hoc} methods.

A program in truth has two aspects to its semantics, the static and the dynamic. \vocab[static semantics]{Static semantics} are those aspects of the program's meaning that are fixed and unchanging. A common example is the type of variables (though there are languages that employ dynamic typing). These aspects are particularly amenable to analysis by the compiler, and information derived from understanding them can be used to optimize the program. A program's \vocab{dynamic semantics} are those aspects of the program that are only determined at runtime.

Nevertheless, a compiler can attempt to prove through analysis certain properties of the running program, for example, that an attempt to access an array element that does not exist (the eleventh element of a ten-element array, for example) can never occur. Some languages require that the compiler guarantee certain runtime behavior: if it is unable to provide that guarantee at compile time through analysis, the compiler must insert code to check the property at runtime. Java, for example, requires that no out-of-bounds array access occur: any such attempt must be refused and raise an error. Since these runtime checks can slow down a program, a frequent point of optimization in languages requiring such checks is proving at compile-time properties that enable the omission of as many such checks as possible from runtime. Many languages, particularly older languages, do not require runtime checks even where they might be worthwhile, while some compilers might permit disabling the insertion of runtime checks, an option favored by some for the generation of final, production code after all debugging has occurred.

\subsubsection{Attribute Grammars}
The formalism mentioned above for generating semantic analyzers is that of \vocab{attribute grammars}. Attribute grammars can be seen as an evolution of context-free grammars. They begin by taking the grammar symbols of the grammar and associating to each one a finite set of \vocab{attributes}. They next take the grammatical productions and associate to each one a similarly finite set of \vocab{semantic rules}. Each rule is a function that describes how to calculate a single attribute of one of the symbols of the production (which attribute we shall call the \vocab{target} of the semantic rule) in terms of any number of attributes of any of the symbols of the production. Where the same symbol occurs multiple times in the same production, subscripts are used to differentiate the different occurrences of the symbol. To refer to a symbol's attribute, we follow the name of the symbol with a dot and the name of the attribute, so that $A.x$ would refer to the attribute $x$ of the symbol $A$. These conventions are amply illustrated in Fig.~\ref{ag:productions} on page~\pageref{ag:productions}.
\input{chapters/background/compilers/ag/grammar}

\paragraph{Inherited and Synthesized Attributes}
Recall that each grammatical production has two parts, a single symbol called the head and a body of some symbols that is derived from the head. The set of attributes of the symbols are likewise partitioned into two disjoint sets, those that can be the target of a semantic rule when the symbol is in the head of the production, called \vocab{synthesized attributes}, and those that can be the target of a semantic rule when the symbol is part of the body of the production, called \vocab{inherited attributes}.

Consider as an example the second production in Fig.~\ref{ag:productions} on page~\pageref{ag:productions}, $\text{\lstinline{ADDER}}_1 \produces \text{\lstinline{DEF '.' ADDER}}_2$. Any attributes of $\text{\lstinline{ADDER}}_1$ targeted by a semantic rule in this production must be by definition synthesized attributes, while any targeted attributes of the other three symbols \lstinline{DEF}, \lstinline{'.'}, and $\text{\lstinline{ADDER}}_2$ must be inherited attributes. If you check the attributes that are in fact targeted against the table of symbols and attributes in Fig.~\ref{ag:symbols} (also on page~\pageref{ag:symbols}), you will find that this is indeed the case.

You might notice in the same table that some entries are prohibitory dashes. That is because every symbol can have both synthesized and inherited attributes, except terminal and start symbols, which are not allowed inherited attributes. Terminal symbols are often prohibited from having inherited attributes so that attribute grammars can be readily composed to form larger attribute grammars by identifying a terminal symbol of one grammar with the start symbol of another. Likewise, they are allowed to have synthesized attributes with the assumption that the values of these attributes will be provided by some source external to the attribute grammar itself, such as another attribute grammar or the lexer. In terms of a single grammar, the start symbol cannot have any inherited attributes since it is never part of a production body. If we compose grammars, the start symbol will still have no inherited attributes, since we have barred terminal symbols from having inherited attributes.

We could, for example, compose the \lstinline{ADDER} grammar of Fig.~\ref{ag:grammar}, page~\pageref{ag:grammar}, with a \lstinline{NUM} grammar for parsing a variety of numerical formats, such as signed integers and scientific notation. The sole modification we might have to make to the \lstinline{NUM} grammar is to convert the information stored in its attributes for storage in the sole \lstinline{amt} attribute of the \lstinline{NUM} symbol of the \lstinline{ADDER} grammar. This easy composition is made possible by the conventions barring terminal and start symbols from having inherited attributes. If terminal symbols had to take into account inherited information, more extensive modifications of the grammars would often be required before they could be composed.

\paragraph{The Attributed Tree}
In terms of a parse tree, a synthesized attribute is computed from attributes at or below itself in the parse tree, while inherited attributes are computed from attributes at their own level or above them in the parse tree. Thus, the computation of synthesized and inherited attributes can be viewed respectively as information flowing up and down the parse tree.

In fact, the parse tree is central to how attribute grammars are used. A bare parse tree is the fruit of a context-free grammar. With an attribute grammar, we produce a parse tree wherein every node is decorated with its own instances of the attributes associated to its symbol. Every node where a given symbol appears will have the same attributes, but the values of the different instances of the attributes can differ. A parse tree annotated as described with attribute occurrences is called an \vocab{attributed tree}. Fig.~\ref{ag:tree} on page~\pageref{ag:tree} provides a small example of an attributed tree for a word in the language of the attribute grammar of Fig.~\ref{ag:grammar}~(page~\pageref{ag:grammar}).
\input{chapters/background/compilers/ag/tree}

\paragraph{Attribute Evaluation}
With the bare parse tree become an attributed tree, the stage is set for us to use the semantic rules to assign concrete values to the tree's attribute instances. This process of computation is known as \vocab{attribute evaluation}. Provided attribute evaluation terminates, the attribute grammar formalism defines the meaning of the program (which now makes up the leaves of an attributed tree) to be the values of the attributes of the start symbol.

This definition of attribute evaluation, however, is purely descriptive. When we look to perform attribute evaluation, things are not so simple. Attribute evaluation will not necessarily terminate,\footnote{A simple example is the production $A \produces B$ together with the semantic actions $A.x \gets B.x + 1$ and $B.x \gets A.x + 1$, which together cause a loop that repeatedly increments $A.x$ and $B.x$.} and determining an appropriate order for evaluation such that evaluation can be performed efficiently is nontrivial.

Part of making this formalism usable involves, as with context-free grammars, finding restricted classes of attribute grammars that are sufficiently powerful to capture the semantic information desired while still allowing efficient evaluation. Two such classes are the \vocab{S-attributed grammars} and the \vocab{L-attributed grammars}. $S$-attributed grammars admit only synthesized attributes. They can thus be evaluated during a simple bottom-up walk of the parse tree like that performed by a shift-reduce parser. $L$-attributed grammars loosen the restrictions imposed by $S$-attributed grammars somewhat. In addition to synthesized attributes, they allow semantic rules targeting the attributes of a symbol $B_k$ in a production $A \produces B_1 B_2 \dotsm B_n$ to use any attributes of $A$ or $B_1 \dotsm B_{k-1}$. In terms of the attributed tree, this allows a symbol's attributes to be computed in terms of those of either its children (in the case of a synthesized attribute) or those of its parent and the siblings to its left (in the case of an inherited attribute). Like the $S$-attributed grammars, $L$-attributed grammars admit information flow from bottom-to-top within the parse tree, but they also allow for left-to-right information flow. This is a natural match for a left-to-right, depth-first walk of the parse tree, as occurs during recursive descent parsing.

Problems faced by practical implementations of the attribute grammar formalism include the management of storage for the multitude of attribute instances used during evaluation and the amount of attributes that exist solely to share non-local information. Non-local information is in general a problem with attribute grammars, and while a symbol table can be used alongside the grammar to avoid this issue, it is also an end-run around the formalism.

%DONE: Move to conclusion of chapter: Attribute grammars augment context-free grammars with attributes and semantic rules in order to describe the program's semantic meaning. We developed attributed trees from parse trees; we next described how semantic rules are used to perform attribute evaluation in the context of an attributed tree; finally, we discussed the problems inherent in this theoretical framework, such as the difficulty it has handling non-local information such as that often stored in a symbol table in \foreign{ad hoc} methods of semantic analysis.
%DONE: Move to conclusion of chapter: Attribute grammars are useful to far more than compiler writers; they can be put to good use in the generation of debuggers, syntax-aware editors, and, more broadly, interactive development environments.
