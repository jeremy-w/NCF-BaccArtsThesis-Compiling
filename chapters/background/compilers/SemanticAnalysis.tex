\subsection{Semantic Analysis}
\vocab[semantic analysis]{Semantic analysis}, also known as \vocab{context-sensitive analysis}, follows scanning and parsing. Its job is to ``understand'' the program as parsed. Not all elements of the language can be checked by either regular expressions or context-free grammars; checking these falls to the semantic analyzer. Approaches to semantic analysis vary widely; while a formalism that permits generating semantic analyzers from a higher-level description, as is done for lexers and parsers, exists, its use has yet to become widespread. Frequently, semantic analysis is done purely through \foreign{ad hoc} methods.

A program in truth has two aspects to its semantics, the static and the dynamic. \vocab[static semantics]{Static semantics} are those aspects of the program's meaning that are fixed and unchanging. A common example is the type of variables (though there are languages that employ dynamic typing). These aspects are particularly amenable to analysis by the compiler, and information derived from understanding them can be used to optimize the program. A program's \vocab{dynamic semantics} are those aspects of the program that are only determined at runtime. A compiler can attempt to prove through analysis certain properties of the running program, for example, that an attempt to access an array element that does not exist (the eleventh element of a ten-element array, for example) can never occur. Some languages require that the compiler guarantee certain runtime behavior: if it is unable to provide that guarantee at compile time through analysis, the compiler must insert code to check the property at runtime. Java requires that no out-of-bounds array access occur: any such attempt must be refused and raise an error. Since these runtime checks can slow down a program, a frequent point of optimization in languages requiring such checks is proving at compile-time properties that enable the omission of as many such checks as possible from runtime. Many languages, particularly older languages, do not require runtime checks even where they might be worthwhile, while some compilers might permit disabling the insertion of runtime checks, an option favored by some for the generation of final, production code after all debugging has occurred.

\subsubsection{Attribute Grammars}
The formalism mentioned above for performing semantic analysis is that of \vocab{attribute grammars}. Attribute grammars piggy-back on the concepts of context-free grammars and parse trees. They associate to each grammar symbol a finite set of \vocab{attributes} that store information and to each production \vocab{semantic rules} that specify how the attributes of the symbols involved in that production are to be computed. Attributes are partitioned into two sets, those of the heads of productions, called \vocab{synthesized attributes}, and those of symbols of the body of a production, called \vocab{inherited attributes}. These can be viewed as flows of information respectively up and down the parse tree.

Attribute grammars are used like so: Once a parse tree is constructed, its symbols are decorated with \vocab{attribute instances}. Each symbol has its attributes, and each occurrence of that symbol in the tree has its own instances of those attributes; the attributes are common between two occurrences of the same symbol, but the values of their instances of those attributes will likely differ. A parse tree together with its attribute instances is called an \vocab{attributed tree}. It is within this attributed tree that attribute evaluation occurs. \vocab{Attribute evaluation is the computation of the values of the attribute instances of an attributed tree}. Such an evaluation will not necessarily terminate, and determining an appropriate order for evaluation such that evaluation can be performed efficiently is nontrivial.%
%FIXME: This will only be comprehensible with a attributed tree _example_.
%FIXME: Need to introduce an example of nonterminating evaluation/hard to determine order of evaluation giving termination.

Part of making this formalism usable involves, as with context-free grammars, finding sufficiently powerful, restricted classes of attribute grammars that can be used to capture the semantic information desired while enabling efficient evaluation. Two such classes are the \vocab{S-attributed grammars}, which admit only synthesized attributes, and so can be evaluated through a simple bottom-up walk of the parse tree, and the \vocab{L-attributed grammars}, which permit the attribute values of a given symbol in a production to depend only on the inherited attributes of the head of the production and the synthesized attributes of any symbols to the symbol's left; like the $S$-attributed grammars, they admit information flow from bottom-to-top within the parse tree, but they also allow for left-to-right information flow, as well, and can be readily evaluated in a left-to-right, depth-first walk of the parse tree, as occurs during recursive descent parsing.

Problems faced by practical implementations of the attribute grammar formalism include the management of storage for the multitude of attribute instances used during evaluation and the amount of attributes that exist solely to share non-local information. Non-local information is in general a problem with attribute grammars, and while a symbol table can be used alongside the grammar to avoid this issue, it is also an end-run around the formalism.

%TODO: Move to conclusion of chapter: Attribute grammars are useful to far more than compiler writers; they can be put to good use in the generation of debuggers, syntax-aware editors, and, more broadly, interactive development environments.
