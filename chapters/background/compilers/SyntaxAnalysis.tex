\subsection{Syntax Analysis}
\vocab{Syntax analysis} follows lexical analysis. If lexical analysis is concerned with categorizing words by part of speech, then syntax analysis is concerned with understanding how these parts of speech are grammatically related and whether the sentences so formed are grammatical or not.

\subsubsection{Context-Free Languages}
In fact, ``grammatical'' is precisely the word, for the formalism affording ready syntax analysis is that of context-free grammars. As with the regular languages, we are able to describe a given context-free language either constructively or declaratively. The context-free languages are a proper superset of the regular languages and a proper subset of the recursive languages. Roughly, the context-free languages are distinguished from the regular languages by their ability to describe ``matching bracket'' constructs, such as the proper nesting of parentheses in an arithmetic expression, while the recursive languages are distinguished from the context-free languages in part by their ability to cope with context.

\subsubsection{Context-Free Grammars}
We use \vocab{context-free grammars} to specify context-free languages declaratively. As with \regexes and \FAs, context-free grammars operate in the context of a specific alphabet. The letters of the alphabet are called \vocab{terminals} or \vocab{terminal symbols}. \CFGs[C] augment this alphabet with a finite set of \vocab{non-terminals (non-terminal symbols)} to be used in specifying grammatical \vocab{productions}, which function as \vocab{rewrite rules}. Together, the set of terminal and non-terminal symbols are called \vocab{grammar symbols}, as they specify all the symbols used by the grammar. Analogous to the start state of the \FA is the context-free grammar's distinguished \vocab{start symbol}. All words in the language described by the \CFG are derived from the start symbol via the productions in a manner to be described shortly.

Putting these rules together, one arrives at a grammar specification like the following:
\begin{equation*}
\begin{split}
&G = \left(N, T, \Sigma, P, S\right) \qquad N = \set{A, B} \qquad T = \set{a, b} \qquad \Sigma = {a, b}\\
&P = \{S \produces A, \quad S \produces B, \quad S \produces aABb, \quad
    A \produces a \alt \emptyword, \quad B \produces b \alt \emptyword\}
\end{split}
\end{equation*}
where $N$ is the set of non-terminals, $T$ the set of terminals, $\Sigma$ the alphabet, and $P$ the set of productions, where \produces\ is read as ``produces.'' %(The symbol \produces is also sometimes written \altproduces.)
The symbol to the left of the arrow is called the \vocab{head} of the production, while those to the right are called the \vocab{body}. For example, in the production $S \produces aABb$, $S$ is the head of the production and $aABb$ is the body:
\[
\underset{\text{\normalsize\textit{head}}}{\underbrace{S}} \produces \underset{\text{\normalsize\textit{body}}}{\underbrace{aABb}}
\]
Derivation proceeds by substitution of production bodies for production heads: for example,
\begin{equation}\label{syntax:deriv}
S \derives[S \produces aABb] aABb \derives[A \produces a] aaBb \derives[B \produces b] aabb
\end{equation}
where \derives\ is read as ``derives in one step'' and the rule justifying the derivation is written above the arrow. Taking a cue from regular expressions, we can also write \derives[\star] for ``derives in zero or more steps'' (all grammar symbols derive themselves in zero steps) and \derives[+] for ``derives in one or more steps,'' where the productions justifying the derivation are implicit in the superscript star; the keen reader should perhaps like to construct their own explicit, step-by-step derivation. The language defined by the grammar is defined to be those strings made up only of terminal symbols that can be derived from the start symbol.

\paragraph{Parse Trees} We can use a \vocab{parse tree} to represent the derivation of a word in the language without concern for unnecessary sequencing of derivations imposed by our sequential presentation. For example, our choice to derive $a$ from $A$ prior to deriving $b$ from $B$ above is irrelevant, but that we first derived $aABb$ from $S$ before performing either of the remaining derivations is not, since the heads of these derivations are introduced by the derivation from $S$. We define parse trees constructively: 
\begin{aenumerate}
\item Begin by making the start symbol the root.
\item\label{parsetree:construction:choose} Select a non-terminal on the leaves of the tree with which to continue the derivation and a production for which it is the head.
\item Create new child nodes of the chosen head symbol, one for each symbol in the body.
\item Repeat from \ref{parsetree:construction:choose}.
\end{aenumerate}
At any point in time, the string of symbols derived thus far\empause those on the leaves, read in the same order applied to the child nodes in the body of a production\empause is called a \vocab{sentential form}. The process terminates when a word in the language is derived, as no non-terminal leaf nodes remain. Fig.~\ref{syntax:parsetree} on page~\pageref{syntax:parsetree} gives the parse trees created in deriving $aabb$ from our example grammar.

\input{chapters/background/compilers/deriv.tex}

\paragraph{Ambiguity} Parse trees represent the derivation of a word without regard to unnecessary sequencing. A given tree represents a given parse. If more than one parse tree can derive the same word in the language, the grammar is said to be \vocab{ambiguous}. This corresponds to the use of a significantly different ordering of productions and potentially even of a different set of productions. The grammar is called ambiguous because, given such a word, it is uncertain which productions were used to derive it. The grammar we gave above is ambiguous when it comes to the empty word \emptyword, because $S \derives A \derives \emptyword$ and $S \derives B \derives \emptyword$ are both valid derivations with corresponding significantly different valid parse trees of \emptyword. However, if we were to eliminate the productions $S \derives A$ and $S \derives B$ from the grammar, we would then have an unambiguous grammar for the context-free language comprising $\set{ab, aab, abb, aabb} \equiv \set{a^{i}b^{j} \where 1 \leq i, j \leq 2}$. The only sequential choices are insignificant: in deriving $aabb$, we must have derived $aABb$ from $S$, but following that, did we first derive the second $a$ or the second $b$?

\paragraph{Derivation Order} While the parse trees for a word in a language factor out differences between possible derivations of the word other than those reflecting ambiguity in the grammar, when performing a derivation or constructing such a parse tree, we must employ such ``insignificant'' sequencing. There are two primary systematic ways to do so: always select the leftmost nonterminal symbol in step \ref{parsetree:construction:choose} of the parse tree construction process, or always select the rightmost. These ways of deterministically choosing the next symbol to replace in the derivation give rise to what are unsurprisingly known as a \vocab{leftmost derivation} and a \vocab{rightmost derivation}; to indicate the use of one or the other, the derivation arrow in all its forms is augmented with a subscript of $lm$ for leftmost derivation and $rm$ for rightmost, giving \derives[][lm] and \derives[][rm]. Since this is only a matter of choice in constructing the parse tree, it should be clear that, for any given parse tree, there exist both leftmost and rightmost derivations of its sentential form. %Leftmost and rightmost derivations will be important when we discuss parsing, which, given a word, must solve the problem of finding a valid derivation. (The problem of what to do if the sentence should prove ungrammatical, that is, if no valid derivation exists, is another problem in itself, that of error handling.)

\subsubsection{Pushdown Automata}
We can also specify context-free languages constructively using an abstract machine called a \vocab{pushdown automaton}. A \PDA is a \FA augmented with a stack and associated stack alphabet. It has an initial stack symbol as well as an initial state. Its transition function and behavior is complicated by its being inherently non-deterministic. As might be expected, its transition function is parameterized by the current input symbol, current state, and the symbol currently on top of the stack. However, for each such triple, the transition function specifies a set of pairs. Each pair consists of a state and a sequence of stack symbols with which to replace the current top of stack. For a given triple, the \PDA simultaneously transitions to all the states indicated by the transition function and replaces the symbol on top of the stack with the corresponding symbols for each new state it is in. Each one of these can be treated as a new \PDA. To ``move,'' each member of the family of \PDAs consults the current input, its state, and the top of the stack, and then transitions accordingly. We again have a choice of representing this either with a table or graphically. While \FA transition diagrams had arrows labelled %
%FIXME: The paragraph introducing PDAs likely needs more work. These things are kind of confusing to explain. Yay non-determinism...
\[
\token{input symbol}
\]
the arrows of \PDA transition diagrams are labelled 
\[
\token{input symbol}, \token{stack symbol to pop} / \token{stack symbols to push}
\]
where the convention used for the stack operations is that the symbol that is to be on top of the stack after pushing is leftmost (that is, the stack conceptually grows to the left).

There are some casualties of the transition from \FAs to the increased descriptive power of \PDAs. \PDAs are inherently non-deterministic: they always admit \emptyword-transitions and can be in a set of states at any given time. This non-determinism is essential for them to define the context-free languages. The languages described by deterministic \PDAs, while still a proper superset of the regular languages, are only a proper subset of the context-free languages. Further, there is no algorithmic way to produce a minimal \PDA for a given language. This poses a particular problem for parsing: as with lexing, we would like to use grammars to describe the syntactic structure and \PDAs to perform parsing by recognizing that structure, but we must now find some way for our inherently deterministic computers to cope with this inherent non-determinism in a reasonable amount of time.

\subsubsection{Parsers}
As exaggeratedly hinted at above, while grammars define a language, parsers are faced with an input that they must characterize as either of that language or not. They must, in fact, do more than simply check that their input is grammatical: they must construct an \IR of their input to pass on to the next part of the compiler.\footnote{If, indeed, there is a next pass: it is possible to construct one-pass compilers that translate from source to target in a single pass over the source code.}

We also mentioned the problem of the non-determinism inherent to \CFGs and \PDAs. So long as we only face insignificant questions of sequencing, we will have no problem determining what to do next. Realistic inputs do not require truly non-deterministic parsing. A program is meant to have a single meaning: to correspond to a parse tree, not a parse forest. Non-determinism occurs in parsing a programming language when the available context is insufficient to predict the shape of the parse tree, and it becomes necessary to entertain several possibilities simultaneously. Eventually, more context will be available to resolve the ambiguity, and we can return to building a single parse tree and abandon the others as false starts. Problems such as these are likely to affect only part of the input, and methods have been developed that handle such ``temporary non-determinism'' gracefully. %NOTE: Run this paragraph by Henckell again. It was a confusing one before.

The remainder of our discussion of parsers will focus on several of the more common of their many types. The level of our discussion will be one of summary, not of definition; for details, the interested reader is referred to the literature discussed in \partandnameref{Section}{background:conclusion:bibliographicnotes}.

\paragraph{Recursive Descent Parsers}
Recursive descent parsers discover a \emph{leftmost derivation} of the input string during a \emph{left-to-right scan} of the input, whose alphabet, thanks to the lexer, will be tokens rather than individual letters and symbols. One function is responsible for handling each token; parsing begins by calling the function associated to the start symbol. They discover the derivation by recursively calling themselves as necessary. The parser is aware of the current input symbol via what is known as \vocab{lookahead}. Since we are dealing with an actual machine, however, we are not restricted to lookahead of a single symbol, though we might prefer to do with only a single symbol's lookahead for effiency' sake. Those grammars parsable by a recursive descent parser with $k$ tokens of lookahead are known as \vocab{LL(k)}: leftmost derivation by left-to-right scan employing $k$ tokens of lookahead.

When recursive descent parsers use one token of lookahead, they act much like a \PDA. The implicitly managed function call stack acts as the \PDA's stack. However, since they trace out a leftmost derivation with only a limited number of tokens of lookahead, they must anticipate the proper derivation with minimal information about the rest of the input stream. This makes recursive descent parsers one of the most limited forms of parsers, though they might be the parser of choice in some cases because of the naturalness of expression they can admit and the simplicity and compactness of their parsers. Many of the disadvantages of recursive descent parsers can be overcome by admitting variable tokens of lookahead, with more tokens being used as needed to disambiguate the choice of production. 
%TODO: Mention ANTLR in the conclusion!

\paragraph{Precedence Parsing}
Recursive descent parsers are sometimes coupled with precedence parsers in order to facilitate parsing of arithmetic expressions. The order in which operations should be carried out is determined by a frequently implicit grouping determined by operator associativity and precedence. For example, multiplication is normally taken to have higher precedence than addition, so that $3 \times 5 + 4$ is understood to mean $(3 \times 5) + 4 = 19$ and not $3 \times (5 + 4) = 27.$ The left associativity of multiplication determines that $2 \times 2 \times 2$ should be understood as $(2 \times 2) \times 2.$ This becomes important, for example, in cases where the operands have side effects: suppose \code{id} is a unary function printing its input and then returning its input unchanged. Assume further that arguments to operators are evaluated left-to-right. Then \code{id}(1) + \code{id}(2) + \code{id}(3) will print \code{123} if addition is understood to be left-associative, but it will print \code{231} if addition is understood to be right-associative, even though the result of the additions will be identical due to the associativity property of addition.\footnote{If argument evaluation proceeded right-to-left, \code{213} and \code{321} would be printed instead.}

Operator precedence parsing is preferred over the use of LL$(k)$ grammar rules not only because it is somewhat unobvious how to enforce the desired associativity and precedence in an LL$(k)$ grammar, but also because doing so introduces a chain of productions that exist solely to enforce the desired associativity and precendence relations between the expression operators. Beyond its use in concert with recursive descent parsers, precedence parsing has mostly been subsumed by the class of grammars we shall describe next. The central idea of using precedence and associativity to disambiguate an otherwise ambiguous choice of productions has lived on in implementations of parser generators for this later class. Without recourse to a way other than grammatical productions to indicate precedence and associativity, grammars would often have to take a form that unnecessarily obscures their meaning simply to grammatically encode the desired precedence and associativity relations.

\paragraph{LR$(k)$ Parsers}
The \vocab[LR$(k)$ family of parsers]{LR$(k)$}\empause left-to-right scan, rightmost derivation with $k$ tokens of lookahead\empause family of parsers is perhaps the most commonly used in practice. I say ``family'' because a number of subtypes (to be discussed shortly) were developed to work around the exponential space and time requirements of the original \abbrev{LR}$(k)$ algorithm. The class of grammars recognizable by an \abbrev{LR}$(k)$ parser is known as the \abbrev{LR}$(k)$ grammars, and it is possible to give a reasonably straightforward \abbrev{LR}$(k)$ grammar for most programming languages. However, it was some time before clever algorithms that avoided unnecessary requirements of exponential space and time were developed, and so other, more restrictive classes of grammars with less demanding parsers were developed and deployed. Parser generators targeting these classes are more limited in terms of the grammars they can generate parsers for, not in terms of the languages such grammars can recognize: all parsers of the LR$(k)$ family, where $k > 0$, accept the same class of languages; they simply place different, more or less restrictive demands on the form of the grammars describing those languages. %NOTE: This para was confusing in the past. Run it by Henckell again.

Where LL$(k)$ parsers create a derivation from the top down by starting with the goal symbol and eventually building a derivation for the input, LR$(k)$ parsers build a rightmost derivation in reverse by reading in the input till they determine that they have seen the body of a production and then reducing the body to the head. They eventually reduce the entire input to the start symbol (often in this context called the \vocab{goal symbol}), at which point parsing is complete. They use a stack to store the symbols seen and recognized so far, so in the course of parsing they carry out a very limited set of actions: shifting input onto their stack, reducing part of the stack to a single symbol, accepting the input as a valid word in the grammar, and indicating an error when none of the above applies. Because of this behavior, such a bottom-up parser is often called a \vocab{shift-reduce parser}.

\paragraph{SLR$(k)$ Parsers}
The earliest and most restricted such class is known as the \vocab{simple LR$(k)$}, or \abbrev{SLR}$(k)$. These parsers use a simplistic method of determining what action to take while in a given state and reading a given input that introduces conflicts that more sophisticated methods would be capable of resolving. In a shift-reduce parser, there are two possible types of conflicts: 
\begin{description}
\item[\vocab{shift/reduce conflicts}] where the parser has seen what it considers the body of a valid production at this point in the parse but has also seen a viable prefix of yet another production, so it cannot determine whether to reduce using the former or shift further symbols onto the stack in an attempt to recognize the latter.
\item[\vocab{reduce/reduce conflicts}] where the parser has seen the entirety of the body of two productions that appear to be valid at this point in the parse and is unable to determine which to reduce to.
\end{description}

\paragraph{LALR$(k)$ Parsers}
More sophisticated parsing methods are more discriminating about what productions are still valid at a given point in the parse by taking into account more or less of the parsing process and input seen thus far, so called \vocab{left context} as it is to the left of where the parser presently is in consuming the input. (In this analogy, the lookahead symbols could be considered right context, though that term is never used.) One such method is known as \vocab{look-ahead LR (LALR)}. These parsers can be seen as ``compressed LR parsers,'' though this compression can introduce spurious reduce/reduce conflicts that would not occur in a full LR parser. This has historically been seen as an acceptable tradeoff for the reduction in table size and construction time, since any LR grammar can be reformulated as an LALR grammar, but with more sophisticated LR algorithms developed later that retained the full power of full LR parsers while producing comparable levels of compression wherever possible (meaning that parsing an LALR grammar with such an LR parser would require the same space as parsing it with an LALR grammar), such a tradeoff became unnecessary, though it remains widespread. 

%TODO: Mention that similar methods allow creating a table-driven implementation of a DFA lexer.

\paragraph{Table-Driven Parsers}
Whereas recursive descent parsers and operator descent parsers can be hand-coded, many of the other parsing algorithms were developed to operate by way of precomputed tables.\footnote{That is not to say that the others cannot also be implemented through tables, simply that the table method is not felt to be the necessity that it is for these others.} They explicitly model a \FA, called the \vocab{characteristic finite automaton}; the tables allow the transition function to be implemented purely by table lookup. As hand-creation of tables is time-consuming and error-prone, tables for parsing are generally created algorithmically and the resulting tables used with a \vocab{driver} that simply does little more than gather the information necessary to perform the operations specified by the table.

\paragraph{Direct-Coded Parsers}
Parsers implemented entirely in code (rather than as a set of tables with a driver) were long seen as something to be generated only by humans, while parsers generated from a higher-level grammar description were to be implemented by way of tables. However, another possibility, often faster and smaller because of its lower overhead and its lack of a need to encode a rather sparse table, is to have the parser generator create a direct-coded parser, a parser that is not table-driven but yet is generated from a higher-level description rather than being written by hand.

\paragraph{GLR Parsers} LR parsers are restricted to parsing only LR languages. However, a very similar technique can be used to parse all context-free languages. \vocab{Generalized LR (GLR) parsers} are more general than LR parsers in two senses: 
\begin{itemize}
\item They are able to parse all context-free grammars, not just LR grammars.
\item Their method of parsing is a generalization of that used in LR parsers.
\end{itemize}
They generalize the parsing method of shift-reduce LR parsers by coping with ambiguity in the grammar by duplicating the parse stack and pursuing competing parses in parallel. When they determine a particular parse is in fact invalid, it and its stack are destroyed. If the grammar is in fact ambiguous and multiple parses are possible, this might lead to a \vocab{parse forest} instead of a parse tree. Making such parsers feasible requires some effort, and part of that effort was to replace several duplicate parse stacks by what amounts to a ``parse lattice'' that share as many grammar symbols as possible as parses converge and diverge, much reducing the space requirements of the parser as well as time spent repeating the identical shifts and reduces on different parse stacks. It is also important to employ similar compression methods as with the newer LR parser generation algorithms, so that extra space and time is only employed as strictly necessary to deal with non-LR constructs.%
%FIXME: Henckell wants an example of a non-LR construct. Would be a good idea. See Elkhound TR; hopefully it gives one.

\paragraph{Semantic Actions}
We generally desire to know more than that a given input is grammatical: we want to create a representation of the information discovered during parsing for later use. This is done by attaching \vocab{semantic actions}, to productions in parsers and to recognized tokens in lexers. Such actions are invoked when the production is reduced or the token recognized, and they are used to build the representation and, in the lexer, to emit the recognized token for the parser's use. They also can be used to compute attributes of the nodes in the parse tree, as discussed next.
%TODO: talk about scannerless parsers -- from incestuous commingling to rad idea -- in the conclusion/bibliography. From modularity to power and effectiveness....

