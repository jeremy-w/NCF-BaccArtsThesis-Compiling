\section{Back End: Generating Target Code}\label{background:compilers:back}
The \vocab{back end} is responsible for completing the work of a compiler. It receives the program in some \IR{}, itself might construct various further \IRs of the program, and ultimately produces the final representation, the program in the target language. The \IR expresses a computation in a form understood by the compiler. The back end must take this and express it in the target language. This requires finding translatable units and recording their translation, then sequencing these translations for the best effect. This translation must obey whatever resource limits exist in the target language.

Here, we will focus on an \ISL as the target language. In this setting, the task of choosing how to represent the elements of the \IR in the target language corresponds to instruction selection; ordering the translations corresponds to instruction scheduling; and working within the limits of the target language corresponds to register allocation.\footnote{This is true if the \IR treats all data as being in registers except when it cannot. If the \IR instead leaves all data in memory and moves it into registers for only as long as necessary, then \vocab{register promotion}, which is the process of figuring out what data can be promoted from storage in memory to storage in register and then promoting it, is a better word for what occurs than register allocation. This promotion step is more a matter of taking advantage of the power of the language rather than one of restricting the translation to obey the language's limits. We will discuss register allocation here, but similar techniques apply to register promotion.}

These tasks are not cleanly separated. Choices made in each can (and when they cannot because of particular architectural decisions, they perhaps should) affect the others. The instructions selected to express a particular subcomputation can increase or decrease the demand on registers, which can require instructions be inserted to free up registers for other computations. The introduction of new instructions would strongly suggest that the whole sequence of instructions be rescheduled, which can again introduce problems with register load. Nevertheless, we will discuss them separately, because that is how they are best dealt with.

\subsection{Instruction Selection}
Instruction selection provides the basic material of the program in the target language. While instruction scheduling and register allocation are necessary for correctness, they simply rework the instructions generated in instruction selection.

Instruction selection is tasked with bridging from an \IR to the actual target language. As with bridges, the nearer one side is to the other, the easier it will be to bridge the gap: the closer the \IR is to the target language, the easier the job of instruction selection. If the \IR is not very low-level, it will be necessary to convert it to something low-level. This will likely not be a very clean conversion if left to so late in compilation; there will be a lot of code meant to work around possible problems that may or may not be present because the results of earlier analyses that matter at a low level were not represented in the \IR. If the \IR is low-level, but its architectural model differs from that of the target platform\empause the \IR is stack-based or resembles the assembly language of a \abbrev{RISC} machine, while the target platform is a \abbrev{CISC} machine, say\empause it will be more difficult to perform instruction scheduling.

However difficult it might be, the same basic ideas suffice for instruction selection. To avoid clouding the exposition, we will assume the low-level \IR that enters instruction selection is tree-based. A simple approach would simply walk the tree and generate, whenever possible, general instructions that ignore related nodes. A more complex approach would attempt to use local optimization and awareness of related nodes to build up a sequence of instructions.

A rather different approach uses a technique called peephole optimization that was originally developed to perform some last optimizations on the target code. It used a library of patterns to simplify a small window, or peephole, of a few instructions at a time. By scrolling this window through the entirety of the target code, less efficient code patterns could be replaced with more efficient counterparts. The limited window size keeps the process very quick, but all the same, it is able to perform some useful optimizations.

\subsubsection{A Simple Tree-Based Approach}
The simplest approach would generate instructions during a single tree walk. This would not be much more complicated than flattening the tree. It would also not produce very good code, since it would either make no use of context or only very limited use. Context is essential to producing a decent instruction sequence. A number of instruction sequences can be used to encode even straightforward arithmetic statements. Consider \code{x = y + 4}. %
%TODO: Insert tree representation of x = y + 4.
Focusing on one node at a time, a \RISC{}-like instruction sequence might load \code{y} into a register, load \code{4} into a register, sum those values and store the result in yet another register, which becomes \code{x}. %
%FIXME: This is an ugly hack. Convert these to listings in floats. Side-by-side listings positioned as tbp would be good.
\begin{align*}
r_{y} &\gets y\\
r_{4} &\gets 4\\
r_{y + 4} &\gets \text{\code{add}}\quad r_{y}\quad r_{4}\\
r_{x} &\gets r_{y + 4}
\end{align*}
If we consider a bit more of the tree, we might load the value of \code{y} into a register, then use an immediate addition operation to compute \code{y + 4} and store the result into a register that represents \code{x}.
\begin{align*}
r_{y} &\gets y\\
r_{x} &\gets \text{\code{addi}}\quad r_{y}\quad 4
\end{align*}

Na\"ively generating code to access array elements (which is how local variables are frequently represented at a low level in a program) can result in many redundant computations as part of the offset from the start of the array is calculated and recalculated and shared elements of the computations are not reused. Trying to eliminate redundant computations significantly complicates the code with special-case optimizations. Traveling any distance along this route of attempting to hand-optimize a simple scheme strongly suggests that more complex methods be employed. Fortunately, more complex methods are available.

\subsubsection{Tree Pattern-Matching}
Tree pattern-matching methods are instruction selection methods that use a store of tree patterns to build instructions. A common approach is that of \vocab{bottom-up rewrite systems (BURS)}. \BURS[Long]{pl} work by tiling the tree with a stock of predefined patterns. As each node is subsumed by a pattern tile, a choice is made based on the tiles of its subtrees that minimizes the cost of the tiling. The costs can be fixed or allowed to vary dynamically during the rewriting, say, to reflect the demand on registers introduced thus far. The costs can represent whatever it is one wishes to optimize for during instruction selection: code size, power consumption, execution time, or whatever else.

The patterns and costs used by a \BURS[long]{sg} in tiling the tree can readily be represented in a table, which suggests the use of ``code generator generators'' similar to the lexer and parser generators used in producing the front-end, and such do exist. The rewrite rules make use of \CFGs{}. Productions represent the shape of the tree. Costs are associated to each production, along with code templates. This is quite similar to an attribute grammar, and a \BURS[long]{sg} likely could be described in that framework.

To tile the tree, we work from the bottom up, considering one node (as the root of a subtree) at a time. The tiling proceeds by identifying productions whose bodies match the subtree headed by the node currently under consideration. The least costly production is selected, and we move on to another node and its subtree. All the information we need know about a subtree is encoded in the head of the production selected when its root was considered. Once we have tiled the entire tree, a traversal fills in the code templates and records the instruction sequence.

This tree pattern-matching process is highly suggestive of number of other processes, which suggests adapting their techniques to fit this purpose. There are the classic pattern matchers, the \FAs{}; the \CFG component as well as the bottom-up method suggests adapting parsing techniques; a tree flattened into a string could perhaps be attacked using string matching methods (which, in many cases, ultimately end in use of \FAs); or, now that the problem is better understood, we can hand-code a tree pattern-matching program.

\subsubsection{Peephole}
Peepholes are generally thought of in terms of \vocab{peephole optimization}, as briefly described at the start of our discussion of the back end. However, their methods can also be used for instruction selection alongside optimization. The problem again reduces to pattern matching, but unlike in our discussion of tree pattern-matching, we assume the \IR used for pattern matching with a peephole is linear, like the assembly code instruction sequences that peepholes were intended to optimize.

Instruction selection through a peephole begins by transforming the \IR to an especially low-level form that models all the side-effects of instructions on the target machine. The peephole is used to simplify this instruction sequence and then to match patterns in this simplified sequence. These patterns are associated with code in the target language. Unlike the bottom-up methods used for trees, here the patterns are matched linearly and sequentially (visually, from top to bottom in the normal way of writing code).

What kind of simplifications can be seen through a peephole? Within a peephole, we can avoid unnecessary storage of temporary values in registers by substituting the value itself in place of its register in operations using the register. We can recognize a store followed by a load of the same value. Some simplifications might enable other simplifications. If we give the simplifier knowledge of when a given value is used by preprocessing the expanded low-level \abbrev{IR}, we can jump back up and eliminate computation and storage of a value if later simplifications eliminate all its uses. Control flow complicates matters: should we use a strictly static window, or should our window include instructions that might follow during execution? Should we look at all uses of a value together, ignoring intervening instructions, and proceed that way? The basic idea is amenable to considerable sophistication; the pattern-recognition and simplification part is, as with tree pattern-matching, also producible through a generator, at least as far as its basic elements go.

Clever implementation can enable the instruction selector to learn about simplification patterns. One can use well thought-out heuristics to quickly generate and test a variety of instruction sequences of various costs, simply by pasting together operations. Sequences that do no have the same effect as that identified for improvement are quickly discarded. Guided by a skilled compiler implementor and a suitable sampling of programs, this exhaustive search approach can be used during development to generate a sophisticated library of simplification patterns for later use.

\subsection{Instruction Scheduling}
Instruction selection produces a sequence of instructions, but its concern is generating instructions that can carry out the needed computations, not making sure all the instructions will work together: Does a use of a value come too soon after its definition, while the value is still being computed and not yet available? Will this introduce an error in the program, or simply unnecessary delay? Instruction scheduling worries about problems like these. It tries to ensure the selected instructions will work well together by reordering them. Its prime directives are to enhance instruction-level parallelism and reduce the time required by the program. It works at the block-level so that it does not have to deal with the consequences of control flow. It is hoped that stitching the scheduled blocks together will result in a good enough overall schedule, and this hope is generally realized.

What limits are placed on reordering? These limits are best expressed in terms of \vocab{dependences} between instructions. We always speak of the later instruction as depending in some way on the earlier. There are a variety of ways one instruction can depend on another. Perhaps the most obvious sort of dependence is \vocab{control dependence}, when a sequence of instructions only executes if some condition is met. Reordering must also respect \vocab{data dependences} in the initial instruction sequence: we cannot use a value before it has been defined. It also must respect or eliminate so-called \vocab{antidependences}. These are dependences between instructions that exist, not because of data flow, but because of conflicting uses of the same resources: specifically, one instruction is antidependent on another, which must follow it, when it makes use of a value in a resource and that resource is overwritten by the other instruction. For example, if one instruction uses a value in a register that is clobbered by the subsequent instruction, reversing the order of these two instructions would prematurely clobber the register and lead to the now following instruction reading the wrong value out of the register: %
%FIXME: To listing.
\begin{align*}
r_{y} &\gets r_{x} + 1\\
&\hphantom{\gets}\vdots\\
r_{x} &\gets 5
\end{align*}
A closely related kind of dependence is \vocab{output dependence}, where two instructions both modify the same resource. If we expand on the previous example by introducing the instruction that creates the value of $r_{x}$ %
%TODO: using math here for registers r_{blah}; this needs to be checked later to ensure consistent typefaces between the listings and here, and between all such uses for r_{blah}. All are not flagged; just grep r_ and follow up.
used in the addition and update of $r_{y}$, then the second instruction setting $r_{x}$ would be output-dependent on the first. You might wonder whether output's dual, input, also has a related dependence. It does not, as instructions are not kept from having their orders reversed simply because they read from the same resource. However, the idea can be useful, so some compilers will track it nevertheless as a sort of \vocab{input pseudo-dependence}.\footnote{Looping structures present a mess of dependence problems of their own. However, we do not discuss them here, as they are generally the target of analysis and optimization in the middle end.}

These dependences can be used to create a \vocab{dependence graph} (also called a \vocab{precedence graph}) representing the program, where each instruction is a node and there is a directed edge from a first node to a second whenever the second depends on the first. The graph is used along with information about the target platform to produce a schedule, which associates each instruction-node with a positive integer specifying the cycle in which it should be issued. The information needed is the functional units required by the instruction and the number of cycles the instruction takes to execute, called the \vocab{delay} of the instruction. If no value is required in the schedule before it is ready, the schedule will be correct. If there are never more instructions executing than the functional units can handle, and there are never more instructions dispatched in a cycle than is possible for the target platform, the schedule will be feasible. Within these constraints, we must attempt to schedule the instructions so that all dependences are respected and the cost of the schedule (often, the amount of time it requires) is minimized. This, of course, is an ideal that we cannot guarantee in practice.

The graph can be usefully annotated with the cumulative delay to each node starting from a root. The path from a root of the dependence graph to the highest-numbered leaf is called the \vocab{critical path} and critically determines the length of the schedule: no matter what, it can take no less time than the annotation at the leaf endpoint of the critical path.

\subsubsection{List Scheduling}
The dominant paradigm for scheduling is called \vocab{list scheduling}. The basic idea of the method is, first, to eliminate antidependences by renaming the contested resources; next, to build the dependence graph; then, to assign priorities to each operation (for example, as determined by the cumulative delay at that operation's node); and, finally, to schedule the instructions in priority order as their dependences are fulfilled. This last step simulates the passage of cycles in order to track when operations can safely be scheduled and record the resulting schedule.

Clearly, there is a lot of detail missing from this sketch. The priority scheme used, for example, has an important effect on the resulting schedule, as does the tiebreaking method between operations with identical priorities. There is no consensus on the best priority scheme, likely because there is no such thing. Additionally, in simulating the passage of cycle to schedule the operations, there is a choice to work forward or backward in time. Working forward, leaves are dealt with before roots. The first operation scheduled executes first. Working backward, roots are schedule before leaves, and the first operation scheduled executes last. Neither forward nor backward scheduling is always best; since scheduling is fairly easily done, often a block will be scheduled both forward and backward, possibly a few times using different priority schemes, and the best of the schedules produced is then chosen.

There is also a lot of room for elaborating the method. Why limit ourselves to scheduling block-by-block? We can produce a better overall schedule if we look beyond a basic block. If either of two blocks can follow a single block, each successor block will work best with one or another scheduling of the predecessor, but we can only schedule the predecessor in one way. How should we decide which successor should determine the schedule of the predecessor? If we were to generate code for the region we wish to schedule, run it several times, and track which blocks execute most frequently, we could make an informed decision. Some schedulers take this tack, called \vocab{trace scheduling} since it makes use of an \vocab{execution trace}, or record of execution.%
%NOTE: We omit discussion of loop scheduling.
%TODO: Mention loop scheduling and give pointers in the bib refs.

\subsection{Register Allocation}
Register allocation is the final step of code generation. Instructions have been generated and scheduled. Now, it is time to ensure they meet the platform's register constraints. The most fundamental constraint imposed on registers is the number available, but others must also be taking into account. These include constraints on register usage imposed by calling conventions and those imposed by register classes.

Register allocation is, in fact, an umbrella term for two closely related tasks: \vocab{register allocation}, which is the task of deciding which values should reside in registers at each point in the program, and \vocab{register assignment}, which takes the values to be allocated to registers and decides which register should hold which value at each point in the program.

Often, all values cannot be kept in registers. Dumping the register's contents to memory is called \vocab{register spilling}. Spilling a register is necessary but expensive. At each use, the data must be loaded into a ``scratch register,'' and any changes to the data must be stored back to memory in order to free up the scratch register to load other spilled values. Sometimes, it is cheaper to recompute a value at each use than to go through the expense of spilling it and loading it back. Recomputation in place of register spilling is referred to as \vocab{rematerialization}: rather than using previously provided ``material,'' we are recreating it as needed.

Clearly, register allocation directly affects register assignment. Unfortunately, the interaction of the two concerns\empause what values should be kept in registers, and which registers should they be kept in\empause are not cleanly separated. Since we might be bound to assign particular sorts of values to particular registers, issues of assignment can affect register allocation: we might be unable to use floating point registers for anything except floating point values, and a calling convention will likely specify that arguments to the procedure must be stored in specific registers, not just in some registers. Since marshaling data to and from register and memory itself requires registers, we do not even have the whole register set available.

The overall process of register allocation (which is what we shall mean by ``register allocation'' from now on), then, is nontrivial.\footnote{In fact, it ends up being NP-complete for any realistic formulation of the problem. A polynomial-time algorithm exists for the simplest of cases, as well as for \SSA form, but virtually any additional complexity\empause including the translation from \SSA from into the processor's ISL\empause promotes the problem to NP-completeness. Naturally, any time you actually find yourself needing to perform register allocation, you will not be dealing with the polynomial-time case.} As with much in compiler design, we are must resort to heuristics. We wish to minimize the amount of register-memory traffic by maximizing the amount of data kept in registers. A simple register allocator would consider only a block at a time. At the end of a block, it would spill all its registers. (A following optimization pass could attempt to remove unnecessary spills.) A top-down approach would estimate how many times a value is used in the block, allocate those to registers throughout the entire block, and spill the rest of the values used in the block to memory. A bottom-up approach would work through the block, instruction by instruction, and ensure that the operands of each instruction are in register. Where possible, it will use values already in register and load values into free registers. When all registers are full and a value not in register is needed, it will spill the value whose next use is farthest from the current instruction.

The top-down approach works, in a sense, by using detailed information about the block to set an overall policy, which it then follows. The bottom-up approach also uses detailed information about the block, but it makes its decisions instruction by instruction, rather than following an overall plan for the block. Its only plan is the same for all blocks: make the tough decisions (which values to spill, which registers to use?) when it has to. This top-down--bottom-up dichotomy persists through all types of register allocators, though much the same effect can be achieved either way.\footnote{It is an artifact of our simple description that the top-down allocator will dedicate a register throughout the entire block to a value heavily used in the block's first half but unused in its second, while the bottom-up allocator will choose to spill the value once it is no longer needed. Getting the top-down allocator to behave similarly, however, would make it less simple.}

More complex algorithms are required to handle register allocation across greater regions than single blocks. Modern register allocation algorithms often draw their inspiration from the graph coloring problem. Because of this, they are called \vocab{graph-coloring register allocators}.

Graph-coloring register allocators begin by reformulating the problem of register allocation in terms of \vocab{live ranges}. A definition of a variable is \vocab[definitions!liveness]{live} until it is \vocab[definitions!killing]{killed} by a redefinition of the same variable. For example, if I assign $5$ to the variable $n$, that definition is live until I later assign another value to $n$, say $6$. The extent of the program where the definition is live is its live range.\footnote{In this case, we are concerned with the \vocab{static extent}, or \vocab{scope}, of the definition. This is made explicit when the program is represented in \SSA[long] form. There is a corresponding notion of \vocab{dynamic extent}, which is the period of time when the definition is live at runtime, but this does not concern us here except as it is reflected in the definition's static extent.} All uses of the variable within a definition's live range refer to that definition. These live ranges are in competition for the limited supply of registers. Where they overlap, they are said to interfere with each other. From this, it is simple to construct an \vocab{interference graph}: each live range is a node, and two nodes are joined by an edge whenever their live ranges interfere. Coloring a node corresponds to assigning its live range to a register.

After we have constructed the interference graph for a region, the nodes divide into two fundamental groups. Those nodes with more neighbors than there are registers are \vocab[live range!constrained]{constrained}, while those with fewer are \vocab[live range!unconstrained]{unconstrained}. This captures a basic distinction: the live ranges represented by unconstrained nodes can always be assigned to a register; those represented by constrained nodes must compete with their neighbors in the interference graph for the limited number of registers.

A top-down graph-coloring register allocator will use this graph to prioritize the live ranges. It will first try to color the constrained nodes based on the estimated cost of having to spill their associated live ranges. After that is done, it is trivial to color the unconstrained nodes. The devil lies in how to estimate spill costs and how to handle cases where this process results in nodes that cannot be colored. 

Rather than using an overall estimate to determine the nodes' coloring priorities, a bottom-up allocator will work directly from the interference graph, node by node, to decide the order in which it will try to color the nodes. For example, it might pluck them out one by one, beginning with the unconstrained nodes, and place them on a stack.\footnote{The bottom-up allocator would remove unconstrained nodes first for two reasons. For one, removing them first puts them at the bottom of the stack, which delays coloring them till the end. For another, removing them reduces the \vocab{degree}, or number of neighbors, of neighboring nodes in the resulting graph. A node that was previously constrained might thus become unconstrained.} To color the nodes, it works through the stack from top to bottom, gradually rebuilding the interference graph. It removes a node from the stack, reinserts it and its edges into the graph, and attempts to color it in the graph as it stands then.

If this process succeeds in coloring all nodes, register allocation is complete; otherwise, the bottom-up allocator must select nodes to spill and then insert the code to handle the spilled value.\footnote{An alternate tactic is \vocab[live ranges!splitting]{live range splitting}. Instead of spilling an entire range, we split the range into two smaller ranges. This might divide the uncolorable node into two colorable nodes. If one split does not, further splitting eventually will: spilling the entire range corresponds to the finest splitting of all, where each use occurs in its own range.} If it has reserved registers to deal with this as we assumed earlier, allocation is complete, though such reservation might create a need to spill. On the other, if it has not, the changed program, which now incorporates the spill code, must undergo analysis and allocation anew.

Unfortunately, the interference graph does not capture all aspects of the problem, and so graph coloring does not provide a complete solution. These weak spots are also the points where graph-coloring register allocation can most be improved. This lack of a perfect fit also leaves room for other approaches with other inspirations, such as jigsaw puzzles: what is the best way to assemble the live-range pieces?

In the end, the program does not need the best register allocation, just a good enough register allocation. Once that is achieved, compilation can be considered complete. The program's odyssey through the compiler, its journey in many guises through the many parts, is at an end.