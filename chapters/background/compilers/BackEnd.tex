\section{Back End: Generating Target Code}\label{background:compilers:back}
The \vocab{back end} is responsible for completing the work of a compiler. It receives the program in some \IR{}, itself might construct various further \IRs of the program, and ultimately produces the final representation, the program in the target language. The \IR expresses a computation in a form understood by the compiler. The back end must take this and express it in the target language. This requires finding translatable units and recording their translation, then sequencing these translations for the best effect. This translation must obey whatever resource limits exist in the target language.

Here, we will focus on an \ISL as the target language. In this setting, the task of choosing how to represent the elements of the \IR in the target language corresponds to instruction selection; ordering the translations corresponds to instruction scheduling; and working within the limits of the target language corresponds to register allocation.\footnote{This is true if the \IR treats all data as being in registers except when it cannot. If the \IR instead leaves all data in memory and moves it into registers for only as long as necessary, then \vocab{register promotion}, which is the process of figuring out what data can be promoted from storage in memory to storage in register and then promoting it, is a better word for what occurs than register allocation. This promotion step is more a matter of taking advantage of the power of the language rather than one of restricting the translation to obey the language's limits. We will discuss register allocation here, but similar techniques apply to register promotion.}

These tasks are not cleanly separated. Choices made in each can (and when they cannot because of particular architectural decisions, they perhaps should) affect the others. The instructions selected to express a particular subcomputation can increase or decrease the demand on registers, which can require instructions be inserted to free up registers for other computations. The introduction of new instructions would strongly suggest that the whole sequence of instructions be rescheduled, which can again introduce problems with register load. Nevertheless, we will discuss them separately, because that is how they are best dealt with.

\subsection{Instruction Selection}
Instruction selection provides the basic material of the program in the target language. While instruction scheduling and register allocation are necessary for correctness, they simply rework the instructions generated in instruction selection.

Instruction selection is tasked with bridging from an \IR to the actual target language. As with bridges, the nearer one side is to the other, the easier it will be to bridge the gap: the closer the \IR is to the target language, the easier the job of instruction selection. If the \IR is not very low-level, it will be necessary to convert it to something low-level. This will likely not be a very clean conversion if left to so late in compilation; there will be a lot of code meant to work around possible problems that may or may not be present because the results of earlier analyses that matter at a low level were not represented in the \IR. If the \IR is low-level, but its architectural model differs from that of the target platform\empause the \IR is stack-based or resembles the assembly language of a \abbrev{RISC} machine, while the target platform is a \abbrev{CISC} machine, say\empause it will be more difficult to perform instruction scheduling.

However difficult it might be, the same basic ideas suffice for instruction selection. To avoid clouding the exposition, we will assume the low-level \IR that enters instruction selection is tree-based. A simple approach would simply walk the tree and generate, whenever possible, general instructions that ignore related nodes. A more complex approach would attempt to use local optimization and awareness of related nodes to build up a sequence of instructions.

A rather different approach uses a technique called peephole optimization that was originally developed to perform some last optimizations on the target code. It used a library of patterns to simplify a small window, or peephole, of a few instructions at a time. By scrolling this window through the entirety of the target code, less efficient code patterns could be replaced with more efficient counterparts. The limited window size keeps the process very quick, but all the same, it is able to perform some useful optimizations.

\subsubsection{A Simple Tree-Based Approach}
The simplest approach would generate instructions during a single tree walk. This would not be much more complicated than flattening the tree. It would also not produce very good code, since it would either make no use of context or only very limited use. Context is essential to producing a decent instruction sequence. A number of instruction sequences can be used to encode even straightforward arithmetic statements. Consider \code{x = y + 4}. %
%TODO: Insert tree representation of x = y + 4.
Focusing on one node at a time, a \RISC{}-like instruction sequence might load \code{y} into a register, load \code{4} into a register, sum those values and store the result in yet another register, which becomes \code{x}. %
%FIXME: This is an ugly hack. Convert these to listings in floats. Side-by-side listings positioned as tbp would be good.
\begin{align*}
r_{y} &\gets y\\
r_{4} &\gets 4\\
r_{y + 4} &\gets \text{\code{add}}\quad r_{y}\quad r_{4}\\
r_{x} &\gets r_{y + 4}
\end{align*}
If we consider a bit more of the tree, we might load the value of \code{y} into a register, then use an immediate addition operation to compute \code{y + 4} and store the result into a register that represents \code{x}.
\begin{align*}
r_{y} &\gets y\\
r_{x} &\gets \text{\code{addi}}\quad r_{y}\quad 4
\end{align*}

Na\"ively generating code to access array elements (which is how local variables are frequently represented at a low level in a program) can result in many redundant computations as part of the offset from the start of the array is calculated and recalculated and shared elements of the computations are not reused. Trying to eliminate redundant computations significantly complicates the code with special-case optimizations. Traveling any distance along this route of attempting to hand-optimize a simple scheme strongly suggests that more complex methods be employed. Fortunately, more complex methods are available.

\subsubsection{Tree Pattern-Matching}
Tree pattern-matching methods are instruction selection methods that use a store of tree patterns to build instructions. A common approach is that of \vocab{bottom-up rewrite systems (BURS)}. \BURS[Long]{pl} work by tiling the tree with a stock of predefined patterns. As each node is subsumed by a pattern tile, a choice is made based on the tiles of its subtrees that minimizes the cost of the tiling. The costs can be fixed or allowed to vary dynamically during the rewriting, say, to reflect the demand on registers introduced thus far. The costs can represent whatever it is one wishes to optimize for during instruction selection: code size, power consumption, execution time, or whatever else.

The patterns and costs used by a \BURS[long]{sg} in tiling the tree can readily be represented in a table, which suggests the use of ``code generator generators'' similar to the lexer and parser generators used in producing the front-end, and such do exist. The rewrite rules make use of \CFGs{}. Productions represent the shape of the tree. Costs are associated to each production, along with code templates. This is quite similar to an attribute grammar, and a \BURS[long]{sg} likely could be described in that framework.

To tile the tree, we work from the bottom up, considering one node (as the root of a subtree) at a time. The tiling proceeds by identifying productions whose bodies match the subtree headed by the node currently under consideration. The least costly production is selected, and we move on to another node and its subtree. All the information we need know about a subtree is encoded in the head of the production selected when its root was considered. Once we have tiled the entire tree, a traversal fills in the code templates and records the instruction sequence.

This tree pattern-matching process is highly suggestive of number of other processes, which suggests adapting their techniques to fit this purpose. There are the classic pattern matchers, the \FAs{}; the \CFG component as well as the bottom-up method suggests adapting parsing techniques; a tree flattened into a string could perhaps be attacked using string matching methods (which, in many cases, ultimately end in use of \FAs); or, now that the problem is better understood, we can hand-code a tree pattern-matching program.

\subsubsection{Peephole}
Peepholes are generally thought of in terms of \vocab{peephole optimization}, as briefly described at the start of our discussion of the back end. However, their methods can also be used for instruction selection alongside optimization. The problem again reduces to pattern matching, but unlike in our discussion of tree pattern-matching, we assume the \IR used for pattern matching with a peephole is linear, like the assembly code instruction sequences that peepholes were intended to optimize.

Instruction selection through a peephole begins by transforming the \IR to an especially low-level form that models all the side-effects of instructions on the target machine. The peephole is used to simplify this instruction sequence and then to match patterns in this simplified sequence. These patterns are associated with code in the target language. Unlike the bottom-up methods used for trees, here the patterns are matched linearly and sequentially (visually, from top to bottom in the normal way of writing code).

What kind of simplifications can be seen through a peephole? Within a peephole, we can avoid unnecessary storage of temporary values in registers by substituting the value itself in place of its register in operations using the register. We can recognize a store followed by a load of the same value. Some simplifications might enable other simplifications. If we give the simplifier knowledge of when a given value is used by preprocessing the expanded low-level \abbrev{IR}, we can jump back up and eliminate computation and storage of a value if later simplifications eliminate all its uses. Control flow complicates matters: should we use a strictly static window, or should our window include instructions that might follow during execution? Should we look at all uses of a value together, ignoring intervening instructions, and proceed that way? The basic idea is amenable to considerable sophistication; the pattern-recognition and simplification part is, as with tree pattern-matching, also producible through a generator, at least as far as its basic elements go.

Clever implementation can enable the instruction selector to learn about simplification patterns. One can use well thought-out heuristics to quickly generate and test a variety of instruction sequences of various costs, simply by pasting together operations. Sequences that do no have the same effect as that identified for improvement are quickly discarded. Guided by a skilled compiler implementor and a suitable sampling of programs, this exhaustive search approach can be used during development to generate a sophisticated library of simplification patterns for later use.

\subsection{Instruction Scheduling}
Instruction selection produces a sequence of instructions, but its concern is generating instructions that can carry out the needed computations, not making sure all the instructions will work together: Does a use of a value come too soon after its definition, while the value is still being computed and not yet available? Will this introduce an error in the program, or simply unnecessary delay? Instruction scheduling worries about problems like these. It tries to ensure the selected instructions will work well together by reordering them. Its prime directives are to enhance instruction-level parallelism and reduce the time required by the program. It works at the block-level so that it does not have to deal with the consequences of control flow. It is hoped that stitching the scheduled blocks together will result in a good enough overall schedule, and this hope is generally realized.

What limits are placed on reordering? These limits are best expressed in terms of \vocab{dependences} between instructions.\footnote{We do indeed mean ``dependence'' (plural ``dependences'') and not ``dependency'' (plural ``dependencies'').} We always speak of the later instruction as depending in some way on the earlier. There are a variety of ways one instruction can depend on another. Perhaps the most obvious sort of dependence is \vocab{control dependence}, when a sequence of instructions only executes if some condition is met. For example, in listing~\ref{backend:controldep}%
%It seems the two differ, even when they should be referring to the same page.
%\ifthenelse{\equal{\pageref{backend:controldep}}{\thepage}}%
%{}{ on page~\pageref{backend:controldep}}%
, the instructions between the first line and the line labeled \lstinline{TARGET} are control dependent on the first line's instruction, which says to branch to the instruction labeled \lstinline{TARGET} if the value in register \lstinline[style=riscpseudo]{rx} is greater than zero, since their execution depends on whether the first line sends the flow of control immediately to \lstinline{TARGET}, skipping over them, or not.
%listing backend:controldep
\begin{lstlisting}[float=btp,%
caption={Example of control dependence},%
label={backend:controldep},%
style=riscpseudo,%
escapechar=!]
bgtz rx =: TARGET
!\textit{instructions\dots{}}!
TARGET: !\textit{instructions\dots{}}!
\end{lstlisting}%
%listing backend:datadep
\lstinputlisting[float=btp,%
caption={Example of data dependences},%
label=backend:datadep,%
style=riscpseudo,%
numbers=left,stepnumber=1]%
{chapters/background/compilers/datadependences.txt}

Reordering also must respect \vocab{data dependences} in the initial instruction sequence. To understand data dependence, it is necessary to think of an instruction as receiving input and producing output, which it stores in an output location, frequently a register. For example, the addition instruction in line two of listing~\ref{backend:datadep} %
%on page~\pageref{backend:datadep}%
takes as input \lstinline[style=riscpseudo]{rx} and \lstinline{1}, produces their sum, and stores that value in the register \lstinline[style=riscpseudo]{ry}.

There are several types of data dependence. A \vocab{true data dependence} exists between two instructions when one requires a value created by another, that is, an input of the one is the output of the other: we cannot use a value before it has been defined. Such is the case in lines one and two of listing~\ref{backend:datadep}, since the value stored in \lstinline[style=riscpseudo]{ry} on line two depends on the value of \lstinline[style=riscpseudo]{rx} defined in line one. Two instructions are \vocab[output dependence]{output dependent} when both modify the same resource, that is, when both have the same output location. In listing~\ref{backend:datadep}, the instructions on the first and fourth lines are output dependent, since both store to \lstinline[style=riscpseudo]{rx}.

You might wonder whether it is also possible for instructions to be input dependent. It is not, as instructions are not kept from having their relative orders inverted simply because they share an input: whether line two of listing~\ref{backend:datadep} precedes line three or line three precedes line two, both orders will result in the same values being stored to \lstinline[style=riscpseudo]{ry} and \lstinline[style=riscpseudo]{rz}. However, the idea can be useful, so some compilers will track it nevertheless as a sort of \vocab{input pseudo-dependence}.

In addition to these various kinds of data dependence, reordering must respect or eliminate \vocab{antidependences}. These are dependences between instructions that exist, not because of data flow, but because of conflicting uses of the same resources: specifically, one instruction is antidependent on a preceding instruction when its output location is used as input to the earlier instruction. The dependence is created solely by the reuse of the location: if the later instruction were to output to a different location and no other dependence existed between the two instructions, then they could be freely reordered with respect to each other. In listing~\ref{backend:datadep}, the instruction on line four is antidependent upon both preceding instructions.%
\footnote{Looping structures present a mess of dependence problems of their own. However, we do not discuss them here, as they are generally the target of analysis and optimization in the middle end.}

%TODO: Insert dependence graph example with labeled dependence arrows.

These dependences can be used to create a \vocab{dependence graph} (also called a \vocab{precedence graph}) representing the program, where each instruction is a node and there is a directed edge from a first node to a second whenever the second depends on the first. The graph is used along with information about the target platform to produce a schedule, which associates each instruction-node with a positive integer specifying the cycle in which it should be issued. The information needed is the functional units required by the instruction and the number of cycles the instruction takes to execute, called the \vocab{delay} of the instruction. If no value is required in the schedule before it is ready, the schedule will be correct. If there are never more instructions executing than the functional units can handle, and there are never more instructions dispatched in a cycle than is possible for the target platform, the schedule will be feasible. Within these constraints, we must attempt to schedule the instructions so that all dependences are respected and the cost of the schedule (often, the amount of time it requires) is minimized. This, of course, is an ideal that we cannot guarantee in practice.

The graph can be usefully annotated with the cumulative delay to each node starting from a root. The path from a root of the dependence graph to the highest-numbered leaf is called the \vocab{critical path} and critically determines the length of the schedule: no matter what, it can take no less time than the annotation at the leaf endpoint of the critical path.

%TODO: Repeat dependence graph with cumulative delay annotations and critical path(s) highlighted.

\subsubsection{List Scheduling}
The dominant paradigm for scheduling is called \vocab{list scheduling}. The basic idea of the method is, first, to eliminate antidependences by renaming the contested resources; next, to build the dependence graph; then, to assign priorities to each operation (for example, as determined by the cumulative delay at that operation's node); and, finally, to schedule the instructions in priority order as their dependences are fulfilled. This last step simulates the passage of cycles in order to track when operations can safely be scheduled and record the resulting schedule.

Clearly, there is a lot of detail missing from this sketch. The priority scheme used, for example, has an important effect on the resulting schedule, as does the tiebreaking method between operations with identical priorities. There is no consensus on the best priority scheme, likely because there is no such thing. Further, we can perform scheduling either forward or backward. Working forward, we first schedule the instruction we want to execute in the first cycle, then repeatedly update the cycle counter and select the next instruction to execute. Before scheduling an instruction, we must check that sufficient cycles have passed that all instructions it depends on have completed. In the dependence graph, then, forward scheduling works from the leaves to the roots. Working backward, roots are scheduled before leaves, and the first operation scheduled executes last. We are then scheduling each instruction before the instructions it depends on. When we were working forward, before scheduling an instruction we would check that all instructions it depended upon had been completed in the simulation; now, working backward, we first schedule an instruction and then delay scheduling each instruction it depends on until we are far enough in advance of that instruction in our simulation that its result will be available to the already scheduled instruction. Neither forward nor backward scheduling is always best; since scheduling is fairly easily done, often a block will be scheduled both forward and backward, possibly a few times using different priority schemes, and the best of the schedules produced is then chosen.

There is also a lot of room to elaborate the method. Why limit ourselves to scheduling block-by-block? We can produce a better overall schedule if we look beyond a basic block. If either of two blocks can follow a single block, each successor block will work best with one or another scheduling of the predecessor, but we can only schedule the predecessor in one way. How should we decide which successor should determine the schedule of the predecessor? If we were to generate code for the region we wish to schedule, run it several times, and track which blocks execute most frequently, we could make an informed decision. Some schedulers take this tack, called \vocab{trace scheduling} since it makes use of an \vocab{execution trace}, or record of execution.%
%NOTE: We omit discussion of loop scheduling.
%TODO: Mention loop scheduling and give pointers in the bib refs.

\subsection{Register Allocation}
Register allocation is the final step of code generation. Instructions have been generated and scheduled. Now, it is time to ensure they meet the platform's register constraints. The most fundamental constraint imposed on registers is the number available, but others must also be taking into account. These include constraints on register usage imposed by calling conventions and those imposed by register classes.

Register allocation is, in fact, an umbrella term for two closely related tasks: \vocab{register allocation}, which is the task of deciding which values should reside in registers when each instruction is issued, and \vocab{register assignment}, which takes the values to be allocated to registers and decides which register should hold which value when each instruction is issued.

Often, all values cannot be kept in registers. Dumping the register's contents to memory is called \vocab{register spilling}. Spilling a register is necessary but expensive. At each use, the data must be loaded into a ``scratch register,'' and any changes to the data must be stored back to memory in order to free up the scratch register to load other spilled values. Sometimes, it is cheaper to recompute a value at each use than to go through the expense of spilling it and loading it back. Recomputation in place of register spilling is referred to as \vocab{rematerialization}: rather than using previously provided ``material,'' we are recreating it as needed.

Clearly, register allocation directly affects register assignment. Unfortunately, the interaction of the two concerns\empause what values should be kept in registers, and which registers should they be kept in\empause are not cleanly separated. Since we might be bound to assign particular sorts of values to particular registers, issues of assignment can affect register allocation: we might be unable to use floating point registers for anything except floating point values, and a calling convention will likely specify that arguments to the procedure must be stored in specific registers, not just in some registers. Since marshaling data to and from register and memory itself requires registers, we do not even have the whole register set available.

The overall process of register allocation (which is what we shall mean by ``register allocation'' from now on), then, is nontrivial.\footnote{In fact, it ends up being NP-complete for any realistic formulation of the problem. A polynomial-time algorithm exists for the simplest of cases, as well as for \SSA form (see \partandnameref{Section}{background:compilers:irs} for a description of \SSA form), but virtually any additional complexity\empause including the translation from \SSA into the processor's instruction set language\empause promotes the problem to NP-completeness. Naturally, any time you actually find yourself needing to perform register allocation, you likely will not be dealing with the polynomial-time case.} As with much in compiler design, we must resort to heuristics. We wish to minimize the amount of register-memory traffic by maximizing the amount of data kept in registers. A simple register allocator would consider only a block at a time. At the end of a block, it would spill all its registers. (A following optimization pass could attempt to remove unnecessary spills.) A top-down approach would estimate how many times a value is used in the block, allocate those to registers throughout the entire block, and spill the rest of the values used in the block to memory. A bottom-up approach would work through the block, instruction by instruction, and ensure that the operands of each instruction are in register. Where possible, it will use values already in register and load values into free registers. When all registers are full and a value not in register is needed, it will spill the value whose next use is farthest from the current instruction.

The top-down approach works, in a sense, by using detailed information about the block to set an overall policy, which it then follows. The bottom-up approach also uses detailed information about the block, but it makes its decisions instruction by instruction, rather than following an overall plan for the block. Its only plan is the same for all blocks: make the tough decisions (which values to spill, which registers to use?) when it has to. This top-down--bottom-up dichotomy persists through all types of register allocators, though much the same effect can be achieved either way.\footnote{It is an artifact of our simple description that the top-down allocator will dedicate a register throughout the entire block to a value heavily used in the block's first half but unused in its second, while the bottom-up allocator will choose to spill the value once it is no longer needed. Getting the top-down allocator to behave similarly, however, would make it less simple.}

More complex algorithms are required to handle register allocation across greater regions than single blocks. Modern register allocation algorithms often draw their inspiration from the graph coloring problem. Because of this, they are called \vocab{graph-coloring register allocators}.

Graph-coloring register allocators begin by reformulating the problem of register allocation in terms of \vocab{live ranges}. A definition of a variable is \vocab[definitions!liveness]{live} until it is \vocab[definitions!killing]{killed} by a redefinition of the same variable. For example, if I assign $5$ to the variable $n$, that definition is live until I later assign another value to $n$, say $6$. The extent of the program where the definition is live is its live range.\footnote{In this case, we are concerned with the \vocab{static extent}, or \vocab{scope}, of the definition. This is made explicit when the program is represented in \SSA[long] form. There is a corresponding notion of \vocab{dynamic extent}, which is the period of time when the definition is live at runtime, but this does not concern us here except as it is reflected in the definition's static extent.} All uses of the variable within a definition's live range refer to that definition. These live ranges are in competition for the limited supply of registers. Where they overlap, they are said to interfere with each other. From this, it is simple to construct an \vocab{interference graph}: each live range is a node, and two nodes are joined by an edge whenever their live ranges interfere. Coloring a node corresponds to assigning its live range to a register.

After we have constructed the interference graph for a region, the nodes divide into two fundamental groups. Those nodes with more neighbors than there are registers are \vocab[live range!constrained]{constrained}, while those with fewer are \vocab[live range!unconstrained]{unconstrained}. This captures a basic distinction: the live ranges represented by unconstrained nodes can always be assigned to a register; those represented by constrained nodes must compete with their neighbors in the interference graph for the limited number of registers.

A top-down graph-coloring register allocator will use this graph to prioritize the live ranges. It will first try to color the constrained nodes based on the estimated cost of having to spill their associated live ranges. After that is done, it is trivial to color the unconstrained nodes. The devil lies in how to estimate spill costs and how to handle cases where this process results in nodes that cannot be colored. 

Rather than using an overall estimate to determine the nodes' coloring priorities, a bottom-up allocator will work directly from the interference graph, node by node, to decide the order in which it will try to color the nodes. For example, it might pluck them out one by one, beginning with the unconstrained nodes, and place them on a stack.\footnote{The bottom-up allocator would remove unconstrained nodes first for two reasons. For one, removing them first puts them at the bottom of the stack, which delays coloring them till the end. For another, removing them reduces the \vocab{degree}, or number of neighbors, of neighboring nodes in the resulting graph. A node that was previously constrained might thus become unconstrained.} To color the nodes, it works through the stack from top to bottom, gradually rebuilding the interference graph. It removes a node from the stack, reinserts it and its edges into the graph, and attempts to color it in the graph as it stands then.

If this process succeeds in coloring all nodes, register allocation is complete; otherwise, the bottom-up allocator must select nodes to spill and then insert the code to handle the spilled value.\footnote{An alternate tactic is \vocab[live ranges!splitting]{live range splitting}. Instead of spilling an entire range, we split the range into two smaller ranges. This might divide the uncolorable node into two colorable nodes. If one split does not, further splitting eventually will: spilling the entire range corresponds to the finest splitting of all, where each use occurs in its own range.} If it has reserved registers to deal with this as we assumed earlier, allocation is complete, though such reservation might create a need to spill. On the other, if it has not, the changed program, which now incorporates the spill code, must undergo analysis and allocation anew.

Unfortunately, the interference graph does not capture all aspects of the problem, and so graph coloring does not provide a complete solution. These weak spots are also the points where graph-coloring register allocation can most be improved. This lack of a perfect fit also leaves room for other approaches with other inspirations, such as jigsaw puzzles: what is the best way to assemble the live-range pieces?

In the end, we do not need to find the absolute best register allocation. To carry out the computation specified by the original source code, it suffices to find a register allocation acceptable to the user. With that done, compilation can be considered complete. The program's odyssey through the compiler, its journey in many guises through the many parts, is at an end.