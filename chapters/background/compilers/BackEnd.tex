\section{Back End: Generating Target Code}
The \vocab{back end} is responsible for completing the work of a compiler. It receives the program in some \IR{}, itself might construct various further \IRs of the program, and ultimately produces the final representation, the program in the target language. The \IR expresses a computation in a form understood by the compiler. The back end must take this and express it in the target language. This requires finding translatable units and recording their translation, then sequencing these translations for the best effect. This translation must obey whatever resource limits exist in the target language.

Here, we will focus on an \ISL as the target language. In this setting, the task of choosing how to represent the elements of the \IR in the target language corresponds to instruction selection; ordering the translations corresponds to instruction scheduling; and working within the limits of the target language corresponds to register allocation.\footnote{This is true if the \IR treats all data as being in registers except when it cannot. If the \IR instead leaves all data in memory and moves it into registers for only as long as necessary, then \vocab{register promotion}, which is the process of figuring out what data can be promoted from storage in memory to storage in register and then promoting it, is a better word for what occurs than register allocation. This promotion step is more a matter of taking advantage of the power of the language rather than one of restricting the translation to obey the language's limits. We will discuss register allocation here, but similar techniques apply to register promotion.}

These tasks are not cleanly separated. Choices made in each can (and when they cannot because of particular architectural decisions, they perhaps should) affect the others. The instructions selected to express a particular subcomputation can increase or decrease the demand on registers, which can require instructions be inserted to free up registers for other computations. The introduction of new instructions would strongly suggest that the whole sequence of instructions be rescheduled, which can again introduce problems with register load. Nevertheless, we will discuss them separately, because that is how they are best dealt with.

\subsection{Instruction Selection}
Instruction selection provides the basic material of the program in the target language. While instruction scheduling and register allocation are necessary for correctness, they simply rework the instructions generated in instruction selection.

Instruction selection is tasked with bridging from an \IR to the actual target language. As with bridges, the nearer one side is to the other, the easier it will be to bridge the gap: the closer the \IR is to the target language, the easier the job of instruction selection. If the \IR is not very low-level, it will be necessary to convert it to something low-level. This will likely not be a very clean conversion if left to so late in compilation; there will be a lot of code meant to work around possible problems that may or may not be present because the results of earlier analyses that matter at a low level were not represented in the \IR. If the \IR is low-level, but its architectural model differs from that of the target platform\empause the \IR is stack-based or resembles the assembly language of a \abbrev{RISC} machine, while the target platform is a \abbrev{CISC} machine, say\empause it will be more difficult to perform instruction scheduling.

However difficult it might be, the same basic ideas suffice for instruction selection. To avoid clouding the exposition, we will assume the low-level \IR that enters instruction selection is tree-based. A simple approach would simply walk the tree and generate, whenever possible, general instructions that ignore related nodes. A more complex approach would attempt to use local optimization and awareness of related nodes to build up a sequence of instructions.

A rather different approach uses a technique called peephole optimization that was originally developed to perform some last optimizations on the target code. It used a library of patterns to simplify a small window, or peephole, of a few instructions at a time. By scrolling this window through the entirety of the target code, less efficient code patterns could be replaced with more efficient counterparts. The limited window size keeps the process very quick, but all the same, it is able to perform some useful optimizations.

\subsubsection{A Simple Tree-Based Approach}
The simplest approach would generate instructions during a single tree walk. This would not be much more complicated than flattening the tree. It would also not produce very good code, since it would either make no use of context or only very limited use. Context is essential to producing a decent instruction sequence. A number of instruction sequences can be used to encode even straightforward arithmetic statements. Consider \code{x = y + 4}. %
%TODO: Insert tree representation of x = y + 4.
Focusing on one node at a time, a \RISC{}-like instruction sequence might load \code{y} into a register, load \code{4} into a register, sum those values and store the result in yet another register, which becomes \code{x}. %
%FIXME: This is an ugly hack. Convert these to listings in floats. Side-by-side listings positioned as tbp would be good.
\begin{align*}
r_{y} &\gets y\\
r_{4} &\gets 4\\
r_{y + 4} &\gets \text{\code{add}}\quad r_{y}\quad r_{4}\\
r_{x} &\gets r_{y + 4}
\end{align*}
If we consider a bit more of the tree, we might load the value of \code{y} into a register, then use an immediate addition operation to compute \code{y + 4} and store the result into a register that represents \code{x}.
\begin{align*}
r_{y} &\gets y\\
r_{x} &\gets \text{\code{addi}}\quad r_{y}\quad 4
\end{align*}

Na\"ively generating code to access array elements (which is how local variables are frequently represented at a low level in a program) can result in many redundant computations as part of the offset from the start of the array is calculated and recalculated and shared elements of the computations are not reused. Trying to eliminate redundant computations significantly complicates the code with special-case optimizations. Traveling any distance along this route of attempting to hand-optimize a simple scheme strongly suggests that more complex methods be employed. Fortunately, more complex methods are available.

\subsubsection{Tree Pattern-Matching}
Tree pattern-matching methods are instruction selection methods that use a store of tree patterns to build instructions. A common approach is that of \vocab{bottom-up rewrite systems (BURS)}. \BURS[Long]{pl} work by tiling the tree with a stock of predefined patterns. As each node is subsumed by a pattern tile, a choice is made based on the tiles of its subtrees that minimizes the cost of the tiling. The costs can be fixed or allowed to vary dynamically during the rewriting, say, to reflect the demand on registers introduced thus far. The costs can represent whatever it is one wishes to optimize for during instruction selection: code size, power consumption, execution time, or whatever else.

The patterns and costs used by a \BURS[long]{sg} in tiling the tree can readily be represented in a table, which suggests the use of ``code generator generators'' similar to the lexer and parser generators used in producing the front-end, and such do exist. The rewrite rules make use of \CFGs{}. Productions represent the shape of the tree. Costs are associated to each production, along with code templates. This is quite similar to an attribute grammar, and a \BURS[long]{sg} likely could be described in that framework.

To tile the tree, we work from the bottom up, considering one node (as the root of a subtree) at a time. The tiling proceeds by identifying productions whose bodies match the subtree headed by the node currently under consideration. The least costly production is selected, and we move on to another node and its subtree. All the information we need know about a subtree is encoded in the head of the production selected when its root was considered. Once we have tiled the entire tree, a traversal fills in the code templates and records the instruction sequence.

This tree pattern-matching process is highly suggestive of number of other processes, which suggests adapting their techniques to fit this purpose. There are the classic pattern matchers, the \FAs{}; the \CFG component as well as the bottom-up method suggests adapting parsing techniques; a tree flattened into a string could perhaps be attacked using string matching methods (which, in many cases, ultimately end in use of \FAs); or, now that the problem is better understood, we can hand-code a tree pattern-matching program.

\subsubsection{Peephole}
Peepholes are generally thought of in terms of \vocab{peephole optimization}, as briefly described at the start of our discussion of the back end. However, their methods can also be used for instruction selection alongside optimization. The problem again reduces to pattern matching, but unlike in our discussion of tree pattern-matching, we assume the \IR used for pattern matching with a peephole is linear, like the assembly code instruction sequences that peepholes were intended to optimize.

Instruction selection through a peephole begins by transforming the \IR to an especially low-level form that models all the side-effects of instructions on the target machine. The peephole is used to simplify this instruction sequence and then to match patterns in this simplified sequence. These patterns are associated with code in the target language. Unlike the bottom-up methods used for trees, here the patterns are matched linearly and sequentially (visually, from top to bottom in the normal way of writing code).

What kind of simplifications can be seen through a peephole? Within a peephole, we can avoid unnecessary storage of temporary values in registers by substituting the value itself in place of its register in operations using the register. We can recognize a store followed by a load of the same value. Some simplifications might enable other simplifications. If we give the simplifier knowledge of when a given value is used by preprocessing the expanded low-level \abbrev{IR}, we can jump back up and eliminate computation and storage of a value if later simplifications eliminate all its uses. Control flow complicates matters: should we use a strictly static window, or should our window include instructions that might follow during execution? Should we look at all uses of a value together, ignoring intervening instructions, and proceed that way? The basic idea is amenable to considerable sophistication; the pattern-recognition and simplification part is, as with tree pattern-matching, also producible through a generator, at least as far as its basic elements go.

Clever implementation can enable the instruction selector to learn about simplification patterns. One can use well thought-out heuristics to quickly generate and test a variety of instruction sequences of various costs, simply by pasting together operations. Sequences that do no have the same effect as that identified for improvement are quickly discarded. Guided by a skilled compiler implementor and a suitable sampling of programs, this exhaustive search approach can be used during development to generate a sophisticated library of simplification patterns for later use.

\subsection{Instruction Scheduling}
Instruction selection produces a sequence of instructions, but its concern is generating instructions that can carry out the needed computations, not making sure all the instructions will work together: Does a use of a value come too soon after its definition, while the value is still being computed and not yet available? Will this introduce an error in the program, or simply unnecessary delay? Instruction scheduling worries about problems like these. It tries to ensure the selected instructions will work well together by reordering them. Its prime directives are to enhance instruction-level parallelism and reduce the time required by the program. It works at the block-level so that it does not have to deal with the consequences of control flow. It is hoped that stitching the scheduled blocks together will result in a good enough overall schedule, and this hope is generally realized.

What limits are placed on reordering? These limits are best expressed in terms of \vocab{dependences} between instructions. We always speak of the later instruction as depending in some way on the earlier. There are a variety of ways one instruction can depend on another. Perhaps the most obvious sort of dependence is \vocab{control dependence}, when a sequence of instructions only executes if some condition is met. Reordering must also respect \vocab{data dependences} in the initial instruction sequence: we cannot use a value before it has been defined. It also must respect or eliminate so-called \vocab{antidependences}. These are dependences between instructions that exist, not because of data flow, but because of conflicting uses of the same resources: specifically, one instruction is antidependent on another, which must follow it, when it makes use of a value in a resource and that resource is overwritten by the other instruction. For example, if one instruction uses a value in a register that is clobbered by the subsequent instruction, reversing the order of these two instructions would prematurely clobber the register and lead to the now following instruction reading the wrong value out of the register: %
%FIXME: To listing.
\begin{align*}
r_{y} &\gets r_{x} + 1\\
&\hphantom{\gets}\vdots\\
r_{x} &\gets 5
\end{align*}
A closely related kind of dependence is \vocab{output dependence}, where two instructions both modify the same resource. If we expand on the previous example by introducing the instruction that creates the value of $r_{x}$ %
%TODO: using math here for registers r_{blah}; this needs to be checked later to ensure consistent typefaces between the listings and here, and between all such uses for r_{blah}. All are not flagged; just grep r_ and follow up.
used in the addition and update of $r_{y}$, then the second instruction setting $r_{x}$ would be output-dependent on the first. You might wonder whether output's dual, input, also has a related dependence. It does not, as instructions are not kept from having their orders reversed simply because they read from the same resource. However, the idea can be useful, so some compilers will track it nevertheless as a sort of \vocab{input pseudo-dependence}.\footnote{Looping structures present a mess of dependence problems of their own. However, we do not discuss them here, as they are generally the target of analysis and optimization in the middle end.}

These dependences can be used to create a \vocab{dependence graph} (also called a \vocab{precedence graph}) representing the program, where each instruction is a node and there is a directed edge from a first node to a second whenever the second depends on the first. The graph is used along with information about the target platform to produce a schedule, which associates each instruction-node with a positive integer specifying the cycle in which it should be issued. The information needed is the functional units required by the instruction and the number of cycles the instruction takes to execute, called the \vocab{delay} of the instruction. If no value is required in the schedule before it is ready, the schedule will be correct. If there are never more instructions executing than the functional units can handle, and there are never more instructions dispatched in a cycle than is possible for the target platform, the schedule will be feasible. Within these constraints, we must attempt to schedule the instructions so that all dependences are respected and the cost of the schedule (often, the amount of time it requires) is minimized. This, of course, is an ideal that we cannot guarantee in practice.

The graph can be usefully annotated with the cumulative delay to each node starting from a root. The path from a root of the dependence graph to the highest-numbered leaf is called the \vocab{critical path} and critically determines the length of the schedule: no matter what, it can take no less time than the annotation at the leaf endpoint of the critical path.

\subsubsection{List Scheduling}
The dominant paradigm for scheduling is called \vocab{list scheduling}. The basic idea of the method is, first, to eliminate antidependences by renaming the contested resources; next, to build the dependence graph; then, to assign priorities to each operation (for example, as determined by the cumulative delay at that operation's node); and, finally, to schedule the instructions in priority order as their dependences are fulfilled. This last step simulates the passage of cycles in order to track when operations can safely be scheduled and record the resulting schedule.

Clearly, there is a lot of detail missing from this sketch. The priority scheme used, for example, has an important effect on the resulting schedule, as does the tiebreaking method between operations with identical priorities. There is no consensus on the best priority scheme, likely because there is no such thing. Additionally, in simulating the passage of cycle to schedule the operations, there is a choice to work forward or backward in time. Working forward, leaves are dealt with before roots. The first operation scheduled executes first. Working backward, roots are schedule before leaves, and the first operation scheduled executes last. Neither forward nor backward scheduling is always best; since scheduling is fairly easily done, often a block will be scheduled both forward and backward, possibly a few times using different priority schemes, and the best of the schedules produced is then chosen.

There is also a lot of room for elaborating the method. Why limit ourselves to scheduling block-by-block? We can produce a better overall schedule if we look beyond a basic block. If either of two blocks can follow a single block, each successor block will work best with one or another scheduling of the predecessor, but we can only schedule the predecessor in one way. How should we decide which successor should determine the schedule of the predecessor? If we were to generate code for the region we wish to schedule, run it several times, and track which blocks execute most frequently, we could make an informed decision. Some schedulers take this tack, called \vocab{trace scheduling} since it makes use of an \vocab{execution trace}, or record of execution.%
%NOTE: We omit discussion of loop scheduling.
%TODO: Mention loop scheduling and give pointers in the bib refs.

\subsection{Register Allocation}
Register allocation is the final step of code generation. Instructions have been generated and scheduled. Now, it is time to ensure they meet the final constraints of the platform, those on the number of registers and their usage. Register allocation is, in fact, an umbrella term for two closely related tasks: register allocation, which is the task of deciding what values should reside in registers at each point in the program, and register assignment, which makes the ultimate decision of which register should be occupied at each point in the program by each value slated to reside in register at that point. % complications: register classes, calling conventions; how did we treat these earlier? was it helpful for here, or should it be rewritten?